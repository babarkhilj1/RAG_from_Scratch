{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document/Text Processing and Embedding Creation\n",
    "\n",
    "Ingredients:\n",
    "* PDF document of choice.\n",
    "* Embedding model of choice.\n",
    "\n",
    "Steps:\n",
    "1. Import PDF document.\n",
    "2. Process text for embedding (e.g. split into chunks of sentences).\n",
    "3. Embed text chunks with embedding model.\n",
    "4. Save embeddings to file for later use (embeddings will store on file for many years or until you lose your hard drive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import PDF Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File harry-potter.pdf exists.\n"
     ]
    }
   ],
   "source": [
    "# Download PDF file\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Get PDF document\n",
    "pdf_path = \"harry-potter.pdf\"\n",
    "\n",
    "print(f\"File {pdf_path} exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF acquired!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221/221 [00:12<00:00, 17.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': 0,\n",
       "  'page_char_count': 0,\n",
       "  'page_word_count': 1,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 0.0,\n",
       "  'text': ''},\n",
       " {'page_number': 1,\n",
       "  'page_char_count': 0,\n",
       "  'page_word_count': 1,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 0.0,\n",
       "  'text': ''},\n",
       " {'page_number': 2,\n",
       "  'page_char_count': 143,\n",
       "  'page_word_count': 33,\n",
       "  'page_sentence_count_raw': 2,\n",
       "  'page_token_count': 35.75,\n",
       "  'text': \"HP 1 - Harry Potter and the Sorcerer's Stone Harry Potter and the Sorcerer's Stone     Harry Potter & The Sorcerer’s Stone     by  J.K. Rowling\"},\n",
       " {'page_number': 3,\n",
       "  'page_char_count': 44,\n",
       "  'page_word_count': 9,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 11.0,\n",
       "  'text': \"HP 1 - Harry Potter and the Sorcerer's Stone\"},\n",
       " {'page_number': 4,\n",
       "  'page_char_count': 2390,\n",
       "  'page_word_count': 471,\n",
       "  'page_sentence_count_raw': 33,\n",
       "  'page_token_count': 597.5,\n",
       "  'text': 'CHAPTER ONE   THE BOY WHO LIVED          M  r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense.       Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.       The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn’t think they could bear it if anyone found out about the Potters. Mrs. Potter was Mrs. Dursley’s sister, but they hadn’t met for several years; in fact, Mrs. Dursley pretended she didn’t have a sister, because her sister and her good-for-nothing husband were as unDursleyish as it was possible to be. The Dursleys shuddered to think what the neighbors would say if the Potters arrived in the street. The Dursleys knew that the Potters had a small son, too, but they had never even seen him. This boy was another good reason for keeping the Potters away; they didn’t want Dudley mixing with a child like that.       When Mr. and Mrs. Dursley woke up on the dull, gray Tuesday our story starts, there was nothing about the cloudy sky outside to suggest that strange and mysterious things would soon be happening all over the country. Mr. Dursley hummed as he picked out his most boring tie for work, and Mrs. Dursley gossiped away happily as she wrestled a screaming Dudley into his high chair.       None of them noticed a large, tawny owl flutter past the window.       At half past eight, Mr. Dursley picked up his briefcase, pecked Mrs. Dursley on the cheek, and tried to kiss Dudley good-bye but missed, because Dudley was now having a tantrum and throwing his cereal at the walls. “Little tyke,” chortled Mr. Dursley as he left the house. He got into his car and backed out of number four’s drive.        It was on the corner of the street that he noticed the first sign of'}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PyPDF2\n",
    "from tqdm import tqdm  # for progress bars\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n",
    "    # Other potential text formatting functions can go here\n",
    "    return cleaned_text\n",
    "\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Opens a PDF file, reads its text content page by page, and collects statistics.\n",
    "\n",
    "    Parameters:\n",
    "        pdf_path (str): The file path to the PDF document to be opened and read.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing the page number\n",
    "        (adjusted), character count, word count, sentence count, token count, and the extracted text\n",
    "        for each page.\n",
    "    \"\"\"\n",
    "    with open(pdf_path, 'rb') as pdf_file:  # Open in binary mode\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        pages_and_texts = []\n",
    "        for page_number in tqdm(range(len(pdf_reader.pages))):  # Iterate through pages\n",
    "            page = pdf_reader.pages[page_number]\n",
    "            text = page.extract_text()\n",
    "            text = text_formatter(text)\n",
    "            pages_and_texts.append({\"page_number\": page_number,  # No adjustment needed here\n",
    "                                    \"page_char_count\": len(text),\n",
    "                                    \"page_word_count\": len(text.split(\" \")),\n",
    "                                    \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                    \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars\n",
    "                                    \"text\": text})\n",
    "        return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path)\n",
    "pages_and_texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get a random sample of the pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 80,\n",
       "  'page_char_count': 2269,\n",
       "  'page_word_count': 479,\n",
       "  'page_sentence_count_raw': 20,\n",
       "  'page_token_count': 567.25,\n",
       "  'text': 'Hagrid’s big hairy face beamed over the sea of heads.       “C’mon, follow me — any more firs’ years? Mind yer step, now! Firs’ years follow me!”       Slipping and stumbling, they followed Hagrid down what seemed to be a steep, narrow path. It was so dark on either side of them that Harry thought there must be thick trees there. Nobody spoke much. Neville, the boy who kept losing his toad, sniffed once or twice.       “Yeh’ll get yer firs’ sight o’ Hogwarts in a sec,” Hagrid called over his shoulder, “jus’ round this bend here.”       There was a loud “Oooooh!”       The narrow path had opened suddenly onto the edge of a great black lake. Perched atop a high mountain on the other side, its windows sparkling in the starry sky, was a vast castle with many turrets and towers.       “No more’n four to a boat!” Hagrid called, pointing to a fleet of little boats sitting in the water by the shore. Harry and Ron were followed into their boat by Neville and Hermione.       “Everyone in?” shouted Hagrid, who had a boat to himself. “Right then — FORWARD!”       And the fleet of little boats moved off all at once, gliding across the lake, which was as smooth as glass. Everyone was silent, staring up at the great castle overhead. It towered over them as they sailed nearer and nearer to the cliff on which it stood.       “Heads down!” yelled Hagrid as the first boats reached the cliff; they all bent their heads and the little boats carried them through a curtain of ivy that hid a wide opening in the cliff face. They were carried along a dark tunnel, which seemed to be taking them right underneath the castle, until they reached a kind of underground harbor, where they clambered out onto rocks and pebbles.       “Oy, you there! Is this your toad?” said Hagrid, who was checking the boats as people climbed out of them.       “Trevor!” cried Neville blissfully, holding out his hands. Then they clambered up a passageway in the rock after Hagrid’s lamp, coming out at last onto smooth, damp grass right in the shadow of the castle.       They walked up a flight of stone steps and crowded around the huge, oak front door.       “Everyone here? You there, still got yer toad?”       Hagrid raised a gigantic fist and knocked three times on the castle door.'},\n",
       " {'page_number': 4,\n",
       "  'page_char_count': 2390,\n",
       "  'page_word_count': 471,\n",
       "  'page_sentence_count_raw': 33,\n",
       "  'page_token_count': 597.5,\n",
       "  'text': 'CHAPTER ONE   THE BOY WHO LIVED          M  r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense.       Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.       The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn’t think they could bear it if anyone found out about the Potters. Mrs. Potter was Mrs. Dursley’s sister, but they hadn’t met for several years; in fact, Mrs. Dursley pretended she didn’t have a sister, because her sister and her good-for-nothing husband were as unDursleyish as it was possible to be. The Dursleys shuddered to think what the neighbors would say if the Potters arrived in the street. The Dursleys knew that the Potters had a small son, too, but they had never even seen him. This boy was another good reason for keeping the Potters away; they didn’t want Dudley mixing with a child like that.       When Mr. and Mrs. Dursley woke up on the dull, gray Tuesday our story starts, there was nothing about the cloudy sky outside to suggest that strange and mysterious things would soon be happening all over the country. Mr. Dursley hummed as he picked out his most boring tie for work, and Mrs. Dursley gossiped away happily as she wrestled a screaming Dudley into his high chair.       None of them noticed a large, tawny owl flutter past the window.       At half past eight, Mr. Dursley picked up his briefcase, pecked Mrs. Dursley on the cheek, and tried to kiss Dudley good-bye but missed, because Dudley was now having a tantrum and throwing his cereal at the walls. “Little tyke,” chortled Mr. Dursley as he left the house. He got into his car and backed out of number four’s drive.        It was on the corner of the street that he noticed the first sign of'},\n",
       " {'page_number': 3,\n",
       "  'page_char_count': 44,\n",
       "  'page_word_count': 9,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 11.0,\n",
       "  'text': \"HP 1 - Harry Potter and the Sorcerer's Stone\"}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.sample(pages_and_texts, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some stats on the text\n",
    "\n",
    "Many embedding models have limits on the size of texts they can ingest, for example, the [`sentence-transformers`](https://www.sbert.net/docs/pretrained_models.html) model [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) has an input size of 384 tokens.\n",
    "\n",
    "Many embedding models have limits on the size of texts they can ingest, for example, the [`sentence-transformers`](https://www.sbert.net/docs/pretrained_models.html) model [`all-distilroberta-v1`](    https://huggingface.co/sentence-transformers/all-distilroberta-v1) has an input size of 384 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>143</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>35.75</td>\n",
       "      <td>HP 1 - Harry Potter and the Sorcerer's Stone H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>11.00</td>\n",
       "      <td>HP 1 - Harry Potter and the Sorcerer's Stone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2390</td>\n",
       "      <td>471</td>\n",
       "      <td>33</td>\n",
       "      <td>597.50</td>\n",
       "      <td>CHAPTER ONE   THE BOY WHO LIVED          M  r....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "0            0                0                1                        1   \n",
       "1            1                0                1                        1   \n",
       "2            2              143               33                        2   \n",
       "3            3               44                9                        1   \n",
       "4            4             2390              471                       33   \n",
       "\n",
       "   page_token_count                                               text  \n",
       "0              0.00                                                     \n",
       "1              0.00                                                     \n",
       "2             35.75  HP 1 - Harry Potter and the Sorcerer's Stone H...  \n",
       "3             11.00       HP 1 - Harry Potter and the Sorcerer's Stone  \n",
       "4            597.50  CHAPTER ONE   THE BOY WHO LIVED          M  r....  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>221.00</td>\n",
       "      <td>221.00</td>\n",
       "      <td>221.00</td>\n",
       "      <td>221.0</td>\n",
       "      <td>221.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>110.00</td>\n",
       "      <td>2049.02</td>\n",
       "      <td>431.28</td>\n",
       "      <td>21.6</td>\n",
       "      <td>512.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>63.94</td>\n",
       "      <td>761.61</td>\n",
       "      <td>158.89</td>\n",
       "      <td>9.0</td>\n",
       "      <td>190.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>55.00</td>\n",
       "      <td>2101.00</td>\n",
       "      <td>443.00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>525.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>110.00</td>\n",
       "      <td>2315.00</td>\n",
       "      <td>493.00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>578.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>165.00</td>\n",
       "      <td>2462.00</td>\n",
       "      <td>518.00</td>\n",
       "      <td>26.0</td>\n",
       "      <td>615.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>220.00</td>\n",
       "      <td>2901.00</td>\n",
       "      <td>564.00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>725.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count       221.00           221.00           221.00                    221.0   \n",
       "mean        110.00          2049.02           431.28                     21.6   \n",
       "std          63.94           761.61           158.89                      9.0   \n",
       "min           0.00             0.00             1.00                      1.0   \n",
       "25%          55.00          2101.00           443.00                     19.0   \n",
       "50%         110.00          2315.00           493.00                     23.0   \n",
       "75%         165.00          2462.00           518.00                     26.0   \n",
       "max         220.00          2901.00           564.00                     45.0   \n",
       "\n",
       "       page_token_count  \n",
       "count            221.00  \n",
       "mean             512.25  \n",
       "std              190.40  \n",
       "min                0.00  \n",
       "25%              525.25  \n",
       "50%              578.75  \n",
       "75%              615.50  \n",
       "max              725.25  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get stats\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average token count per page is 512.\n",
    "\n",
    "For this particular use case, it means we could embed an average whole page with the `all-distilroberta-v1` model (this model has an input capacity of 512)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further text processing (splitting pages into sentences)\n",
    "\n",
    "`Ingest text -> split it into groups/chunks -> embed the groups/chunks -> use the embeddings`\n",
    "\n",
    "Some options for splitting text into sentences:\n",
    "\n",
    "1. Split into sentences with simple rules (e.g. split on \". \" with `text = text.split(\". \")`, like we did above).\n",
    "2. Split into sentences with a natural language processing (NLP) library such as [spaCy](https://spacy.io/) or [nltk](https://www.nltk.org/).\n",
    "\n",
    "Why split into sentences?\n",
    "\n",
    "* Easier to handle than larger pages of text (especially if pages are densely filled with text).\n",
    "* Can get specific and find out which group of sentences were used to help within a RAG pipeline.\n",
    "\n",
    "> **Resource:** See [spaCy install instructions](https://spacy.io/usage). \n",
    "\n",
    "Let's use spaCy to break our text into sentences since it's likely a bit more robust than just using `text.split(\". \")`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This is a sentence., This another sentence.]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English # see https://spacy.io/usage for install instructions\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Add a sentencizer pipeline, see https://spacy.io/api/sentencizer/ \n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Create a document instance as an example\n",
    "doc = nlp(\"This is a sentence. This another sentence.\")\n",
    "assert len(list(doc.sents)) == 2\n",
    "\n",
    "# Access the sentences of the document\n",
    "list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221/221 [00:00<00:00, 300.42it/s]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "    \n",
    "    # Make sure all sentences are strings\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "    \n",
    "    # Count the sentences \n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 166,\n",
       "  'page_char_count': 2312,\n",
       "  'page_word_count': 495,\n",
       "  'page_sentence_count_raw': 23,\n",
       "  'page_token_count': 578.0,\n",
       "  'text': 'in the library?”       Hagrid shuffled into view, hiding something behind his back. He looked very out of place in his moleskin overcoat.       “Jus’ lookin’,” he said, in a shifty voice that got their interest at once. “An’ what’re you lot up ter?” He looked suddenly suspicious. “Yer not still lookin’ fer Nicolas Flamel, are yeh?”       “Oh, we found out who he is ages ago,” said Ron impressively. “And we know what that dog’s guarding, it’s a Sorcerer’s St—”       “Shhhh!” Hagrid looked around quickly to see if anyone was listening. “Don’ go shoutin’ about it, what’s the matter with yeh?”       “There are a few things we wanted to ask you, as a matter of fact,” said Harry, “about what’s guarding the Stone apart from Fluffy —”       “SHHHH!” said Hagrid again. “Listen — come an’ see me later, I’m not promisin’ I’ll tell yeh anythin’, mind, but don’ go rabbitin’ about it in here, students aren’ s’pposed ter know. They’ll think I’ve told yeh —”       “See you later, then,” said Harry.       Hagrid shuffled off.       “What was he hiding behind his back?” said Hermione thoughtfully.       “Do you think it had anything to do with the Stone?”       “I’m going to see what section he was in,” said Ron, who’d had enough of working. He came back a minute later with a pile of books in his arms and slammed them down on the table.       “Dragons!” he whispered. “Hagrid was looking up stuff about dragons! Look at these: Dragon Species of Great Britain and Ireland; From Egg to Inferno, A Dragon Keeper’s Guide.”       “Hagrid’s always wanted a dragon, he told me so the first time I ever met him, “ said Harry.       “But it’s against our laws,” said Ron. “Dragon breeding was outlawed by the Warlocks’ Convention of 1709, everyone knows that. It’s hard to stop Muggles from noticing us if we’re keeping dragons in the back garden — anyway, you can’t tame dragons, it’s dangerous. You should see the burns Charlie’s got off wild ones in Romania.”       “But there aren’t wild dragons in Britain?” said Harry.       “Of course there are,” said Ron. “Common Welsh Green and Hebridean Blacks. The Ministry of Magic has a job hushing them up, I can tell you. Our kind have to keep putting spells on Muggles who’ve spotted them, to make them forget.”       “So what on earth’s Hagrid up to?” said Hermione.',\n",
       "  'sentences': ['in the library?”',\n",
       "   '      Hagrid shuffled into view, hiding something behind his back.',\n",
       "   'He looked very out of place in his moleskin overcoat.',\n",
       "   '      “Jus’ lookin’,” he said, in a shifty voice that got their interest at once. “',\n",
       "   'An’ what’re you lot up ter?”',\n",
       "   'He looked suddenly suspicious. “',\n",
       "   'Yer not still lookin’ fer Nicolas Flamel, are yeh?”',\n",
       "   '      “Oh, we found out who he is ages ago,” said Ron impressively. “',\n",
       "   'And we know what that dog’s guarding, it’s a Sorcerer’s St—”       “Shhhh!”',\n",
       "   'Hagrid looked around quickly to see if anyone was listening. “',\n",
       "   'Don’ go shoutin’ about it, what’s the matter with yeh?”',\n",
       "   '      “There are a few things we wanted to ask you, as a matter of fact,” said Harry, “about what’s guarding the Stone apart from Fluffy —”       “SHHHH!”',\n",
       "   'said Hagrid again. “',\n",
       "   'Listen — come an’ see me later, I’m not promisin’ I’ll tell yeh anythin’, mind, but don’ go rabbitin’ about it in here, students aren’ s’pposed ter know.',\n",
       "   'They’ll think I’ve told yeh —”       “See you later, then,” said Harry.',\n",
       "   '      Hagrid shuffled off.',\n",
       "   '      “What was he hiding behind his back?”',\n",
       "   'said Hermione thoughtfully.',\n",
       "   '      “Do you think it had anything to do with the Stone?”',\n",
       "   '      “I’m going to see what section he was in,” said Ron, who’d had enough of working.',\n",
       "   'He came back a minute later with a pile of books in his arms and slammed them down on the table.',\n",
       "   '      “Dragons!”',\n",
       "   'he whispered. “',\n",
       "   'Hagrid was looking up stuff about dragons!',\n",
       "   'Look at these: Dragon Species of Great Britain and Ireland; From Egg to Inferno, A Dragon Keeper’s Guide.”',\n",
       "   '      “Hagrid’s always wanted a dragon, he told me so the first time I ever met him, “ said Harry.',\n",
       "   '      “But it’s against our laws,” said Ron. “',\n",
       "   'Dragon breeding was outlawed by the Warlocks’ Convention of 1709, everyone knows that.',\n",
       "   'It’s hard to stop Muggles from noticing us if we’re keeping dragons in the back garden — anyway, you can’t tame dragons, it’s dangerous.',\n",
       "   'You should see the burns Charlie’s got off wild ones in Romania.”',\n",
       "   '      “But there aren’t wild dragons in Britain?”',\n",
       "   'said Harry.',\n",
       "   '      “Of course there are,” said Ron. “',\n",
       "   'Common Welsh Green and Hebridean Blacks.',\n",
       "   'The Ministry of Magic has a job hushing them up, I can tell you.',\n",
       "   'Our kind have to keep putting spells on Muggles who’ve spotted them, to make them forget.”',\n",
       "   '      “So what on earth’s Hagrid up to?”',\n",
       "   'said Hermione.'],\n",
       "  'page_sentence_count_spacy': 38}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect an example\n",
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>221.00</td>\n",
       "      <td>221.00</td>\n",
       "      <td>221.00</td>\n",
       "      <td>221.0</td>\n",
       "      <td>221.00</td>\n",
       "      <td>221.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>110.00</td>\n",
       "      <td>2049.02</td>\n",
       "      <td>431.28</td>\n",
       "      <td>21.6</td>\n",
       "      <td>512.25</td>\n",
       "      <td>29.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>63.94</td>\n",
       "      <td>761.61</td>\n",
       "      <td>158.89</td>\n",
       "      <td>9.0</td>\n",
       "      <td>190.40</td>\n",
       "      <td>12.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>55.00</td>\n",
       "      <td>2101.00</td>\n",
       "      <td>443.00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>525.25</td>\n",
       "      <td>25.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>110.00</td>\n",
       "      <td>2315.00</td>\n",
       "      <td>493.00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>578.75</td>\n",
       "      <td>33.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>165.00</td>\n",
       "      <td>2462.00</td>\n",
       "      <td>518.00</td>\n",
       "      <td>26.0</td>\n",
       "      <td>615.50</td>\n",
       "      <td>37.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>220.00</td>\n",
       "      <td>2901.00</td>\n",
       "      <td>564.00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>725.25</td>\n",
       "      <td>51.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count       221.00           221.00           221.00                    221.0   \n",
       "mean        110.00          2049.02           431.28                     21.6   \n",
       "std          63.94           761.61           158.89                      9.0   \n",
       "min           0.00             0.00             1.00                      1.0   \n",
       "25%          55.00          2101.00           443.00                     19.0   \n",
       "50%         110.00          2315.00           493.00                     23.0   \n",
       "75%         165.00          2462.00           518.00                     26.0   \n",
       "max         220.00          2901.00           564.00                     45.0   \n",
       "\n",
       "       page_token_count  page_sentence_count_spacy  \n",
       "count            221.00                     221.00  \n",
       "mean             512.25                      29.63  \n",
       "std              190.40                      12.15  \n",
       "min                0.00                       0.00  \n",
       "25%              525.25                      25.00  \n",
       "50%              578.75                      33.00  \n",
       "75%              615.50                      37.00  \n",
       "max              725.25                      51.00  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking our sentences together\n",
    "\n",
    "Let's take a step to break down our list of sentences/text into smaller chunks.\n",
    "\n",
    "As you might've guessed, this process is referred to as **chunking**.\n",
    "\n",
    "Why do we do this?\n",
    "\n",
    "1. Easier to manage similar sized chunks of text.\n",
    "2. Don't overload the embedding models capacity for tokens (e.g. if an embedding model has a capacity of 512 tokens, there could be information loss if you try to embed a sequence of 550+ tokens).\n",
    "3. Our LLM context window (the amount of tokens an LLM can take in) may be limited and requires compute power so we want to make sure we're using it as well as possible.\n",
    "\n",
    "Something to note is that there are many different ways emerging for creating chunks of information/text.\n",
    "\n",
    "For now, we're going to keep it simple and break our pages of sentences into groups of 20 (this number is arbitrary and can be changed, I just picked it because it seemed to line up well with our embedding model capacity of 512).\n",
    "\n",
    "On average each of our pages has 10 sentences.\n",
    "\n",
    "And an average total of 512 tokens per page.\n",
    "\n",
    "So our groups of 20 sentences will also be ~250 tokens long.\n",
    "\n",
    "This gives us plenty of room for the text to embedded by our `all-distilroberta-v1` model (it has a capacity of 512 tokens).\n",
    "\n",
    "To split our groups of sentences into chunks of 10 or less, let's create a function which accepts a list as input and recursively breaks into down into sublists of a specified size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221/221 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define split size to turn groups of sentences into chunks\n",
    "num_sentence_chunk_size = 25\n",
    "\n",
    "# Create a function that recursively splits a list into desired sizes\n",
    "def split_list(input_list: list, \n",
    "               slice_size: int) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Splits the input_list into sublists of size slice_size (or as close as possible).\n",
    "\n",
    "    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n",
    "    \"\"\"\n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "# Loop through pages and texts and split sentences into chunks\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                         slice_size=num_sentence_chunk_size)\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 98,\n",
       "  'page_char_count': 2447,\n",
       "  'page_word_count': 495,\n",
       "  'page_sentence_count_raw': 24,\n",
       "  'page_token_count': 611.75,\n",
       "  'text': 'Hedwig. Hagrid   Harry borrowed Ron’s quill, scribbled Yes, please, see you later on the back of the note, and sent Hedwig off again.       It was lucky that Harry had tea with Hagrid to look forward to, because the Potions lesson turned out to be the worst thing that had happened to him so far.       At the start-of-term banquet, Harry had gotten the idea that Professor Snape disliked him. By the end of the first Potions lesson, he knew he’d been wrong. Snape didn’t dislike Harry — he hated him.       Potions lessons took place down in one of the dungeons. It was colder here than up in the main castle, and would have been quite creepy enough without the pickled animals floating in glass jars all around the walls.       Snape, like Flitwick, started the class by taking the roll call, and like Flitwick, he paused at Harry’s name.       “Ah, Yes,” he said softly, “Harry Potter. Our new — celebrity.”       Draco Malfoy and his friends Crabbe and Goyle sniggered behind their hands. Snape finished calling the names and looked up at the class. His eyes were black like Hagrid’s, but they had none of Hagrid’s warmth. They were cold and empty and made you think of dark tunnels.       “You are here to learn the subtle science and exact art of potionmaking,” he began. He spoke in barely more than a whisper, but they caught every word — like Professor McGonagall, Snape had the gift of keeping a class silent without effort. “As there is little foolish wand-waving here, many of you will hardly believe this is magic. I don’t expect you will really understand the beauty of the softly simmering cauldron with its shimmering fumes, the delicate power of liquids that creep through human veins, bewitching the mind, ensnaring the senses.…I can teach you how to bottle fame, brew glory, even stopper death — if you aren’t as big a bunch of dunderheads as I usually have to teach.”       More silence followed this little speech. Harry and Ron exchanged looks with raised eyebrows. Hermione Granger was on the edge of her seat and looked desperate to start proving that she wasn’t a dunderhead.       “Potter!” said Snape suddenly. “What would I get if I added powdered root of asphodel to an infusion of wormwood?”        Powdered root of what to an infusion of what? Harry glanced at Ron, who looked as stumped as he was; Hermione’s hand had shot into the air.       “I don’t know, sir,” said Harry.       Snape’s lips curled into a sneer.',\n",
       "  'sentences': ['Hedwig.',\n",
       "   'Hagrid   Harry borrowed Ron’s quill, scribbled Yes, please, see you later on the back of the note, and sent Hedwig off again.',\n",
       "   '      It was lucky that Harry had tea with Hagrid to look forward to, because the Potions lesson turned out to be the worst thing that had happened to him so far.',\n",
       "   '      At the start-of-term banquet, Harry had gotten the idea that Professor Snape disliked him.',\n",
       "   'By the end of the first Potions lesson, he knew he’d been wrong.',\n",
       "   'Snape didn’t dislike Harry — he hated him.',\n",
       "   '      Potions lessons took place down in one of the dungeons.',\n",
       "   'It was colder here than up in the main castle, and would have been quite creepy enough without the pickled animals floating in glass jars all around the walls.',\n",
       "   '      Snape, like Flitwick, started the class by taking the roll call, and like Flitwick, he paused at Harry’s name.',\n",
       "   '      “Ah, Yes,” he said softly, “Harry Potter.',\n",
       "   'Our new — celebrity.”',\n",
       "   '      Draco Malfoy and his friends Crabbe and Goyle sniggered behind their hands.',\n",
       "   'Snape finished calling the names and looked up at the class.',\n",
       "   'His eyes were black like Hagrid’s, but they had none of Hagrid’s warmth.',\n",
       "   'They were cold and empty and made you think of dark tunnels.',\n",
       "   '      “You are here to learn the subtle science and exact art of potionmaking,” he began.',\n",
       "   'He spoke in barely more than a whisper, but they caught every word — like Professor McGonagall, Snape had the gift of keeping a class silent without effort. “',\n",
       "   'As there is little foolish wand-waving here, many of you will hardly believe this is magic.',\n",
       "   'I don’t expect you will really understand the beauty of the softly simmering cauldron with its shimmering fumes, the delicate power of liquids that creep through human veins, bewitching the mind, ensnaring the senses.…I can teach you how to bottle fame, brew glory, even stopper death — if you aren’t as big a bunch of dunderheads as I usually have to teach.”',\n",
       "   '      More silence followed this little speech.',\n",
       "   'Harry and Ron exchanged looks with raised eyebrows.',\n",
       "   'Hermione Granger was on the edge of her seat and looked desperate to start proving that she wasn’t a dunderhead.',\n",
       "   '      “Potter!”',\n",
       "   'said Snape suddenly. “',\n",
       "   'What would I get if I added powdered root of asphodel to an infusion of wormwood?”',\n",
       "   '       Powdered root of what to an infusion of what?',\n",
       "   'Harry glanced at Ron, who looked as stumped as he was; Hermione’s hand had shot into the air.',\n",
       "   '      “I don’t know, sir,” said Harry.',\n",
       "   '      Snape’s lips curled into a sneer.'],\n",
       "  'page_sentence_count_spacy': 29,\n",
       "  'sentence_chunks': [['Hedwig.',\n",
       "    'Hagrid   Harry borrowed Ron’s quill, scribbled Yes, please, see you later on the back of the note, and sent Hedwig off again.',\n",
       "    '      It was lucky that Harry had tea with Hagrid to look forward to, because the Potions lesson turned out to be the worst thing that had happened to him so far.',\n",
       "    '      At the start-of-term banquet, Harry had gotten the idea that Professor Snape disliked him.',\n",
       "    'By the end of the first Potions lesson, he knew he’d been wrong.',\n",
       "    'Snape didn’t dislike Harry — he hated him.',\n",
       "    '      Potions lessons took place down in one of the dungeons.',\n",
       "    'It was colder here than up in the main castle, and would have been quite creepy enough without the pickled animals floating in glass jars all around the walls.',\n",
       "    '      Snape, like Flitwick, started the class by taking the roll call, and like Flitwick, he paused at Harry’s name.',\n",
       "    '      “Ah, Yes,” he said softly, “Harry Potter.',\n",
       "    'Our new — celebrity.”',\n",
       "    '      Draco Malfoy and his friends Crabbe and Goyle sniggered behind their hands.',\n",
       "    'Snape finished calling the names and looked up at the class.',\n",
       "    'His eyes were black like Hagrid’s, but they had none of Hagrid’s warmth.',\n",
       "    'They were cold and empty and made you think of dark tunnels.',\n",
       "    '      “You are here to learn the subtle science and exact art of potionmaking,” he began.',\n",
       "    'He spoke in barely more than a whisper, but they caught every word — like Professor McGonagall, Snape had the gift of keeping a class silent without effort. “',\n",
       "    'As there is little foolish wand-waving here, many of you will hardly believe this is magic.',\n",
       "    'I don’t expect you will really understand the beauty of the softly simmering cauldron with its shimmering fumes, the delicate power of liquids that creep through human veins, bewitching the mind, ensnaring the senses.…I can teach you how to bottle fame, brew glory, even stopper death — if you aren’t as big a bunch of dunderheads as I usually have to teach.”',\n",
       "    '      More silence followed this little speech.',\n",
       "    'Harry and Ron exchanged looks with raised eyebrows.',\n",
       "    'Hermione Granger was on the edge of her seat and looked desperate to start proving that she wasn’t a dunderhead.',\n",
       "    '      “Potter!”',\n",
       "    'said Snape suddenly. “',\n",
       "    'What would I get if I added powdered root of asphodel to an infusion of wormwood?”'],\n",
       "   ['       Powdered root of what to an infusion of what?',\n",
       "    'Harry glanced at Ron, who looked as stumped as he was; Hermione’s hand had shot into the air.',\n",
       "    '      “I don’t know, sir,” said Harry.',\n",
       "    '      Snape’s lips curled into a sneer.']],\n",
       "  'num_chunks': 2}]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample an example from the group (note: many samples have only 1 chunk as they have <=10 sentences total)\n",
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "      <th>num_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>221.00</td>\n",
       "      <td>221.00</td>\n",
       "      <td>221.00</td>\n",
       "      <td>221.0</td>\n",
       "      <td>221.00</td>\n",
       "      <td>221.00</td>\n",
       "      <td>221.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>110.00</td>\n",
       "      <td>2049.02</td>\n",
       "      <td>431.28</td>\n",
       "      <td>21.6</td>\n",
       "      <td>512.25</td>\n",
       "      <td>29.63</td>\n",
       "      <td>1.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>63.94</td>\n",
       "      <td>761.61</td>\n",
       "      <td>158.89</td>\n",
       "      <td>9.0</td>\n",
       "      <td>190.40</td>\n",
       "      <td>12.15</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>55.00</td>\n",
       "      <td>2101.00</td>\n",
       "      <td>443.00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>525.25</td>\n",
       "      <td>25.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>110.00</td>\n",
       "      <td>2315.00</td>\n",
       "      <td>493.00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>578.75</td>\n",
       "      <td>33.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>165.00</td>\n",
       "      <td>2462.00</td>\n",
       "      <td>518.00</td>\n",
       "      <td>26.0</td>\n",
       "      <td>615.50</td>\n",
       "      <td>37.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>220.00</td>\n",
       "      <td>2901.00</td>\n",
       "      <td>564.00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>725.25</td>\n",
       "      <td>51.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count       221.00           221.00           221.00                    221.0   \n",
       "mean        110.00          2049.02           431.28                     21.6   \n",
       "std          63.94           761.61           158.89                      9.0   \n",
       "min           0.00             0.00             1.00                      1.0   \n",
       "25%          55.00          2101.00           443.00                     19.0   \n",
       "50%         110.00          2315.00           493.00                     23.0   \n",
       "75%         165.00          2462.00           518.00                     26.0   \n",
       "max         220.00          2901.00           564.00                     45.0   \n",
       "\n",
       "       page_token_count  page_sentence_count_spacy  num_chunks  \n",
       "count            221.00                     221.00      221.00  \n",
       "mean             512.25                      29.63        1.74  \n",
       "std              190.40                      12.15        0.47  \n",
       "min                0.00                       0.00        0.00  \n",
       "25%              525.25                      25.00        1.00  \n",
       "50%              578.75                      33.00        2.00  \n",
       "75%              615.50                      37.00        2.00  \n",
       "max              725.25                      51.00        3.00  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to get stats\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting each chunk into its own item\n",
    "\n",
    "We'd like to embed each chunk of sentences into its own numerical representation.\n",
    "\n",
    "So to keep things clean, let's create a new list of dictionaries each containing a single chunk of sentences with relative information such as page number as well statistics about each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221/221 [00:00<00:00, 8504.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Split each chunk into its own item\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_texts):\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "        \n",
    "        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "        # Get stats about the chunk\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "        \n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "# How many chunks do we have?\n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 186,\n",
       "  'sentence_chunk': 'He was still shaking.   “Snape wants the stone for Voldemort…and Voldemort’s waiting in the forest…and all this time we thought Snape just wanted to get rich….”   “Stop saying the name!”said Ron in a terrified whisper, as if he thought Voldemort could hear them.   Harry wasn’t listening.   “Firenze saved me, but he shouldn’t have done so.…Bane was furious… he was talking about interfering with what the planets say is going to happen…. They must show that Voldemort’s coming back.…Bane thinks Firenze should have let Voldemort kill me.…I suppose that’s written in the stars as well.”    “Will you stop saying the name!”Ron hissed.',\n",
       "  'chunk_char_count': 633,\n",
       "  'chunk_word_count': 112,\n",
       "  'chunk_token_count': 158.25}]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View a random sample\n",
    "random.sample(pages_and_chunks, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent!\n",
    "\n",
    "Now we've broken our whole textbook into chunks of 10 sentences or less as well as the page number they came from.\n",
    "\n",
    "This means we could reference a chunk of text and know its source.\n",
    "\n",
    "Let's get some stats about our chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>385.0</td>\n",
       "      <td>385.00</td>\n",
       "      <td>385.00</td>\n",
       "      <td>385.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>111.9</td>\n",
       "      <td>1144.65</td>\n",
       "      <td>216.45</td>\n",
       "      <td>286.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>63.8</td>\n",
       "      <td>662.89</td>\n",
       "      <td>124.23</td>\n",
       "      <td>165.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.0</td>\n",
       "      <td>42.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>10.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>56.0</td>\n",
       "      <td>616.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>154.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>112.0</td>\n",
       "      <td>1135.00</td>\n",
       "      <td>213.00</td>\n",
       "      <td>283.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>168.0</td>\n",
       "      <td>1660.00</td>\n",
       "      <td>310.00</td>\n",
       "      <td>415.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>220.0</td>\n",
       "      <td>2761.00</td>\n",
       "      <td>515.00</td>\n",
       "      <td>690.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  chunk_char_count  chunk_word_count  chunk_token_count\n",
       "count        385.0            385.00            385.00             385.00\n",
       "mean         111.9           1144.65            216.45             286.16\n",
       "std           63.8            662.89            124.23             165.72\n",
       "min            2.0             42.00              8.00              10.50\n",
       "25%           56.0            616.00            116.00             154.00\n",
       "50%          112.0           1135.00            213.00             283.75\n",
       "75%          168.0           1660.00            310.00             415.00\n",
       "max          220.0           2761.00            515.00             690.25"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get stats about our chunks\n",
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm looks like some of our chunks have quite a low token count.\n",
    "\n",
    "How about we check for samples with less than 30 tokens (about the length of a sentence) and see if they are worth keeping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk token count: 11.0 | Text: HP 1 - Harry Potter and the Sorcerer's Stone\n",
      "Chunk token count: 11.0 | Text: HP 1 - Harry Potter and the Sorcerer's Stone\n",
      "Chunk token count: 11.0 | Text: HP 1 - Harry Potter and the Sorcerer's Stone\n",
      "Chunk token count: 11.0 | Text: HP 1 - Harry Potter and the Sorcerer's Stone\n",
      "Chunk token count: 12.0 | Text: Never told him what was in the letter Dumbledore\n",
      "Chunk token count: 11.0 | Text: HP 1 - Harry Potter and the Sorcerer's Stone\n",
      "Chunk token count: 11.0 | Text: HP 1 - Harry Potter and the Sorcerer's Stone\n",
      "Chunk token count: 11.0 | Text: HP 1 - Harry Potter and the Sorcerer's Stone\n",
      "Chunk token count: 11.0 | Text: HP 1 - Harry Potter and the Sorcerer's Stone\n",
      "Chunk token count: 11.0 | Text: HP 1 - Harry Potter and the Sorcerer's Stone\n",
      "Chunk token count: 23.25 | Text: Come on, hurry up.”   Harry glanced down at his broom. It was old and some of the twigs stuck\n",
      "Chunk token count: 18.75 | Text: The other Slytherins joined in.   “Shut up, Malfoy,” snapped Parvati Patil.\n",
      "Chunk token count: 11.0 | Text: HP 1 - Harry Potter and the Sorcerer's Stone\n",
      "Chunk token count: 16.0 | Text: Harry’s partner was Seamus Finnigan (which was a relief, because\n",
      "Chunk token count: 11.0 | Text: HP 1 - Harry Potter and the Sorcerer's Stone\n",
      "Chunk token count: 11.0 | Text: HP 1 - Harry Potter and the Sorcerer's Stone\n",
      "Chunk token count: 25.25 | Text: He stepped in front of it.   He had to clap his hands to his mouth to stop himself from screaming. He\n",
      "Chunk token count: 11.0 | Text: HP 1 - Harry Potter and the Sorcerer's Stone\n",
      "Chunk token count: 19.75 | Text: When’s he ever refereed a Quidditch match?He’s not going to be fair if we might\n",
      "Chunk token count: 10.5 | Text: “It’ll be gone by next Tuesday,” said Ron.\n",
      "Chunk token count: 11.0 | Text: HP 1 - Harry Potter and the Sorcerer's Stone\n",
      "Chunk token count: 13.75 | Text: They’d left the invisibility cloak on top of the tower.\n",
      "Chunk token count: 11.0 | Text: HP 1 - Harry Potter and the Sorcerer's Stone\n",
      "Chunk token count: 11.0 | Text: HP 1 - Harry Potter and the Sorcerer's Stone\n",
      "Chunk token count: 11.0 | Text: HP 1 - Harry Potter and the Sorcerer's Stone\n",
      "Chunk token count: 14.0 | Text: I’m going to have a lot of fun with Dudley this summer.…\n"
     ]
    }
   ],
   "source": [
    "# Show random chunks with under 30 tokens in length\n",
    "min_token_length = 30\n",
    "for row in df[df[\"chunk_token_count\"] <= min_token_length].iterrows():\n",
    "    print(f'Chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like many of these are headers and footers of different pages.\n",
    "\n",
    "They don't seem to offer too much information.\n",
    "\n",
    "Let's filter our DataFrame/list of dictionaries to only include chunks with over 30 tokens in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 2,\n",
       "  'sentence_chunk': \"HP 1 - Harry Potter and the Sorcerer's Stone Harry Potter and the Sorcerer's Stone   Harry Potter & The Sorcerer’s Stone   by J. K. Rowling\",\n",
       "  'chunk_char_count': 139,\n",
       "  'chunk_word_count': 29,\n",
       "  'chunk_token_count': 34.75},\n",
       " {'page_number': 4,\n",
       "  'sentence_chunk': 'CHAPTER ONE  THE BOY WHO LIVED     M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense.   Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.   The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn’t think they could bear it if anyone found out about the Potters. Mrs. Potter was Mrs. Dursley’s sister, but they hadn’t met for several years; in fact, Mrs. Dursley pretended she didn’t have a sister, because her sister and her good-for-nothing husband were as unDursleyish as it was possible to be. The Dursleys shuddered to think what the neighbors would say if the Potters arrived in the street. The Dursleys knew that the Potters had a small son, too, but they had never even seen him. This boy was another good reason for keeping the Potters away; they didn’t want Dudley mixing with a child like that.   When Mr. and Mrs. Dursley woke up on the dull, gray Tuesday our story starts, there was nothing about the cloudy sky outside to suggest that strange and mysterious things would soon be happening all over the country. Mr. Dursley hummed as he picked out his most boring tie for work, and Mrs. Dursley gossiped away happily as she wrestled a screaming Dudley into his high chair.   None of them noticed a large, tawny owl flutter past the window.   At half past eight, Mr. Dursley picked up his briefcase, pecked Mrs. Dursley on the cheek, and tried to kiss Dudley good-bye but missed, because Dudley was now having a tantrum and throwing his cereal at the walls. “Little tyke,” chortled Mr. Dursley as he left the house. He got into his car and backed out of number four’s drive.    It was on the corner of the street that he noticed the first sign of',\n",
       "  'chunk_char_count': 2359,\n",
       "  'chunk_word_count': 440,\n",
       "  'chunk_token_count': 589.75}]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n",
    "pages_and_chunks_over_min_token_len[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding our text chunks\n",
    "\n",
    "While humans understand text, machines understand numbers best.\n",
    "\n",
    "An [embedding](https://vickiboykis.com/what_are_embeddings/index.html) is a broad concept.\n",
    "\n",
    "But one of my favourite and simple definitions is \"a useful numerical representation\".\n",
    "\n",
    "The most powerful thing about modern embeddings is that they are *learned* representations.\n",
    "\n",
    "Meaning rather than directly mapping words/tokens/characters to numbers directly (e.g. `{\"a\": 0, \"b\": 1, \"c\": 3...}`), the numerical representation of tokens is learned by going through large corpuses of text and figuring out how different tokens relate to each other.\n",
    "\n",
    "Ideally, embeddings of text will mean that similar meaning texts have similar numerical representation.\n",
    "\n",
    "> **Note:** Most modern NLP models deal with \"tokens\" which can be considered as multiple different sizes and combinations of words and characters rather than always whole words or single characters. For example, the string `\"hello world!\"` gets mapped to the token values `{15339: b'hello', 1917: b' world', 0: b'!'}` using [Byte pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding) (or BPE via OpenAI's [`tiktoken`](https://github.com/openai/tiktoken) library). Google has a tokenization library called [SentencePiece](https://github.com/google/sentencepiece).\n",
    "\n",
    "Our goal is to turn each of our chunks into a numerical representation (an embedding vector, where a vector is a sequence of numbers arranged in order).\n",
    "\n",
    "Once our text samples are in embedding vectors, us humans will no longer be able to understand them.\n",
    "\n",
    "However, we don't need to.\n",
    "\n",
    "The embedding vectors are for our computers to understand.\n",
    "\n",
    "We'll use our computers to find patterns in the embeddings and then we can use their text mappings to further our understanding.\n",
    "\n",
    "Enough talking, how about we import a text embedding model and see what an embedding looks like.\n",
    "\n",
    "To do so, we'll use the [`sentence-transformers`](https://www.sbert.net/docs/installation.html) library which contains many pre-trained embedding models.\n",
    "\n",
    "Specifically, we'll get the `all-distilroberta-v1` model (you can see the model's intended use on the [Hugging Face model card](https://huggingface.co/sentence-transformers/all-mpnet-base-v2#intended-uses))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The Sentences Transformers library provides an easy and open-source way to create embeddings.\n",
      "Embedding: [-1.65902898e-02 -2.12335847e-02 -2.28640642e-02  8.87595234e-04\n",
      " -4.47793826e-02  3.57989669e-02 -4.02949713e-02  1.52442595e-02\n",
      " -3.82167995e-02 -3.07057966e-02 -2.67945845e-02  5.92993433e-03\n",
      " -6.57519512e-03 -3.78465466e-02 -1.22566912e-02  2.68598758e-02\n",
      "  6.79623261e-02 -9.76923853e-03 -2.20672823e-02 -3.65231708e-02\n",
      "  2.31705885e-03 -2.60507222e-02 -3.11338659e-02 -2.90707983e-02\n",
      " -2.35388279e-02  1.36790741e-02  8.21494237e-02  1.42724961e-02\n",
      "  4.72347206e-03  3.86894941e-02 -1.83376763e-02  2.75211912e-02\n",
      " -1.56715233e-02  2.34227683e-02  2.47984864e-02 -2.05263253e-02\n",
      "  4.73984703e-02  3.46599426e-03 -2.22720597e-02  1.92791745e-02\n",
      "  1.06708752e-02  8.44341069e-02 -6.89409822e-02  3.47610302e-02\n",
      "  2.49605570e-02 -1.25561710e-02 -7.43995607e-02 -6.67000487e-02\n",
      " -1.10511221e-02  9.09242500e-03  2.56566703e-02 -1.84278209e-02\n",
      " -3.60638537e-02  3.19261395e-04 -6.98118936e-03  3.67710032e-02\n",
      "  3.05326772e-03 -1.25956151e-03  5.74665591e-02 -9.76270717e-03\n",
      "  2.74340920e-02  2.40300782e-03 -4.35188189e-02 -5.59056029e-02\n",
      "  8.79028812e-02 -3.05436496e-02 -3.26718614e-02  8.08005687e-03\n",
      "  7.63156451e-03 -6.32095709e-02 -4.65786923e-03 -8.90410878e-03\n",
      " -2.12666560e-02 -4.87976596e-02 -1.55120911e-02 -7.12744668e-02\n",
      " -2.80335099e-02 -2.48307623e-02 -1.16104735e-02  4.48399521e-02\n",
      "  1.36126531e-02  5.69958575e-02 -2.14343723e-02 -3.29638831e-02\n",
      "  6.66818023e-02  8.31928030e-02  1.69849396e-02 -3.49969193e-02\n",
      " -1.38192857e-02  3.63680683e-02 -2.12522466e-02 -5.10459999e-03\n",
      "  4.50926684e-02 -3.16723548e-02 -1.04140742e-02  1.89510081e-02\n",
      "  7.92730786e-03 -4.54072170e-02 -2.86069755e-02 -4.46699932e-02\n",
      "  6.34449944e-02  3.82512808e-02  6.46742508e-02 -7.21961074e-03\n",
      "  1.99634358e-02  3.33112292e-02  1.48599863e-03 -6.66681305e-02\n",
      "  4.15424816e-02 -1.26806246e-02 -1.95443397e-03 -8.74419510e-02\n",
      "  1.05827656e-02 -5.36209680e-02  1.88885815e-02 -5.34995981e-02\n",
      "  1.42311193e-02 -1.61449090e-02  2.55851131e-02 -3.71361002e-02\n",
      "  1.07407896e-02  2.34211190e-03  1.96106620e-02 -2.10927520e-02\n",
      " -3.41919325e-02  1.31618651e-02  2.49171983e-02  5.30135110e-02\n",
      " -5.97527251e-02  2.99047045e-02  2.44085398e-02 -6.57337606e-02\n",
      " -1.94595810e-02  3.97100719e-03 -1.84610952e-02  1.02137364e-02\n",
      "  4.92299534e-02  4.01803628e-02 -5.81720807e-02  3.22821215e-02\n",
      "  1.59251094e-02  2.20937319e-02 -8.88023898e-03 -1.46671068e-02\n",
      "  4.40747738e-02  5.57756200e-02 -8.14599730e-03  2.24899333e-02\n",
      " -7.44708404e-02  5.01949638e-02  3.20252590e-02  2.92552412e-02\n",
      "  6.82563037e-02 -1.83754116e-02 -3.89370248e-02  4.28053811e-02\n",
      "  1.01679573e-02 -2.14874069e-03 -1.71415992e-02 -9.61070298e-04\n",
      "  3.34356688e-02  4.78158332e-02  4.14352641e-02  1.04095004e-02\n",
      "  7.15077817e-02 -4.44886088e-03  1.66255962e-02 -2.61753425e-03\n",
      "  8.54841061e-03 -1.32064754e-02  3.33485752e-02 -7.22421217e-04\n",
      "  2.09614541e-03  3.48579735e-02 -3.70360613e-02 -4.12736572e-02\n",
      "  3.27214226e-02  6.76215887e-02 -4.97362800e-02  6.30188314e-03\n",
      " -2.09218897e-02 -3.59745580e-03 -9.01658610e-02  2.69962866e-02\n",
      "  3.38005804e-04 -7.78834615e-03  1.09754666e-03 -6.24398962e-02\n",
      "  2.73952670e-02 -6.71192184e-02  8.62958878e-02 -5.30462665e-03\n",
      " -3.17746960e-02  6.07266836e-02 -7.24434108e-02 -5.10782525e-02\n",
      "  5.36884926e-02  1.88325346e-02  1.91424992e-02 -4.43821698e-02\n",
      " -1.83151606e-02  2.37659682e-02 -6.73269061e-03  2.14282726e-03\n",
      " -5.05957846e-03  6.13241270e-03  5.55954203e-02 -6.92871958e-02\n",
      " -3.60294878e-02  6.03556354e-03 -1.86803602e-02 -3.24501395e-02\n",
      " -5.35324402e-02 -3.99938598e-02 -1.26819555e-02  7.24313036e-03\n",
      "  6.19147085e-02 -1.52548216e-02  7.39991441e-02  1.84077099e-02\n",
      " -1.25280991e-02  4.05580644e-03 -8.45704600e-03 -6.73966706e-02\n",
      "  4.90405858e-02  2.11720169e-02  2.52998201e-03 -3.43066012e-03\n",
      " -3.46821290e-03 -5.57979122e-02  1.85510740e-02 -5.39022759e-02\n",
      "  6.16086088e-02 -2.34245928e-03 -5.61404321e-03  7.70045072e-02\n",
      " -2.39377022e-02 -2.00519003e-02  9.78184417e-02 -1.18508048e-01\n",
      "  6.23907112e-02 -6.06725104e-02  2.18434706e-02 -7.05090445e-03\n",
      "  1.36367287e-02 -3.34774889e-02  6.01171404e-02 -3.39568444e-02\n",
      "  3.20835263e-02 -9.17496905e-03 -4.67099575e-03 -4.53296211e-03\n",
      " -3.79474424e-02 -1.57829653e-02  2.81725619e-02 -1.82709191e-02\n",
      "  2.80573145e-02  2.95153894e-02  5.55474870e-02  2.11571455e-02\n",
      " -1.19119808e-02 -3.56343086e-03 -3.54808085e-02  6.30842671e-02\n",
      " -4.71609533e-02 -1.03341257e-02  3.15119289e-02  4.66167964e-02\n",
      "  3.41427885e-02  2.22997437e-03  1.26885925e-03  5.63309453e-02\n",
      " -1.61590800e-02  7.18228742e-02  1.95161924e-02  3.27513665e-02\n",
      "  4.21712957e-02 -2.32012160e-02 -3.53635824e-03  1.07753733e-02\n",
      "  6.08001426e-02 -5.02168536e-02  8.53808224e-02  3.85925993e-02\n",
      "  5.68417599e-03  2.44627837e-02 -9.70991049e-03  2.12806594e-02\n",
      "  5.40992133e-02 -1.69204921e-02 -4.20807526e-02 -1.75992940e-02\n",
      "  2.94377320e-02  1.93737354e-02 -2.28676163e-02 -1.52969854e-02\n",
      "  3.17724858e-04  5.02485037e-03  4.05740887e-02 -6.02938756e-02\n",
      " -8.26549158e-02  2.44237185e-02  2.36510970e-02 -9.75865591e-03\n",
      " -1.65280793e-02 -4.16171923e-02 -7.41903717e-03 -2.72681508e-02\n",
      " -8.24333206e-02  7.49535672e-03  2.03636512e-02 -5.19627519e-03\n",
      "  1.38725387e-02  4.44791988e-02  1.89377517e-02 -2.15027984e-02\n",
      " -3.10878437e-02  2.75182873e-02 -4.05452810e-02 -5.04118949e-02\n",
      " -1.81034636e-02 -4.64768894e-02  1.62545834e-02 -1.78411584e-02\n",
      "  5.11651533e-03  2.98931729e-03  4.14630165e-03  1.04815587e-02\n",
      "  1.26511790e-02 -1.96799040e-02 -2.68161558e-02 -1.67585630e-02\n",
      " -1.09539554e-01  1.21664256e-02 -1.90443825e-02 -1.40720308e-02\n",
      "  3.24433409e-02 -1.47619948e-03 -4.18384038e-02 -3.52827050e-02\n",
      " -2.12218110e-02 -5.00304885e-02  2.54620332e-02 -3.43445526e-03\n",
      " -4.94664209e-03  2.31812596e-02 -4.29280987e-03  2.52510477e-02\n",
      "  3.91770117e-02  1.16416654e-02 -5.66626787e-02  6.76572099e-02\n",
      " -4.02272344e-02  6.68368209e-03  1.29379993e-02 -2.08721776e-02\n",
      "  1.54581666e-02  2.06862222e-02  2.32384373e-02  1.05626639e-02\n",
      " -2.93277204e-03  4.49131988e-02  1.13074239e-02 -3.68735343e-02\n",
      " -5.77734970e-03  2.05111261e-02 -5.43949455e-02  1.55874910e-02\n",
      "  2.15910914e-04 -6.03020266e-02  7.32739735e-03  9.27544087e-02\n",
      "  1.90764572e-02 -1.08936571e-01 -3.39534953e-02  3.49137522e-02\n",
      " -6.15843534e-02  2.59175356e-02 -2.81201415e-02  1.08013768e-02\n",
      " -1.73878558e-02  2.67288871e-02 -2.34947484e-02  2.96678767e-03\n",
      " -2.82620490e-02 -2.48976741e-02  1.65644735e-02  2.14388575e-02\n",
      "  1.52718474e-03  3.17372265e-03 -4.27877996e-03  4.28946726e-02\n",
      " -1.17889727e-02 -3.11002117e-02 -6.02140045e-03  2.06835312e-03\n",
      "  4.90759313e-02 -8.38066684e-04  6.47955984e-02 -4.34059277e-02\n",
      " -2.65744496e-02  1.88361034e-02  3.14421132e-02  5.92746660e-02\n",
      "  8.93925428e-02 -2.23949477e-02  2.78147552e-02 -3.42483930e-02\n",
      "  9.37148836e-03 -1.42542943e-02  5.03243469e-02  3.32843661e-02\n",
      " -5.95628843e-03 -1.46273589e-02  8.59863535e-02 -2.77939234e-02\n",
      " -4.89724055e-03 -4.69218418e-02  4.77389917e-02  1.38374651e-02\n",
      " -3.19210142e-02  1.46269593e-02 -3.52768274e-03  1.70005020e-03\n",
      " -3.44218239e-02 -1.23978054e-04  7.25264428e-03 -3.64128835e-02\n",
      " -9.05740634e-03 -5.36335558e-02  3.21268104e-02 -1.24498922e-02\n",
      "  4.39323224e-02  1.21256420e-02 -1.52653810e-02 -4.27581817e-02\n",
      " -3.30331437e-02  5.76680638e-02  4.81416620e-02  4.29579727e-02\n",
      " -3.85161564e-02  5.49577512e-02  6.81387633e-03 -2.85678376e-02\n",
      " -5.08067608e-02 -2.58135777e-02  1.19096352e-04 -1.08942380e-02\n",
      " -3.02506201e-02  1.37060165e-01  2.53276769e-02  1.67166274e-02\n",
      " -2.46103927e-02 -3.35790478e-02  7.36371875e-02  5.35343308e-03\n",
      " -2.56847497e-02 -1.15197282e-02 -4.46400084e-02  1.97605044e-02\n",
      "  4.96246926e-02  8.75270925e-03  2.29254533e-02 -8.97951797e-03\n",
      "  6.73791906e-03 -6.23706281e-02  6.64516985e-02 -1.26701510e-02\n",
      " -7.39416778e-02 -3.11292317e-02 -1.94747001e-02 -3.53464857e-02\n",
      " -3.44317406e-02  5.70715219e-02 -2.81001311e-02  6.04563095e-02\n",
      "  9.95491818e-03  2.53261141e-02  1.70766655e-02 -2.90926080e-02\n",
      "  3.71675082e-02 -2.30671670e-02  1.65396165e-02 -3.47185438e-03\n",
      " -3.83209880e-03 -6.45110989e-03 -1.76797174e-02  2.24383604e-02\n",
      " -6.25337660e-02 -5.08883744e-02  3.26640904e-03 -3.73724550e-02\n",
      " -2.22620973e-03  7.02750161e-02  1.80380582e-33 -2.46157944e-02\n",
      "  2.96633616e-02 -4.90065813e-02 -4.63024825e-02  4.32886332e-02\n",
      "  1.21397628e-02 -3.04098353e-02 -7.60980370e-03  1.67999845e-02\n",
      " -1.88249145e-02 -7.94803724e-03  2.51319446e-02 -9.98358149e-03\n",
      "  9.61799696e-02  1.44412834e-02 -2.71646045e-02 -3.74628417e-02\n",
      "  3.80055122e-02 -1.70058440e-02 -2.28625014e-02 -2.87974961e-02\n",
      " -6.43526092e-02  6.29079491e-02  2.65992433e-03  1.00239394e-02\n",
      " -2.29380764e-02 -3.43021452e-02  1.21368347e-02 -1.36033976e-02\n",
      " -1.53423706e-03  1.49984946e-02 -4.03802376e-03  1.77040547e-02\n",
      " -5.72189838e-02 -2.61936355e-02 -6.48146495e-04 -2.34863739e-02\n",
      " -1.80978626e-02  2.18351427e-02  8.13952535e-02 -1.33310948e-02\n",
      " -2.74529960e-03  1.75661165e-02 -6.60762517e-03 -3.72255780e-02\n",
      "  6.96603348e-03  4.25388217e-02  9.12699383e-03  1.50642749e-02\n",
      " -6.64711744e-02  6.38256669e-02  3.25540826e-02  2.43313499e-02\n",
      " -2.89967265e-02 -2.45356448e-02  2.29389928e-02  1.88524816e-02\n",
      "  1.81427933e-02  5.52712530e-02  1.16127934e-02  3.68593112e-02\n",
      "  2.26230919e-02  2.11016554e-02 -1.75653715e-02  2.14300770e-02\n",
      "  8.82195905e-02  5.56881949e-02 -3.62775405e-04  4.46835011e-02\n",
      " -1.31794978e-02 -2.02224473e-03 -2.73702275e-02  1.81948058e-02\n",
      "  6.57817945e-02  1.21558933e-02  5.26763611e-02 -1.19964108e-02\n",
      "  4.54741083e-02  9.18522030e-02 -4.66426089e-02  1.95570011e-02\n",
      " -4.27304115e-03 -6.05209991e-02 -2.49374676e-02 -9.72746778e-03\n",
      " -3.58191989e-02  3.47406045e-02  3.17637175e-02  1.78962555e-02\n",
      "  3.19614597e-02  7.86358565e-02  7.38144368e-02  5.03782295e-02\n",
      " -2.34239129e-03 -1.17960470e-02 -2.12881397e-02 -4.97400239e-02\n",
      "  1.95843726e-02  4.52700211e-03 -9.74445045e-03  2.73531377e-02\n",
      " -3.60663747e-03 -1.71784926e-02  1.15288014e-03  3.64402905e-02\n",
      "  2.51818523e-02  4.12311032e-03 -9.47685540e-03  1.89576223e-02\n",
      " -3.56488302e-02  1.35556245e-02  1.76661760e-02 -2.77104191e-02\n",
      " -1.56310983e-02 -1.32765388e-02 -1.64837278e-02 -9.77750309e-03\n",
      "  4.09395806e-02  2.80736294e-02 -1.92174583e-03  3.32036614e-02\n",
      "  7.17339478e-03  3.09875738e-02  2.45732535e-02 -1.40837263e-02\n",
      "  4.60710563e-02 -7.71707594e-02  2.81387911e-04  1.53056346e-02\n",
      " -1.17439069e-02 -3.60022448e-02 -7.44421268e-03  2.01699464e-03\n",
      " -1.05232028e-02 -1.30388951e-02 -9.43030044e-03  9.14998911e-03\n",
      " -4.91609685e-02 -1.31351622e-02 -2.54385136e-02  3.78920995e-02\n",
      "  5.27274758e-02  5.30224927e-02 -2.73693725e-02 -5.83846346e-02\n",
      " -8.35748448e-04 -4.08380367e-02 -2.20838301e-02 -2.47002067e-03\n",
      " -2.95620896e-02 -3.98498215e-02 -2.95594335e-02 -2.97221858e-02\n",
      "  2.82623898e-03  3.26529518e-02 -1.57589675e-03 -2.52484027e-02\n",
      "  6.99253827e-02 -9.71484650e-03  7.38361198e-03  8.39954056e-03\n",
      "  2.65689529e-02 -1.41871525e-02  3.29822302e-02  2.78689489e-02\n",
      "  2.02164915e-03  4.30212868e-03  2.08594860e-03 -3.01189418e-03\n",
      "  3.47046852e-02 -5.15272878e-02  6.09128457e-03  5.30976504e-02\n",
      "  1.06535982e-02  4.68575545e-02 -5.24881706e-02  9.48055461e-03\n",
      "  2.48008762e-02  6.88202586e-03 -4.60824966e-02 -3.68830049e-03\n",
      " -1.65157523e-02 -5.96723706e-02 -4.90686707e-02  7.61382794e-03\n",
      " -2.66868658e-02  3.87521349e-02  1.21596539e-02 -4.41061296e-02\n",
      "  4.19057496e-02  2.33071800e-02  2.53892355e-02  1.82197113e-02\n",
      " -3.38201523e-02  5.17597310e-02 -4.46128361e-02  4.04000990e-02\n",
      "  3.54135633e-02 -5.19629382e-02 -1.64290220e-02 -6.14326121e-03\n",
      " -2.68836506e-02 -1.68111396e-03 -3.88330407e-02 -3.60925607e-02\n",
      " -1.16256624e-02 -1.67781289e-03  4.99083437e-02  8.89629312e-03\n",
      "  2.00272016e-02  4.78492398e-03  5.97883500e-02 -2.00939961e-02\n",
      "  9.85680986e-03 -3.74296792e-02 -2.83320192e-02  4.62172814e-02\n",
      "  9.54717398e-03  1.05645845e-03  4.71724644e-02 -3.03796176e-02\n",
      " -4.12398055e-02  2.59161387e-02 -5.53934947e-02 -1.23973573e-02\n",
      " -2.62891334e-02 -2.46955510e-02  5.47237322e-03 -2.29579695e-02\n",
      " -8.24911986e-03  3.26612405e-02 -1.32661639e-03 -3.25948559e-02\n",
      " -6.83793938e-03  9.00836010e-03  3.61583903e-02 -4.52284003e-03\n",
      "  5.26114069e-02  2.09962483e-02  5.12761548e-02  3.68080325e-02\n",
      "  5.75975794e-03  1.03558628e-02 -1.79288462e-02 -4.38263938e-02\n",
      " -1.49073275e-02 -5.21189198e-02  8.09496716e-02 -5.53296022e-02\n",
      " -1.97728593e-02 -2.89120767e-02 -2.01741401e-02 -4.53249700e-02\n",
      "  4.56787348e-02 -8.52142554e-03  3.39370854e-02  6.19854182e-02\n",
      " -1.49066225e-02 -5.07898219e-02  1.49201248e-02 -1.74803957e-02\n",
      "  5.09076081e-02  1.22595401e-02 -2.61788499e-02 -1.04630617e-02\n",
      " -3.30829173e-02  8.94264784e-04  3.43963541e-02 -1.12481331e-02\n",
      "  9.66506898e-02  3.80663946e-02 -5.51547809e-03 -6.65431900e-04]\n",
      "\n",
      "Sentence: Sentences can be embedded one by one or as a list of strings.\n",
      "Embedding: [-2.15228982e-02 -7.79802073e-03  9.25099012e-04 -8.50584731e-03\n",
      "  5.22884307e-03  3.85090448e-02 -4.49492745e-02  6.06165901e-02\n",
      " -2.77314056e-02 -1.29296072e-02 -1.68407417e-03 -6.83187228e-03\n",
      " -7.65867950e-03 -6.64654188e-03 -2.00638175e-02  3.87779400e-02\n",
      "  2.86662076e-02 -5.51781990e-03 -1.22674415e-02 -8.34266655e-03\n",
      " -6.48086937e-03 -2.40233131e-02  2.29458362e-02 -2.44883820e-02\n",
      " -2.05650106e-02  1.66674443e-02  7.21327737e-02  4.76790033e-02\n",
      " -2.81534735e-02  2.77372357e-02 -2.43528765e-02  4.89520505e-02\n",
      " -7.03037996e-03  1.64505206e-02  3.87898535e-02  2.32622176e-02\n",
      "  4.35343385e-02  1.20665366e-02 -1.42591121e-02 -1.50841838e-02\n",
      " -8.29565302e-02  9.12268832e-02 -1.90074556e-02  3.79963522e-03\n",
      "  1.63399484e-02  3.65550742e-02 -1.45577854e-02 -5.18743470e-02\n",
      " -3.24401297e-02  1.81982592e-02  7.85926636e-03  2.16432326e-02\n",
      " -4.97880168e-02 -3.61221679e-03 -8.56438559e-03 -1.03401374e-02\n",
      "  1.56821441e-02 -2.75810268e-02 -7.16333743e-03 -2.63017379e-02\n",
      "  2.79277060e-02 -2.71650348e-02 -5.32178767e-02  6.29210903e-04\n",
      "  1.08092651e-01  1.39153395e-02 -1.70661211e-02  4.38998900e-02\n",
      " -1.65359285e-02 -6.67736679e-02  5.87043539e-03 -4.08510603e-02\n",
      " -2.11900324e-02 -7.42749274e-02  3.41704674e-02 -7.69784749e-02\n",
      " -2.67933272e-02 -5.77680431e-02 -3.03256698e-02  1.41533343e-02\n",
      "  3.05690169e-02  4.23202477e-02 -5.77048361e-02  1.82859898e-02\n",
      "  7.29770139e-02 -7.90171791e-03  2.70543173e-02 -5.23664430e-02\n",
      " -2.88359970e-02  1.62492786e-02 -2.87909713e-02  1.40715353e-02\n",
      "  3.25558819e-02 -5.90184778e-02 -7.95009825e-03  2.42011733e-02\n",
      "  1.19027030e-02  2.36515626e-02 -1.82428081e-02 -1.71578974e-02\n",
      "  3.23837325e-02  2.70667449e-02  3.80730093e-03  2.97397492e-03\n",
      " -1.01050166e-02  6.14312254e-02 -2.95424648e-02 -2.06969455e-02\n",
      "  3.72565538e-02  6.88353367e-03  2.45685671e-02 -5.97646534e-02\n",
      "  2.93044839e-02 -5.21380603e-02  2.97159515e-02 -9.31652412e-02\n",
      "  3.93321216e-02 -4.78455052e-03  2.51197312e-02  5.04884636e-03\n",
      " -3.60987969e-02 -4.44360375e-02 -3.72305997e-02  1.74289364e-02\n",
      " -5.91451265e-02  1.69028938e-02  5.47571480e-02 -3.16726565e-02\n",
      " -5.32997064e-02  2.34307311e-02  4.89776209e-02 -2.35146731e-02\n",
      " -1.91505440e-02 -8.74457508e-02 -3.53405699e-02 -4.94928146e-03\n",
      " -2.81799864e-03  3.28886695e-03 -3.78609076e-02  2.49722507e-02\n",
      "  3.02903913e-02  1.35219013e-02 -1.50381345e-02 -3.52248885e-02\n",
      "  1.71383731e-02  5.30944653e-02 -1.75941829e-02  2.06144098e-02\n",
      "  1.39621301e-02  7.23014921e-02  4.50918404e-03  3.69534120e-02\n",
      " -3.25177722e-02 -1.52178062e-02  6.32713642e-03  4.00047265e-02\n",
      " -2.71663107e-02  1.08832838e-02  2.44987234e-02  8.65017809e-03\n",
      "  1.64620727e-02  4.74339398e-03  3.67232785e-02  4.96446677e-02\n",
      "  3.55654173e-02 -9.24846251e-03  4.04141508e-02  8.93776026e-03\n",
      " -6.94125192e-04 -8.61860055e-04  4.00994904e-02 -2.24048309e-02\n",
      " -5.76792359e-02 -1.21093169e-02 -5.08388579e-02 -3.50042023e-02\n",
      "  1.67205110e-02  1.16062863e-02 -9.73118376e-03  1.70614403e-02\n",
      "  1.10313306e-02  2.59382240e-02 -7.54773095e-02 -4.70512398e-02\n",
      "  4.50869370e-03 -3.29303071e-02  4.64596637e-02 -9.42940451e-03\n",
      " -2.56709214e-02 -9.90570709e-02  6.60109296e-02  4.08833362e-02\n",
      " -4.95852865e-02  1.68002266e-02 -8.46680328e-02 -7.29619116e-02\n",
      "  3.35790142e-02  4.76308214e-03  3.75376344e-02 -5.57547957e-02\n",
      "  1.70999914e-02 -1.16730547e-02  3.41019481e-02  1.68322865e-02\n",
      " -5.26350981e-04  1.89747866e-02 -4.86020334e-02 -3.50383632e-02\n",
      " -4.14872244e-02  1.45470852e-03  1.26351127e-02 -2.65011545e-02\n",
      " -3.41715962e-02  6.93242392e-03 -2.07681842e-02  4.01476100e-02\n",
      "  3.45804431e-02  1.08575337e-02  3.42816412e-02  4.23322655e-02\n",
      " -8.10069311e-03  2.96809785e-02 -9.39209014e-03 -6.63605658e-03\n",
      "  6.60046861e-02  4.29035397e-03 -2.06552688e-02  1.12097748e-02\n",
      " -5.04649021e-02 -4.92263958e-02  1.62704587e-02 -3.22788917e-02\n",
      "  2.61608884e-02  1.95173454e-02  1.29342675e-02  4.85146679e-02\n",
      " -5.42903021e-02  1.44990450e-02  9.99005139e-02 -6.95130676e-02\n",
      " -4.95595112e-03 -1.77037884e-02 -1.37647614e-02 -2.83440258e-02\n",
      " -2.88115293e-02 -5.70283532e-02  5.89221567e-02 -3.86230908e-02\n",
      "  3.35461758e-02 -7.74013996e-03  1.41369393e-02 -1.10670691e-02\n",
      " -4.21974994e-03  4.60349722e-03  5.66767491e-02 -4.88095842e-02\n",
      "  3.58184502e-02 -1.80671234e-02  1.01320811e-01  6.42055795e-02\n",
      "  6.08769478e-03 -1.58803444e-02 -1.42040448e-02  4.06201296e-02\n",
      " -4.84540723e-02 -4.94356491e-02  8.11407994e-03  3.31632607e-02\n",
      "  1.73655315e-03 -1.01410048e-02  2.96979174e-02  4.34657522e-02\n",
      " -1.34893470e-02  4.39849645e-02  2.26651523e-02  1.25344545e-02\n",
      "  1.22435912e-02 -2.05916297e-02 -1.40960496e-02 -2.75447965e-02\n",
      "  5.78651056e-02 -6.68787211e-02  4.98068668e-02  4.29513752e-02\n",
      "  9.85620916e-03  2.97745280e-02 -5.86009696e-02  6.54283464e-02\n",
      "  4.88063209e-02 -2.56016124e-02 -4.42865193e-02 -1.46858180e-02\n",
      "  8.60323384e-03  4.18894775e-02 -1.30106891e-02  2.27113180e-02\n",
      "  3.22510600e-02 -1.31252399e-02  2.10826583e-02 -7.98784196e-02\n",
      " -7.84291252e-02  4.83091325e-02 -1.15547050e-02 -2.18870174e-02\n",
      "  2.72850692e-02  3.70882861e-02  9.74850729e-03 -1.89277641e-02\n",
      "  2.95159277e-02  6.10503852e-02  3.81889269e-02  2.20022239e-02\n",
      " -8.77353922e-03  5.28390557e-02 -9.32123512e-03  2.20449161e-04\n",
      " -2.94645075e-02 -6.19591735e-02  9.71089583e-03 -5.11895828e-02\n",
      " -1.22394795e-02 -6.48608208e-02 -2.21565142e-02 -1.10507766e-02\n",
      " -1.30585488e-02  1.08915372e-02 -3.07445563e-02 -1.19031845e-02\n",
      "  2.09992938e-03 -7.57960603e-04  2.71390267e-02  5.65768443e-02\n",
      " -7.43712708e-02  3.97551991e-02 -6.20402675e-03 -5.52947372e-02\n",
      "  7.59455934e-03  2.46278029e-02 -4.62248847e-02 -2.81296242e-02\n",
      " -3.40412967e-02 -4.32912521e-02 -9.58360732e-03  5.90345170e-03\n",
      "  4.27316837e-02 -3.93412635e-02  2.56451778e-04 -7.79603934e-03\n",
      "  3.33294496e-02  2.86483485e-02 -2.92312484e-02  5.56678288e-02\n",
      "  1.58567633e-02  2.72279996e-02  2.86977664e-02 -3.81967165e-02\n",
      " -1.52876917e-02 -5.32363215e-03  4.07030806e-03 -1.52098213e-03\n",
      " -6.07552156e-02  1.57411005e-02  4.41372730e-02 -3.29356752e-02\n",
      "  4.18707263e-03  6.10776525e-03 -3.54036987e-02  5.82160577e-02\n",
      " -2.42292061e-02 -5.67969419e-02  2.46705562e-02  7.64016509e-02\n",
      " -1.70787666e-02 -8.25189874e-02 -2.99086627e-02  1.98063683e-02\n",
      " -2.88503412e-02  2.27361545e-02  2.93965526e-02  5.02186716e-02\n",
      " -3.07468381e-02 -3.90718430e-02 -2.60850564e-02  4.89783124e-04\n",
      "  2.36902665e-02 -2.07286999e-02  1.46895964e-02 -1.44212767e-02\n",
      "  1.02716144e-02 -2.69169658e-02  4.26815227e-02  2.74504516e-02\n",
      " -2.03525480e-02 -4.55363188e-03 -2.19547399e-03 -2.48928089e-02\n",
      "  4.17505316e-02 -2.15010755e-02  2.02234201e-02 -2.96150465e-02\n",
      "  1.02198878e-02  1.54044926e-02  4.15158309e-02 -1.72114815e-03\n",
      "  6.10230230e-02  2.95865946e-02 -2.41737626e-02 -2.43112054e-02\n",
      "  4.27502170e-02  3.17859873e-02  6.84548989e-02  5.86677678e-02\n",
      " -6.83040079e-03 -5.75580969e-02  7.16388300e-02 -1.79629009e-02\n",
      "  1.90173760e-02  2.67682667e-03  1.31722745e-02  1.91450566e-02\n",
      "  3.33502553e-02  9.32533201e-03 -1.70159899e-02 -4.18030098e-02\n",
      " -1.19661298e-02  4.38202731e-02  4.03314196e-02 -4.10567857e-02\n",
      " -4.77207527e-02 -3.52674648e-02  3.39080091e-03  2.58128438e-02\n",
      "  2.50301827e-02 -5.70061337e-03 -2.94603631e-02 -1.09285815e-02\n",
      " -2.56372266e-03  1.83977969e-02  8.23411420e-02  3.12244985e-02\n",
      " -4.09296006e-02  5.42510450e-02  4.01297994e-02  5.29822428e-03\n",
      " -2.42915358e-02 -3.08636446e-02 -7.65368575e-03 -4.57850937e-03\n",
      " -2.82634180e-02  9.64395031e-02  2.89543420e-02  1.92972310e-02\n",
      " -3.32763381e-02 -2.67283153e-03  7.48563558e-02  6.23942800e-02\n",
      "  5.96133480e-03 -4.64799143e-02 -3.09171025e-02 -2.77158455e-03\n",
      "  1.05406772e-02  1.54179642e-02 -1.44856535e-02  1.63917132e-02\n",
      "  1.67722348e-02 -7.82346502e-02  3.41065205e-03 -3.20805684e-02\n",
      " -4.43537608e-02 -6.45602215e-03 -7.08878338e-02 -5.70817851e-02\n",
      " -2.12429557e-02  2.34312285e-02  2.21958244e-03 -2.11620657e-03\n",
      " -3.05215619e-03  2.36532744e-02  2.54918635e-02  6.90297829e-03\n",
      "  1.75465718e-02  1.25617385e-02  1.19329235e-02 -1.95599999e-02\n",
      " -1.76153313e-02 -6.68683369e-03  8.70821159e-03  1.20763632e-03\n",
      " -3.56985144e-02 -7.65360519e-02  2.22751056e-03 -4.47666161e-02\n",
      " -5.80691639e-03  2.80035883e-02  2.06701167e-33 -3.86263952e-02\n",
      "  1.03980722e-02 -2.25318372e-02 -1.09205907e-02 -6.09315652e-03\n",
      "  2.17080303e-02 -2.52090879e-02  5.23333214e-02  1.69765600e-03\n",
      "  3.49956960e-03 -5.00398502e-03  2.47254390e-02 -8.19996931e-03\n",
      "  6.22178800e-02  3.39485183e-02 -1.66806369e-03 -3.13671529e-02\n",
      "  6.07062615e-02  7.01395283e-03 -1.58467628e-02 -1.31561207e-02\n",
      " -4.78530899e-02  6.53859274e-03 -2.00169142e-02  8.27857032e-02\n",
      " -4.54734042e-02 -7.13099763e-02  2.08531283e-02 -3.04002427e-02\n",
      "  2.69476809e-02 -6.07176078e-03 -3.58041422e-03 -7.44849117e-03\n",
      " -5.26576452e-02 -1.82651244e-02 -4.38513681e-02 -2.11511124e-02\n",
      " -5.12038171e-02  6.68710619e-02  2.58756466e-02 -4.72214445e-03\n",
      " -1.44625120e-02 -6.90015871e-03 -2.49098837e-02 -6.42965436e-02\n",
      " -9.10301413e-03  4.03396450e-02  3.03872898e-02  7.33511662e-03\n",
      "  1.08352127e-02  4.98617031e-02  5.19862734e-02  3.53986621e-02\n",
      " -1.49297277e-02 -2.20196657e-02  5.64400442e-02  4.46461588e-02\n",
      " -3.84628363e-02  7.66947195e-02  1.97097138e-02 -1.89003535e-02\n",
      "  9.06499550e-02  2.10484546e-02 -4.63209487e-03 -3.42289917e-02\n",
      "  9.64953974e-02  6.11828528e-02 -1.90013610e-02  5.07390406e-03\n",
      "  4.95525338e-02  2.97802947e-02 -8.91914591e-03  2.37727761e-02\n",
      "  1.40797468e-02 -1.92913052e-03  1.17767483e-01 -1.32040679e-02\n",
      "  9.90699977e-03  9.45470929e-02 -3.69254872e-02  3.55997533e-02\n",
      "  6.31540641e-03 -7.64536252e-03 -4.15868014e-02 -3.99596430e-03\n",
      "  1.33439520e-04  4.04367521e-02  2.39032861e-02  1.48134008e-02\n",
      "  3.84915024e-02  7.03489035e-02  7.89425001e-02  3.00962757e-02\n",
      " -1.90497682e-04  5.89191094e-02  3.11055351e-02 -5.67665473e-02\n",
      "  3.38916178e-03  1.35418074e-02 -2.90492475e-02  1.93993021e-02\n",
      "  7.96410348e-03  1.83577146e-02  4.42266697e-03 -1.61006546e-03\n",
      " -3.82846855e-02 -4.30544429e-02 -3.59540945e-03  1.16069615e-02\n",
      " -1.89123210e-02 -2.79755741e-02  5.61765544e-02 -3.34076509e-02\n",
      " -5.08807227e-02 -3.25226746e-02  7.83521775e-03 -1.91734023e-02\n",
      "  5.94736636e-02 -2.65357587e-02  4.53172810e-02 -1.80402212e-02\n",
      " -1.07418206e-02  2.63088699e-02 -1.75722428e-02 -9.31730785e-04\n",
      " -4.44799894e-03 -6.34835064e-02  3.49458004e-03  4.54389155e-02\n",
      " -4.12257314e-02 -4.84148152e-02 -1.71956196e-02 -1.80804431e-02\n",
      " -2.63073575e-03 -3.02607846e-02 -3.50830033e-02  4.20266530e-03\n",
      " -3.56156081e-02 -2.81791594e-02 -5.54149318e-03  2.38171499e-02\n",
      "  5.63822910e-02  7.33753890e-02 -2.24820655e-02  4.52477951e-03\n",
      " -3.31595056e-02 -1.97024774e-02 -3.40226144e-02 -3.33411805e-02\n",
      " -3.63468304e-02 -5.13177738e-03 -2.46759839e-02  2.00916063e-02\n",
      "  5.57936449e-03 -3.64194028e-02  1.40958382e-02 -4.64291833e-02\n",
      "  4.92615849e-02 -3.70536856e-02  3.66368244e-04 -2.25139912e-02\n",
      "  2.08797976e-02 -5.81128001e-02  1.50869116e-02 -5.48285805e-03\n",
      "  5.42319205e-04  3.68645117e-02 -2.39754431e-02 -4.31912392e-02\n",
      "  6.71679601e-02 -9.23241749e-02  1.88795831e-02  2.27905717e-02\n",
      "  3.32120247e-02  2.31583063e-02 -1.30487587e-02  1.34720271e-02\n",
      "  8.20852220e-02  1.06502511e-02 -2.23771539e-02  1.47919394e-02\n",
      "  6.35388587e-03 -3.28639783e-02 -1.71262550e-03  5.28082475e-02\n",
      " -8.25087279e-02 -2.32286565e-02  4.23677638e-02 -5.45634292e-02\n",
      "  3.36574391e-02  4.35426645e-02  1.71139184e-02  4.92438599e-02\n",
      " -1.37344263e-02  4.96943519e-02 -5.75561970e-02 -5.40280342e-03\n",
      " -1.26684434e-03 -1.43954083e-02 -2.18227953e-02 -9.99155361e-03\n",
      " -2.90870033e-02 -2.59137340e-02  1.26986895e-02 -3.19436309e-03\n",
      " -1.35016628e-02 -9.75760818e-03  3.63758542e-02 -4.36577760e-03\n",
      " -1.87006854e-02  5.22086769e-02  7.21984580e-02 -2.26308387e-02\n",
      "  2.07442529e-02 -1.39183095e-02 -1.34625938e-03  4.87105846e-02\n",
      "  1.36510516e-03  2.04825699e-02  3.57983895e-02 -2.05948818e-02\n",
      " -1.77338254e-02  4.42667902e-02 -1.00496702e-01 -6.87099900e-03\n",
      "  4.14285576e-03 -2.97053549e-02 -2.44097002e-02 -3.12399417e-02\n",
      "  6.18085731e-03 -9.84526426e-03 -2.07634475e-02 -1.75898504e-02\n",
      "  1.34774763e-02  2.12179348e-02  1.77283946e-03 -2.62890402e-02\n",
      "  4.51455675e-02  3.59200425e-02  4.77867238e-02  2.34301984e-02\n",
      "  3.03890537e-02  1.66282058e-02 -3.77523974e-02 -4.28887196e-02\n",
      "  2.04049051e-02 -4.32218332e-03  1.35475174e-02 -8.86799395e-02\n",
      " -7.46056950e-03 -2.47586723e-02  1.38286119e-02 -5.51470593e-02\n",
      "  3.51191387e-02 -4.21440415e-02  1.75776966e-02  1.46228485e-02\n",
      " -2.37191059e-02 -5.22530526e-02 -5.03631420e-02 -6.08381256e-02\n",
      "  7.92744290e-03  5.90820471e-03 -3.46167013e-02  1.71444453e-02\n",
      " -1.17022386e-02 -2.89281625e-02  1.58888511e-02  2.15968437e-04\n",
      "  9.39113945e-02  8.54471800e-05 -6.95311278e-02  2.00913139e-02]\n",
      "\n",
      "Sentence: Embeddings are one of the most powerful concepts in machine learning!\n",
      "Embedding: [-1.22164143e-02 -2.13698354e-02 -3.36170569e-03 -3.22685316e-02\n",
      " -2.41235606e-02  3.00755203e-02 -2.04258468e-02  5.20664379e-02\n",
      " -9.10869893e-03 -5.31128310e-02 -3.06588896e-02  3.09261288e-02\n",
      " -2.71168817e-02 -1.64833770e-03 -7.86010101e-02  2.21535508e-02\n",
      "  2.27728542e-02 -7.81786889e-02 -4.53958921e-02 -2.52812151e-02\n",
      " -1.61554608e-02 -1.60243995e-02 -3.33075486e-02 -7.18192533e-02\n",
      " -3.55768874e-02  1.38197836e-04  2.26320922e-02  6.60716090e-03\n",
      "  1.12220477e-02  2.19996572e-02 -1.02163563e-02  6.73651993e-02\n",
      " -6.05756268e-02  1.05325155e-01  4.37143892e-02 -6.48564147e-03\n",
      "  1.81881990e-02 -3.20585780e-02  3.92653234e-02  4.31282399e-03\n",
      " -5.75722754e-02  9.81879607e-02 -2.57260986e-02  1.18376203e-02\n",
      "  3.93918976e-02 -1.16888145e-02 -6.75629377e-02 -6.54780865e-02\n",
      "  4.90322988e-03  9.83750597e-02  1.89386085e-02 -1.14147160e-02\n",
      " -7.27443099e-02 -3.52954455e-02  9.96270776e-03 -2.42876913e-02\n",
      "  3.81819531e-02 -1.42911833e-03  4.06645015e-02 -1.83045324e-02\n",
      " -1.33975782e-02  4.29213382e-02  4.80985548e-03 -2.41242517e-02\n",
      "  1.01373367e-01 -2.90144961e-02  2.17544362e-02  1.14272023e-02\n",
      "  2.92580388e-02 -5.21350503e-02  4.07293625e-02 -3.20466310e-02\n",
      "  1.98586229e-02 -5.83529808e-02  7.97565747e-03 -7.88913667e-02\n",
      " -5.17887920e-02 -4.09288108e-02 -5.36423512e-02  6.88369013e-03\n",
      "  1.09867454e-02  5.52038364e-02 -7.96607658e-02 -5.10764159e-02\n",
      "  2.41172053e-02  6.69669127e-03  3.60606313e-02  9.01244394e-03\n",
      " -1.74744669e-02  1.65497568e-02 -1.93523336e-02  4.45868224e-02\n",
      "  1.71294417e-02 -1.90815143e-02 -1.80165451e-02  9.49393865e-03\n",
      "  1.03442073e-02 -7.52062574e-02 -3.78942341e-02  2.71298476e-02\n",
      "  4.29151282e-02  6.11749943e-03  7.11465254e-02 -3.59038524e-02\n",
      " -3.41157056e-02  4.49460112e-02  1.27722267e-02 -3.50714698e-02\n",
      "  2.61237361e-02 -1.09268390e-02  1.43386200e-02 -5.65083474e-02\n",
      "  1.30678630e-02 -1.59905069e-02  1.12654800e-02 -2.54021809e-02\n",
      "  8.02352838e-03  4.37819138e-02  3.98928672e-02  1.32178841e-02\n",
      " -5.44119030e-02 -5.86248077e-02  1.59983914e-02 -4.88557331e-02\n",
      " -4.30398136e-02  2.12666970e-02 -1.37660708e-02 -2.85869348e-03\n",
      " -9.37073976e-02  3.51297110e-02 -7.13547226e-03 -4.39215191e-02\n",
      " -1.77440699e-02 -4.28103982e-03 -4.58650803e-03 -3.72301340e-02\n",
      "  2.47469768e-02  6.85516521e-02 -6.21693544e-02 -7.35389628e-03\n",
      " -3.71867861e-03  2.04148497e-02  3.42106819e-02 -6.96840603e-03\n",
      " -3.74106667e-03  4.31439765e-02  1.79017894e-02 -4.04813420e-03\n",
      "  7.34758680e-04 -6.75009447e-04  1.16878264e-02  1.10743726e-02\n",
      "  3.34323421e-02 -6.68463632e-02  2.71700583e-02  3.84191680e-03\n",
      " -4.49036919e-02  6.27337098e-02 -1.31522212e-02 -2.23288704e-02\n",
      " -3.67486267e-03 -3.42806317e-02  4.38310653e-02 -3.40693407e-02\n",
      "  5.25650568e-03  2.45336294e-02  3.39337848e-02  4.05368023e-02\n",
      "  1.27802351e-02  5.28851058e-03 -1.74890570e-02  2.11984441e-02\n",
      " -2.23777462e-02  1.01537863e-03 -7.75141492e-02  3.26117640e-03\n",
      " -2.50199270e-02  3.34013775e-02  7.24781817e-03  9.72881273e-04\n",
      "  1.24598120e-03  1.43187605e-02 -4.73273546e-02  1.14977658e-02\n",
      " -2.92196646e-02  1.26599502e-02  8.77868291e-03  1.22458506e-02\n",
      "  1.74610168e-02 -5.94886392e-02  5.64167649e-02  6.17909152e-03\n",
      " -7.16853142e-02  2.85265176e-03  1.87976975e-02  2.98468098e-02\n",
      " -1.55859925e-02 -1.09791756e-02 -1.08146491e-02 -5.93945459e-02\n",
      "  3.62779126e-02 -6.40274435e-02  1.47659946e-02  5.25501557e-02\n",
      "  1.74148940e-02 -1.79984327e-02 -1.62573140e-02 -4.20104377e-02\n",
      " -1.22859608e-02 -3.52696031e-02  7.93191604e-03 -7.62610212e-02\n",
      "  2.12845905e-03  2.06637988e-03 -5.01622073e-03  5.07789366e-02\n",
      "  3.97171341e-02  1.95173733e-02  6.30217493e-02 -4.15246497e-04\n",
      "  3.89896557e-02 -7.60139618e-03 -1.20174279e-02 -1.39655685e-02\n",
      "  3.55325118e-02 -6.83424179e-04 -1.59205534e-02  1.90931037e-02\n",
      "  1.52063835e-02 -7.89900869e-02  9.59298108e-03  4.02888320e-02\n",
      "  4.62102853e-02 -3.15587502e-03 -3.90835153e-03  4.70784716e-02\n",
      "  6.92800991e-03  1.02967536e-02  8.64409953e-02 -6.14721933e-03\n",
      "  4.58153598e-02 -7.33062392e-03 -2.71873374e-04 -2.34046467e-02\n",
      "  2.92513650e-02 -3.58601213e-02  5.37800156e-02 -1.93161089e-02\n",
      "  3.73144671e-02 -4.49774135e-03 -6.49123592e-03 -4.54459600e-02\n",
      "  3.38811404e-03  4.03590640e-03  1.01454407e-02 -1.95511840e-02\n",
      " -1.05034979e-02  1.52459173e-02 -3.81945670e-02  4.29573804e-02\n",
      "  3.61978053e-03 -4.35573943e-02 -4.39289287e-02  9.38897356e-02\n",
      " -6.19876087e-02 -1.04596214e-02  4.28161398e-03  1.17470678e-02\n",
      "  4.99061448e-03 -1.22798467e-02  5.68797160e-03  5.38440496e-02\n",
      " -1.63445473e-02  2.97678616e-02  1.65455006e-02 -1.21843684e-02\n",
      "  7.10414676e-03  3.19588333e-02 -1.51297739e-02  3.53990234e-02\n",
      "  5.16489409e-02 -4.42692712e-02  2.34562382e-02  2.79628504e-02\n",
      "  1.71405673e-02 -4.60046227e-04 -1.29158255e-02  4.44831438e-02\n",
      "  4.82987687e-02 -7.61514949e-03 -4.10700068e-02 -3.79312374e-02\n",
      " -2.05529258e-02  2.38438509e-03  6.24959283e-02 -2.64068991e-02\n",
      " -2.36876949e-04 -1.79177504e-02  1.33443996e-02 -3.06371283e-02\n",
      " -8.41427073e-02 -5.25661092e-03 -2.15982236e-02  3.27200703e-02\n",
      "  2.84425560e-02 -7.35473447e-03  2.88453624e-02 -7.49379164e-03\n",
      " -1.78212784e-02 -1.40953409e-02  4.46038842e-02 -3.95580344e-02\n",
      "  1.66229960e-02  3.08321230e-02 -2.76660025e-02  4.49127937e-03\n",
      " -6.02306575e-02  3.04986592e-02  4.11146395e-02  6.96263369e-03\n",
      " -2.65229177e-02 -3.50042582e-02 -5.58389947e-02 -2.29872204e-02\n",
      "  1.08738178e-02 -2.19426001e-03 -9.42736305e-03  5.53239509e-03\n",
      " -1.38010858e-02  2.10793614e-02  4.13151737e-03 -8.70909821e-03\n",
      " -6.03573434e-02  1.17390864e-01 -2.14535389e-02 -3.79899540e-03\n",
      "  7.67155141e-02  2.26545390e-02 -5.93619607e-02 -1.16131350e-03\n",
      " -2.97306571e-02 -9.39253904e-03 -4.07408699e-02  8.57337266e-02\n",
      " -4.20205519e-02 -6.29779473e-02 -2.35068705e-02 -2.18479875e-02\n",
      " -2.05521341e-02 -2.71465722e-02 -9.58782062e-03  6.36660904e-02\n",
      "  1.52647803e-02  4.19336446e-02  5.15136495e-02 -1.63780227e-02\n",
      " -1.05248857e-02  5.36684096e-02  3.31993699e-02  2.88626552e-02\n",
      "  4.27957214e-02  2.85309888e-02  8.34287424e-03 -1.76903885e-02\n",
      "  6.69285879e-02 -4.98739518e-02 -6.71996847e-02 -1.03315036e-03\n",
      " -4.12875228e-02 -5.46945855e-02  6.78889379e-02  6.14245236e-02\n",
      "  2.74481401e-02 -5.09627275e-02 -5.29168807e-02  4.86863106e-02\n",
      "  1.35616148e-02  1.73227323e-04  2.89344112e-03  2.25522654e-05\n",
      " -7.23234117e-02  6.51353272e-03 -2.18479186e-02 -3.15575581e-03\n",
      "  5.81254512e-02 -1.16037382e-02  3.08346841e-02  6.73101400e-04\n",
      " -1.04043158e-02 -2.16118153e-02 -5.06330393e-02  4.34187911e-02\n",
      "  2.63047451e-03 -3.06619401e-03  2.84230366e-04  1.98841356e-02\n",
      "  2.60986798e-02  1.63624920e-02 -2.32910328e-02  3.06294542e-02\n",
      "  1.58752054e-02 -5.82722202e-02  6.22080974e-02  9.29626375e-02\n",
      "  1.57141127e-02 -2.54178215e-02 -4.33426239e-02 -4.83524539e-02\n",
      "  7.42995832e-03  6.63533481e-03  3.80434841e-02  2.30913907e-02\n",
      "  2.86016110e-02 -9.27383080e-03  3.53168696e-02 -6.21482283e-02\n",
      "  2.57540885e-02 -1.52124220e-03  6.12052623e-03 -9.60376987e-04\n",
      "  8.66786297e-03 -1.71348304e-02  3.44652496e-02  9.09447484e-03\n",
      "  1.21925548e-02 -2.12888885e-02  8.19058716e-03 -1.20529411e-02\n",
      " -5.00703678e-02 -5.69866821e-02  3.21256407e-02 -1.91880520e-02\n",
      "  5.91764711e-02  8.60036258e-03 -1.09348707e-02 -4.37611341e-02\n",
      " -2.57801302e-02  3.20603512e-02 -1.00977477e-02  4.64921258e-02\n",
      " -2.06920598e-03  4.93489504e-02  1.69440228e-02 -2.49490254e-02\n",
      " -1.24202278e-02 -4.92830724e-02 -3.11362352e-02  1.10380389e-02\n",
      " -3.90875824e-02  1.29109666e-01  1.81439053e-02 -5.85707277e-02\n",
      "  1.33008708e-03  2.78328210e-02  4.29810025e-02 -1.56340729e-02\n",
      " -4.09074239e-02 -3.03135291e-02  1.85556512e-03 -3.07139810e-02\n",
      "  6.72171041e-02  2.81571243e-02 -3.11283190e-02 -7.97195360e-03\n",
      "  7.18796486e-03 -6.88957572e-02  2.72047799e-02 -2.80911084e-02\n",
      " -6.54577790e-03 -2.12920289e-02 -9.76123288e-03 -4.99457978e-02\n",
      "  6.51107430e-02  2.51633581e-02 -1.42327920e-02 -1.92829985e-02\n",
      " -3.03968638e-02  9.71561577e-03  2.93186400e-02 -1.11449361e-02\n",
      " -3.00777610e-03 -6.15728125e-02  3.44085954e-02 -1.14368228e-02\n",
      "  9.11082886e-03  5.23227686e-03  4.12273332e-02  4.03453745e-02\n",
      " -5.35161942e-02 -4.89423312e-02 -1.50131974e-02  2.20726547e-03\n",
      "  3.79190557e-02  5.49071468e-02  1.96146900e-33 -2.19645742e-02\n",
      "  4.16761125e-03 -1.10492259e-02  3.56590748e-02  6.23922143e-03\n",
      " -1.32971928e-02  3.37554850e-02  7.63571705e-04  4.20450568e-02\n",
      "  6.39708759e-03  1.96152590e-02  3.62785608e-02 -4.54359362e-03\n",
      "  9.66309104e-03 -9.64657683e-03 -4.56818826e-02 -3.98486704e-02\n",
      "  5.30101471e-02 -4.36068326e-03  9.73617099e-03  4.59568226e-04\n",
      " -2.32505463e-02  7.46989623e-02 -7.88044557e-03 -3.84603185e-03\n",
      " -3.75637598e-03 -8.72893911e-03 -3.20462547e-02  7.66592752e-03\n",
      "  1.65601205e-02 -2.92172446e-03  8.99545103e-03 -5.87034458e-03\n",
      " -2.70173065e-02 -2.50395276e-02 -2.68910471e-02 -4.78132889e-02\n",
      " -9.69189126e-03  3.06099653e-02  6.37396872e-02  3.22448313e-02\n",
      "  4.34311815e-02  4.59276838e-03 -1.65440794e-02 -3.82582508e-02\n",
      "  8.31083767e-03 -2.46847104e-02  3.15137990e-02 -4.67736740e-03\n",
      " -6.99140597e-03 -2.44365353e-02  3.37581113e-02  4.36644368e-02\n",
      " -3.52899656e-02 -3.81676033e-02  5.13059348e-02  4.05260995e-02\n",
      "  2.72041820e-02  6.39677942e-02  7.23076984e-02  3.47029045e-02\n",
      " -9.99074988e-03  2.91952994e-02 -2.48727463e-02 -1.09459609e-01\n",
      "  3.61446664e-02  6.52754307e-02  1.90428877e-03 -2.06614900e-02\n",
      "  3.19191478e-02 -3.37370299e-02  1.39720412e-02  5.72276348e-03\n",
      " -7.41463061e-03  6.34118170e-02  7.91258365e-02 -4.82490141e-04\n",
      " -1.61614490e-03  2.10266933e-02 -2.67816689e-02  1.56706125e-02\n",
      " -1.97279416e-02 -2.15714909e-02  4.06080782e-02  1.98916905e-02\n",
      " -3.26035917e-02  4.89075035e-02  6.01660609e-02  7.21849278e-02\n",
      "  4.01844271e-02  1.12890147e-01 -9.10681579e-03 -1.07979365e-02\n",
      "  9.58714445e-05 -3.57160419e-02  1.89743675e-02 -6.07034266e-02\n",
      " -3.92846949e-02 -1.86862499e-02 -1.19475890e-02 -8.54716916e-03\n",
      "  1.20472105e-03  1.53368227e-02 -2.05733851e-02 -1.12162251e-02\n",
      "  2.16416339e-03  4.05089073e-02  9.20544751e-03  1.82919297e-02\n",
      " -5.48366308e-02 -1.08562158e-02  4.30798195e-02 -5.41272201e-02\n",
      " -3.94558087e-02  4.23342548e-03 -9.77823045e-03 -5.00141419e-02\n",
      " -9.82264336e-03  2.15956010e-02  2.58817757e-03 -1.77251901e-02\n",
      "  2.47332063e-02  4.67579626e-02  6.77175745e-02 -2.26824861e-02\n",
      "  2.78415028e-02 -4.15456556e-02  7.33882040e-02  2.03456748e-02\n",
      "  2.55577378e-02 -1.26612894e-02 -1.02006048e-02 -7.58157810e-03\n",
      "  1.49521390e-02 -2.60276385e-02 -7.87929259e-03  2.07499526e-02\n",
      "  1.67159792e-02  1.16993459e-02 -5.34797600e-03 -5.18575683e-03\n",
      "  2.04152595e-02  3.71050788e-03  3.11703999e-02 -3.51871401e-02\n",
      " -6.96811778e-03 -6.39502406e-02  9.49779991e-03 -3.01080644e-02\n",
      "  1.76533200e-02  1.10145900e-02  2.57795565e-02  2.48333905e-02\n",
      "  1.96341760e-02  1.77973453e-02  8.53687059e-03 -5.72359338e-02\n",
      "  7.87549168e-02 -3.27238883e-03 -3.59622613e-02 -1.12784309e-02\n",
      " -1.90719275e-03 -2.46230140e-02 -4.02159207e-02 -5.35711087e-03\n",
      "  5.21604717e-02  4.32058014e-02  4.64817509e-02  1.33563252e-02\n",
      "  3.75774391e-02 -1.96694545e-02 -1.74744260e-02  1.42601097e-03\n",
      "  5.56794107e-02  4.01503183e-02 -4.06827889e-02  3.66287865e-02\n",
      "  4.04857211e-02 -5.32001965e-02 -8.40041637e-02  2.64976174e-03\n",
      " -5.58047965e-02 -3.93060502e-03 -2.11393423e-02  3.37587819e-02\n",
      " -2.62645055e-02  2.10639425e-02 -2.20663305e-02 -2.53170151e-02\n",
      " -2.25023925e-02 -1.14467926e-02  2.16869339e-02 -1.54950200e-02\n",
      " -7.29033817e-03  4.79715178e-03 -6.44843802e-02  1.62935480e-02\n",
      "  1.12122046e-02 -3.21637802e-02 -4.01290283e-02 -5.77260144e-02\n",
      "  5.12447767e-02  2.57027689e-02 -1.99324097e-02 -4.97193588e-03\n",
      " -2.35844050e-02 -1.07710240e-02 -3.70196812e-02 -4.71807271e-02\n",
      " -4.33887243e-02  4.61778510e-03  5.79151250e-02 -3.10254842e-02\n",
      "  1.33855622e-02  4.82191594e-04 -4.50102729e-04  5.45415394e-02\n",
      "  2.28798985e-02  5.44311618e-03  2.62301490e-02 -5.05142435e-02\n",
      "  8.23888422e-06  1.39496708e-02 -3.37011479e-02 -3.17831188e-02\n",
      " -5.08098416e-02 -6.02801666e-02  5.87724261e-02  5.44314214e-04\n",
      "  3.07394145e-03 -1.95792090e-04  2.89458539e-02 -4.23617512e-02\n",
      " -1.73597001e-02 -1.28306188e-02  6.98157726e-03  2.64882036e-02\n",
      "  3.28202732e-02  9.47437659e-02 -4.47073113e-03  1.17253743e-01\n",
      "  3.39710750e-02  3.00531331e-02 -7.27051031e-03 -4.39194851e-02\n",
      "  8.41725245e-03 -7.51756318e-03  2.26023775e-02 -8.73232819e-03\n",
      "  2.36799195e-02 -1.39164068e-02 -1.98966339e-02 -8.87863524e-03\n",
      " -2.24909261e-02 -8.76107905e-03  5.82054816e-02  3.85198817e-02\n",
      " -4.78317142e-02 -4.73025702e-02 -4.50039282e-02 -4.65447865e-02\n",
      "  3.02283466e-02 -6.18410064e-03 -1.10683450e-03 -3.51431742e-02\n",
      " -1.81022454e-02  2.76126135e-02  8.27683788e-03  8.99322052e-03\n",
      "  9.22542363e-02 -6.15586378e-02 -6.03839271e-02 -3.24200466e-02]\n",
      "\n",
      "Sentence: Learn to use embeddings well and you'll be well on your way to being an AI engineer.\n",
      "Embedding: [-1.27362064e-03 -1.54341934e-02 -8.66567064e-03 -1.83742475e-02\n",
      " -5.69552891e-02 -4.16346528e-02 -5.75635172e-02 -6.95571536e-03\n",
      " -4.35472317e-02 -1.01444386e-01 -1.86654720e-02  6.85101887e-03\n",
      "  3.06317341e-02  5.66019639e-02 -6.20234851e-03  5.37179187e-02\n",
      " -7.50226527e-03 -4.65109199e-02 -1.75817013e-02 -1.07620265e-02\n",
      " -2.84414701e-02 -4.90557961e-03 -5.69019206e-02 -3.53091918e-02\n",
      " -9.12379567e-03  2.42871325e-03  1.14618922e-02  1.19835678e-02\n",
      " -3.03620044e-02  1.37243671e-02 -6.96566328e-02  4.71940264e-03\n",
      " -4.40859422e-02  3.65208462e-02  2.89269499e-02  4.89480942e-02\n",
      "  4.07518484e-02 -4.36624624e-02  1.95373464e-02 -1.62816793e-02\n",
      " -1.12342285e-02 -1.99471344e-03 -3.12269088e-02 -4.72739488e-02\n",
      "  2.23609172e-02 -2.92920065e-03 -5.95299937e-02 -8.84137973e-02\n",
      " -2.40843855e-02  3.87748182e-02 -8.31167959e-03  4.95167300e-02\n",
      " -6.23732358e-02 -9.05024633e-03  4.48902622e-02  1.05241523e-03\n",
      "  1.51566127e-02  5.12214638e-02  5.37964925e-02  1.77612696e-02\n",
      " -6.29934743e-02  3.53279449e-02 -1.73390508e-02 -3.74631360e-02\n",
      "  7.95784220e-02  5.20433187e-02  7.21564377e-03  4.23880666e-03\n",
      " -1.38940581e-03 -2.02069916e-02  3.49659398e-02  1.99903417e-02\n",
      " -2.53464263e-02 -2.93021034e-02  5.77128232e-02 -5.62125109e-02\n",
      " -3.69553417e-02 -6.34371713e-02 -5.08899726e-02 -1.70162246e-02\n",
      " -1.66935902e-02 -6.45801499e-02 -7.30899945e-02 -4.70271073e-02\n",
      "  9.49142908e-04  1.33875478e-02  2.39530746e-02  3.15555115e-03\n",
      " -2.96470486e-02 -1.38123259e-02 -4.16273028e-02  1.37374382e-02\n",
      "  5.08991331e-02 -9.41795576e-03  3.63903530e-02 -5.69861196e-02\n",
      "  3.56107056e-02 -8.13319907e-02 -8.39568488e-03  1.76614779e-03\n",
      "  5.14752083e-02  4.55307923e-02  2.92409603e-02  1.86265782e-02\n",
      "  7.38073932e-03  4.27583084e-02 -4.92686266e-03 -7.43663013e-02\n",
      "  3.24523821e-02  1.92142874e-02 -5.25369868e-03 -5.68218715e-02\n",
      "  1.77262276e-02  5.82253421e-03 -3.00542358e-03 -5.00561632e-02\n",
      " -2.93329693e-02 -2.41048425e-03 -2.94579007e-03  2.80014873e-02\n",
      " -3.24892588e-02 -1.27681224e-02 -2.55539734e-02 -4.07880060e-02\n",
      " -2.68235318e-02 -2.68613044e-02  3.86876194e-03 -2.12315260e-03\n",
      " -4.98712435e-02  2.28559058e-02 -3.62256132e-02 -3.75633426e-02\n",
      " -2.04534791e-02  2.92124003e-02 -7.72747351e-03  1.27874571e-03\n",
      "  8.96972325e-03  2.43819840e-02 -5.76097704e-02 -1.80398673e-02\n",
      " -9.82231554e-03 -1.08060529e-02  4.40571979e-02 -1.60764027e-02\n",
      "  2.93245213e-03  2.72541009e-02  2.06259433e-02  2.35220734e-02\n",
      "  1.09668577e-03 -1.63700061e-05  5.01896604e-04 -3.72250751e-02\n",
      "  2.43283971e-03 -3.73724848e-02 -1.23432055e-02  5.71158379e-02\n",
      " -2.06795079e-03  3.30931246e-02  6.42836690e-02 -2.40922682e-02\n",
      "  1.36497458e-02 -4.72871251e-02 -2.28622388e-02  2.55047297e-03\n",
      " -2.46368884e-03 -2.65180953e-02  3.74127366e-02  1.35779679e-02\n",
      "  3.23007628e-03  2.75751296e-02  3.50741968e-02  2.03507226e-02\n",
      " -1.56163760e-02  4.32741120e-02  3.04548237e-02  1.82568096e-03\n",
      " -2.51468122e-02  7.52793029e-02  4.36129700e-03  2.72825593e-03\n",
      "  1.38339093e-02 -6.59173308e-03 -5.86360507e-02  3.91121656e-02\n",
      " -1.52467275e-02 -3.52510856e-03 -2.66035348e-02  2.74840705e-02\n",
      "  6.16145581e-02 -1.76431611e-02 -1.50416954e-03  6.02777824e-02\n",
      " -2.06826534e-02 -1.31394463e-02  2.66498793e-02 -3.24712545e-02\n",
      " -2.21466348e-02 -4.11989428e-02  9.90800094e-03  1.96388997e-02\n",
      "  5.61811887e-02 -3.94120105e-02  1.08117824e-02  3.09972800e-02\n",
      "  5.36297560e-02  1.75601486e-02  1.83835197e-02 -7.25810826e-02\n",
      " -4.78347763e-02  1.93291402e-03 -3.11306175e-02 -4.79600057e-02\n",
      "  1.65493041e-02 -2.63989773e-02  9.98059101e-03  4.17170674e-02\n",
      "  1.99524686e-02  4.98292558e-02  8.88863355e-02  4.42877784e-02\n",
      "  5.29035134e-03  2.82974318e-02 -5.05881459e-02 -2.09721550e-02\n",
      " -3.56762409e-02  2.15620045e-02 -6.64992677e-03 -9.93970502e-03\n",
      "  9.60838050e-03 -4.81883809e-02  2.63234526e-02  1.66827347e-02\n",
      "  4.61210124e-02  4.79878411e-02 -1.44554174e-03  5.63640520e-02\n",
      " -2.16559321e-02 -3.15963179e-02  9.13035423e-02  1.66695975e-02\n",
      " -1.36835072e-02 -1.05257779e-02  3.92308347e-02 -1.25357555e-02\n",
      "  4.25589718e-02 -2.16689017e-02  4.12736535e-02 -3.92410532e-02\n",
      " -6.01732843e-02 -7.45622516e-02 -2.05468573e-02 -4.82865348e-02\n",
      " -2.38446929e-02  2.41863634e-03  1.30868377e-02 -4.77632619e-02\n",
      "  1.88787468e-02  1.74356215e-02 -2.26105060e-02  4.85587008e-02\n",
      "  1.98856518e-02 -2.80565303e-02 -5.62787950e-02  6.57102093e-02\n",
      " -6.37453943e-02 -6.35114312e-02 -4.69829813e-02  7.56338006e-03\n",
      " -6.48957416e-02  4.22970988e-02  2.08776779e-02  2.76161451e-03\n",
      " -8.87909206e-04  1.43224970e-02  3.30637731e-02 -7.45571591e-03\n",
      "  2.44107880e-02  1.02035599e-02 -2.58671287e-02  1.94708556e-02\n",
      "  8.15286711e-02 -2.98610795e-02  5.98778464e-02  2.80829463e-02\n",
      " -2.51866225e-02  8.55933037e-03 -1.85661670e-02  4.11858521e-02\n",
      "  4.05376405e-02 -3.74305956e-02  6.74326345e-03 -1.77999269e-02\n",
      " -2.40240339e-02 -2.42125504e-02  3.34647149e-02 -3.71127762e-02\n",
      " -1.40796723e-02  1.53745839e-03 -2.21663043e-02 -2.18037218e-02\n",
      " -5.86122088e-02 -2.33848188e-02  2.29033064e-02 -7.14422911e-02\n",
      "  4.22133654e-02 -8.10800772e-03  5.33788232e-03  1.16637284e-02\n",
      " -3.99143025e-02 -3.14993188e-02  3.54596935e-02  2.28698999e-02\n",
      " -3.55821811e-02  5.98325506e-02 -5.87501703e-03 -3.36615182e-03\n",
      " -1.48331914e-02  2.93912785e-03  2.51703002e-02 -1.79073159e-02\n",
      " -2.29480267e-02 -2.07362603e-02 -2.99669080e-03  5.10530882e-02\n",
      "  1.89011020e-03  1.75525472e-02  6.18197657e-02  7.88339647e-04\n",
      "  2.66012680e-02  3.43304761e-02 -1.57850441e-02  1.81231147e-03\n",
      " -4.29937877e-02  4.53398302e-02 -4.34956327e-02 -4.36819904e-02\n",
      " -2.82557029e-03  2.71338895e-02  1.73402149e-02 -1.26127787e-02\n",
      " -3.33879441e-02 -2.08824649e-02 -2.64400784e-02 -8.70595314e-03\n",
      "  2.63013020e-02 -2.13165190e-02 -2.48070769e-02  6.53882418e-03\n",
      "  1.02486387e-02 -1.06097199e-02 -5.02407588e-02  2.66965758e-02\n",
      " -1.69663448e-02  4.02012877e-02  3.61632854e-02 -1.58026796e-02\n",
      " -1.88067928e-03  3.10005210e-02 -1.22341292e-03  1.11500975e-02\n",
      "  2.58466881e-02  3.62872854e-02  4.44972888e-02  2.87195621e-03\n",
      "  3.56289707e-02 -3.27184759e-02 -8.88136476e-02  2.47899313e-02\n",
      " -2.87285037e-02 -5.38368570e-03  7.90464878e-03  5.05467132e-02\n",
      "  3.50590446e-04 -8.32358301e-02  1.03852269e-03  1.85216609e-02\n",
      "  4.89885956e-02  3.52009237e-02 -5.05210832e-02 -1.80323012e-02\n",
      " -6.75277086e-03  7.22748367e-03 -4.42312621e-02  5.35137355e-02\n",
      "  6.87053055e-02 -2.10968088e-02 -8.84411391e-03 -2.43200287e-02\n",
      " -5.73814511e-02 -2.17334870e-02  6.03747256e-02  1.59222521e-02\n",
      "  2.87384796e-03  3.00619397e-02 -6.48647025e-02 -3.56271975e-02\n",
      "  3.41966137e-04  5.97507926e-03 -2.78856009e-02  8.21759775e-02\n",
      " -4.34630513e-02 -2.21294723e-02  6.11384623e-02  1.34110361e-01\n",
      "  2.61387718e-03 -6.66510314e-02 -1.08557818e-02 -2.27634441e-02\n",
      "  6.56756982e-02  5.04081100e-02  2.60738675e-02  2.96083745e-02\n",
      "  2.96403770e-03 -2.68989541e-02  3.25817429e-02 -4.77534495e-02\n",
      "  2.91177165e-02  1.21364053e-02  3.61824129e-03  3.34559679e-02\n",
      " -1.87795376e-03  1.39309661e-02 -3.87431751e-03 -2.63950024e-02\n",
      "  5.21587543e-02  7.95530807e-03 -3.25279161e-02  5.50867105e-03\n",
      " -8.02455842e-02  2.54014805e-02  2.42111403e-02 -6.88694566e-02\n",
      "  6.80361837e-02  3.49501526e-04  9.00671538e-03 -2.77494714e-02\n",
      " -1.52270123e-02  4.65111248e-02 -2.67316680e-02  1.13789532e-02\n",
      "  4.28157896e-02  2.64182109e-02 -1.14518385e-02 -1.58912055e-02\n",
      " -1.81100685e-02 -2.91360319e-02 -5.04318736e-02  3.48882973e-02\n",
      " -4.88506481e-02  8.51980224e-02 -2.40058172e-02 -4.74805245e-04\n",
      "  1.08796954e-02 -1.95390563e-02  1.62021648e-02 -3.42027023e-02\n",
      " -3.78363132e-02 -3.74191403e-02  2.08850447e-02  6.25741261e-04\n",
      "  4.09244262e-02 -7.96503201e-03 -3.07211597e-02 -2.86010131e-02\n",
      "  9.19208210e-03 -8.83709043e-02  3.94571722e-02 -3.70749496e-02\n",
      " -1.16303870e-02 -4.02609892e-02 -1.66663807e-02 -4.54829559e-02\n",
      "  7.62197450e-02 -2.10805312e-02 -1.15806088e-02  2.26928145e-02\n",
      " -1.55341253e-02 -6.81816321e-03  5.35224602e-02 -4.68827710e-02\n",
      " -8.04427487e-04  1.05660791e-02 -2.78787711e-03 -1.66233201e-02\n",
      " -1.61674730e-02 -4.92973626e-03 -2.02183127e-02 -5.66957938e-03\n",
      " -2.95121465e-02 -5.26972190e-02  1.90177914e-02  6.66106353e-04\n",
      " -1.29930340e-02  5.33003956e-02  2.68755845e-33 -3.21150385e-02\n",
      " -2.11053574e-03 -5.07444069e-02  4.78233099e-02  3.72024253e-02\n",
      " -2.56987587e-02 -1.19759534e-02 -5.76877557e-02  5.49337603e-02\n",
      " -2.13820022e-02  1.91575568e-02  6.34295167e-03 -4.75406786e-03\n",
      "  2.04408541e-02  1.07866973e-02 -8.51493403e-02  1.47765568e-02\n",
      "  6.25373274e-02  1.04148062e-02  1.56272631e-02 -1.14781260e-02\n",
      " -5.61868586e-02  4.21102569e-02  3.52333188e-02  3.41385254e-03\n",
      " -4.67980690e-02 -1.80712845e-02 -6.14240952e-03 -2.17472669e-02\n",
      "  2.03627255e-02 -2.80616432e-02  2.13502832e-02 -4.08370756e-02\n",
      " -1.21226795e-02 -2.88871191e-02 -1.02638733e-02  1.64860282e-02\n",
      " -4.48425184e-04  2.42318101e-02  3.66818421e-02 -1.95624642e-02\n",
      "  6.60977000e-03  1.03314281e-01  3.88364941e-02 -7.43389055e-02\n",
      "  5.18580116e-02 -1.37119070e-02  2.13431045e-02  2.23212373e-02\n",
      " -4.46436666e-02  2.64632944e-02  8.55354890e-02 -1.78489590e-03\n",
      " -3.54052149e-02 -5.94411008e-02  2.44842120e-03  3.77057381e-02\n",
      "  2.52434378e-03  3.28403488e-02  7.18777105e-02 -4.21842560e-03\n",
      " -4.10485500e-03  7.12756440e-02 -1.18006235e-02 -2.61001959e-02\n",
      "  4.89613041e-02  7.80019015e-02  1.58549100e-02 -2.10710522e-02\n",
      " -1.64562091e-02  3.93063901e-03  3.91908363e-02  1.92021914e-02\n",
      " -5.17402515e-02  3.25031467e-02  5.53387366e-02  2.49874890e-02\n",
      "  2.76886839e-02 -3.56663167e-02 -3.41968238e-02  2.06229594e-02\n",
      "  3.26713324e-02 -4.29041311e-02  2.62591359e-03  4.12291475e-02\n",
      " -8.74129757e-02  9.52266157e-02  8.62808246e-03  4.67638038e-02\n",
      "  4.27409336e-02  1.16691053e-01  1.32856928e-02 -2.50164121e-02\n",
      "  1.73932826e-03 -8.60073231e-03  1.24992686e-03 -4.61972542e-02\n",
      " -4.03604209e-02 -3.22943293e-02 -2.30257586e-02 -4.63813497e-03\n",
      " -2.45359093e-02  5.35283647e-02  4.19440772e-03  1.01437494e-02\n",
      "  3.06844488e-02  2.80387532e-02  4.09806930e-02 -2.69035585e-02\n",
      "  3.79183120e-03  2.15080418e-02  4.28831689e-02 -4.30176258e-02\n",
      " -6.86156377e-02 -2.72122771e-02  7.68235559e-03 -6.22538552e-02\n",
      "  3.81456898e-03  2.61061955e-02 -3.46263833e-02  1.28830243e-02\n",
      "  5.93077741e-04  4.99908999e-02  1.69820320e-02  4.29438725e-02\n",
      "  1.70877390e-02 -2.87876260e-02 -1.17119849e-02  4.55158427e-02\n",
      "  3.51275802e-02 -3.13966163e-02  3.07939574e-02 -1.50829158e-03\n",
      "  2.24454552e-02 -1.85893551e-02 -3.23756933e-02  3.66898403e-02\n",
      "  1.88710052e-03 -1.24026490e-02  5.96230337e-03  2.03220248e-02\n",
      "  3.16509455e-02 -1.78939700e-02 -7.09910830e-03  1.12109892e-02\n",
      " -9.62416269e-03 -2.60764174e-02 -2.47969441e-02 -1.65694412e-02\n",
      " -2.81837080e-02 -1.77687835e-02  3.18063758e-02  1.11796306e-02\n",
      "  1.32764848e-02  7.54720997e-03  4.30178903e-02 -1.54438615e-02\n",
      "  8.06555748e-02 -4.25643567e-03  3.52593628e-03 -3.78522873e-02\n",
      "  4.92975153e-02 -5.03921025e-02 -2.93354969e-02  8.58211890e-03\n",
      "  2.68631596e-02  4.03082184e-02  2.95627154e-02  5.57620674e-02\n",
      "  6.31305203e-02 -4.87381779e-02 -2.46596755e-03  1.05649699e-02\n",
      "  2.14446522e-02  5.63113391e-02 -2.21867803e-02  1.07072331e-01\n",
      " -2.56065633e-02 -3.33635099e-02 -3.34037505e-02 -2.75849588e-02\n",
      "  4.55308182e-04 -5.01803420e-02 -4.82621156e-02 -3.02250553e-02\n",
      " -3.23276296e-02  6.16881112e-03  3.42307910e-02  7.63496244e-03\n",
      " -1.77230164e-02  6.71720039e-03 -2.70274002e-02  8.23468268e-02\n",
      " -4.38831002e-02 -2.93297153e-02 -2.81882752e-02  4.12143469e-02\n",
      "  2.18016673e-02 -2.48112455e-02 -4.80811335e-02 -1.10460408e-01\n",
      "  3.70273702e-02  6.90882094e-03 -3.24926674e-02  2.99199149e-02\n",
      " -1.48300864e-02  2.06583962e-02 -7.07678730e-03 -1.09593058e-02\n",
      "  2.53681634e-02  5.11374790e-03  4.53540273e-02  3.71298604e-02\n",
      "  5.42714773e-03  2.59081759e-02 -2.15023216e-02  2.77117640e-02\n",
      "  1.00831408e-02  4.54039387e-02  2.31525786e-02 -2.60809623e-02\n",
      "  5.59955202e-02 -2.77955327e-02 -3.13352421e-02 -2.57340204e-02\n",
      " -5.71258403e-02 -6.03812141e-03  2.88834162e-02 -9.78067517e-03\n",
      " -2.77344603e-02  5.54856518e-03  1.62502378e-02 -3.11377132e-03\n",
      " -1.92464031e-02  6.45407662e-03  4.94345725e-02 -1.83710847e-02\n",
      " -1.19027859e-02  2.15626992e-02  1.79084986e-02  9.95923057e-02\n",
      "  7.63741732e-02  1.50433108e-02 -2.91355848e-02 -3.89983058e-02\n",
      " -2.95918081e-02 -4.13562283e-02 -8.54154676e-03 -1.89884994e-02\n",
      "  8.57208949e-03  8.77251755e-03 -8.42606835e-03  1.20966863e-02\n",
      " -9.61464271e-03 -5.13859168e-02 -1.11315241e-02  5.11423834e-02\n",
      " -6.29280647e-03 -5.16711473e-02 -2.12988034e-02 -1.11759817e-02\n",
      "  2.49515306e-02 -1.45137254e-02 -3.58739495e-03  4.16563675e-02\n",
      "  5.15723228e-03  1.59327388e-02 -6.01370111e-02 -3.45670544e-02\n",
      "  4.29583266e-02 -2.85811592e-02 -6.64402992e-02 -6.28364235e-02]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Requires !pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-distilroberta-v1\", \n",
    "                                      device=\"cuda\") # choose the device to load the model to (note: GPU will often be *much* faster than CPU)\n",
    "\n",
    "# Create a list of sentences to turn into numbers\n",
    "sentences = [\n",
    "    \"The Sentences Transformers library provides an easy and open-source way to create embeddings.\",\n",
    "    \"Sentences can be embedded one by one or as a list of strings.\",\n",
    "    \"Embeddings are one of the most powerful concepts in machine learning!\",\n",
    "    \"Learn to use embeddings well and you'll be well on your way to being an AI engineer.\"\n",
    "]\n",
    "\n",
    "# Sentences are encoded/embedded by calling model.encode()\n",
    "embeddings = embedding_model.encode(sentences)\n",
    "embeddings_dict = dict(zip(sentences, embeddings))\n",
    "\n",
    "# See the embeddings\n",
    "for sentence, embedding in embeddings_dict.items():\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! That's a lot of numbers.\n",
    "\n",
    "How about we do just once sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Yo! How cool are embeddings?\n",
      "Embedding:\n",
      "[ 1.08260913e-02  2.41692699e-02 -5.50694950e-03 -8.78381729e-02\n",
      "  4.02133204e-02  5.20530120e-02 -2.10508443e-02 -3.94526729e-03\n",
      " -2.29506586e-02 -3.05871833e-02 -2.15222649e-02  4.00971174e-02\n",
      " -4.41348702e-02 -6.95999414e-02  4.64823935e-03  5.68864420e-02\n",
      "  3.78482905e-03 -2.37432551e-02 -8.03357363e-03 -4.99030091e-02\n",
      "  4.06385995e-02  1.21118426e-02  3.38867493e-02 -4.95439395e-02\n",
      " -1.42667750e-02 -2.26875320e-02  3.49207260e-02 -1.09987622e-02\n",
      " -3.37442160e-02  3.29349227e-02  1.40444702e-02  4.57680486e-02\n",
      "  1.81821864e-02  3.24518345e-02 -4.85767163e-02  2.39300914e-02\n",
      "  1.60664283e-02 -3.34798321e-02  7.98382144e-03 -4.71427478e-03\n",
      "  2.84693576e-03 -5.78676932e-04 -1.88540611e-02 -1.59974489e-02\n",
      "  2.75714919e-02  1.45352373e-04  1.30773792e-02 -9.23535377e-02\n",
      "  1.74976271e-02  3.60502005e-02  1.59503389e-02  1.34556750e-02\n",
      " -4.19778302e-02  9.27382708e-03  2.27063205e-02  6.86729923e-02\n",
      "  1.84429679e-02  2.15068497e-02  1.29141044e-02  3.44659500e-02\n",
      " -2.89168973e-02  1.48917073e-02 -2.10672487e-02 -4.40885574e-02\n",
      "  4.29354534e-02  1.47787565e-02  5.72202727e-03  1.47168292e-02\n",
      " -2.14426611e-02 -1.17971422e-02  3.27390097e-02 -1.22982347e-02\n",
      " -1.11593446e-02 -4.21808548e-02  2.13266183e-02  1.57495905e-02\n",
      " -8.22773855e-03 -3.85033749e-02 -2.94417422e-02  6.00807518e-02\n",
      "  1.69122629e-02  3.92937660e-02 -5.07948771e-02 -4.75694016e-02\n",
      " -2.78106285e-03  2.24482771e-02  5.07322326e-02  1.66234616e-02\n",
      "  3.69281927e-03  5.89906145e-03 -3.15623097e-02  2.76781600e-02\n",
      "  8.49933177e-03 -7.73044899e-02  3.68934199e-02  4.32453603e-02\n",
      "  1.39048435e-02 -7.95404240e-02 -1.71953924e-02 -3.66013832e-02\n",
      "  6.48010597e-02  1.92784350e-02  4.72230744e-03 -4.89871763e-02\n",
      "  2.66537406e-02  4.85890545e-02 -5.18371947e-02 -4.23862450e-02\n",
      "  2.69119050e-02 -2.14077570e-02  2.88797263e-02 -1.01784162e-01\n",
      " -7.71638239e-03 -3.64925563e-02 -1.38570117e-02 -5.57462312e-02\n",
      "  2.67055966e-02 -1.03652794e-02  4.58762869e-02  3.56889479e-02\n",
      "  4.08880301e-02 -3.27138081e-02 -2.02358118e-03 -4.29364517e-02\n",
      " -7.04241395e-02 -1.33122494e-02 -3.54776867e-02  5.17461859e-02\n",
      " -3.63688357e-02  7.45037720e-02  3.55739333e-03 -3.70833836e-02\n",
      " -1.71278752e-02  4.13968749e-02  4.04369570e-02 -6.06693514e-03\n",
      "  4.64594690e-03  3.09206359e-02 -5.45153394e-02 -5.93561260e-03\n",
      " -9.94842872e-03 -1.58128049e-02  3.69674973e-02  3.72255221e-02\n",
      "  1.45935183e-02  6.60967035e-03  4.96151065e-03  2.22660936e-02\n",
      "  2.49482784e-03  1.57933272e-02 -3.05037107e-02  5.77565841e-02\n",
      "  5.40358648e-02 -3.16023943e-03 -3.33364308e-02  7.14947982e-03\n",
      "  7.61846546e-03  1.20741678e-02 -2.73434278e-02  5.95168993e-02\n",
      "  2.04747356e-02 -5.18700667e-02 -2.90808380e-02 -3.42983101e-03\n",
      " -1.23356916e-02  1.11330599e-02  4.42048050e-02  6.53859135e-03\n",
      "  4.72902181e-03  1.86977088e-02 -2.90878154e-02  4.65991162e-03\n",
      "  2.57558003e-03 -2.43274625e-02 -1.91747062e-02  9.68340226e-03\n",
      " -5.38327321e-02  7.72943571e-02 -1.23340674e-02 -1.39941252e-03\n",
      " -1.91598162e-02  3.37586626e-02 -5.80420755e-02  6.47749146e-03\n",
      "  5.88047598e-03 -1.25942240e-03  2.36867759e-02  3.25877517e-02\n",
      "  3.55001837e-02 -3.32353823e-02  4.32332382e-02  1.14252465e-02\n",
      " -1.37306973e-02  5.58508676e-04 -4.38374951e-02 -4.31177430e-02\n",
      "  2.27002371e-02 -1.08064497e-02  1.22234961e-02 -2.27974858e-02\n",
      "  2.69942936e-02 -8.72965455e-02  4.23120596e-02 -1.23364506e-02\n",
      "  3.68462391e-02 -2.24115630e-03 -8.61508027e-03 -6.66196495e-02\n",
      " -2.16259677e-02  9.63442959e-03  3.96033190e-02 -9.06255543e-02\n",
      " -2.02792082e-02 -1.83419231e-02 -4.50078165e-03  3.80039625e-02\n",
      " -7.03426311e-03  1.38224475e-02  4.45462167e-02 -5.29578365e-02\n",
      "  3.25749745e-04 -1.89781375e-02 -8.82708207e-02 -2.16181781e-02\n",
      " -5.52318618e-03  3.37858871e-02 -3.95878889e-02 -7.04384502e-03\n",
      "  2.81860661e-02 -9.84685123e-03 -1.43244211e-02  2.55991984e-02\n",
      "  6.75271824e-02  6.46527708e-02 -5.11129387e-02  5.19175492e-02\n",
      " -1.57853272e-02 -1.56200235e-03  6.26118779e-02 -9.34498012e-03\n",
      " -8.84149849e-05 -3.51231135e-02  3.46243642e-02 -4.31087986e-03\n",
      "  2.52958741e-02 -7.84893036e-02  1.53837800e-02 -7.33629754e-03\n",
      "  1.32750068e-02 -1.15827676e-02  1.16930958e-02 -7.16161430e-02\n",
      " -3.20236087e-02 -1.08003113e-02  2.07232498e-03 -6.85923314e-03\n",
      " -2.37986948e-02  3.67761627e-02 -3.39296013e-02  4.02844772e-02\n",
      "  2.84563210e-02 -9.47130993e-02 -1.70994587e-02  3.03932987e-02\n",
      " -9.26400200e-02 -2.23155133e-02 -1.05461292e-03 -4.77502868e-03\n",
      "  5.02519011e-02 -2.67287642e-02 -1.97344720e-02 -2.41351128e-03\n",
      "  4.77260277e-02 -2.05750554e-03  7.83092529e-03 -2.56457496e-02\n",
      "  5.09714410e-02  4.13870737e-02  3.19360644e-02  7.84981158e-03\n",
      "  7.58009478e-02 -3.54966596e-02  1.01488270e-01 -2.52832230e-02\n",
      " -6.39281468e-03 -3.49854156e-02 -3.69307213e-02  5.06416373e-02\n",
      "  1.36472266e-02 -4.13364060e-02  3.31079364e-02  1.34848189e-02\n",
      " -3.93462414e-03 -5.26181720e-02 -2.15408076e-02 -5.63620403e-02\n",
      " -9.46792192e-04 -9.18579753e-03  8.57119784e-02 -4.43336479e-02\n",
      " -1.39774475e-02 -4.29182574e-02  3.82948644e-03 -1.01560280e-02\n",
      "  2.43306905e-02  2.79308786e-03  2.53226757e-02 -1.11908773e-02\n",
      " -1.11031026e-01  1.00716166e-02  3.37308720e-02 -3.19051766e-03\n",
      "  2.34519597e-02  3.25720124e-02 -4.81830761e-02 -1.66555345e-02\n",
      " -1.74819753e-02  5.36616109e-02  1.76263731e-02  4.94872630e-02\n",
      " -3.96546610e-02 -4.84619662e-02 -6.52604699e-02  3.18873152e-02\n",
      " -7.80272996e-03  5.44865280e-02  9.67995450e-03  4.42935154e-02\n",
      "  2.64919680e-02  4.36910382e-03 -1.15235724e-01 -4.74707820e-02\n",
      " -3.58013771e-02  6.38522953e-02 -1.25456266e-02  1.97242349e-02\n",
      "  2.35997457e-02  3.06572001e-02 -1.24634970e-02 -2.38370616e-02\n",
      " -9.10773035e-03 -2.05568131e-03 -6.00743399e-04  6.75559789e-02\n",
      "  4.05675508e-02 -5.64920157e-02 -9.92758572e-03 -9.63817351e-03\n",
      "  4.16152067e-02 -1.22759268e-02 -7.89963081e-02  3.67506631e-02\n",
      " -4.94728312e-02  5.03591187e-02  1.18724573e-02  6.74811676e-02\n",
      " -2.86566862e-03  1.68700516e-02  7.31362775e-03  2.60172877e-02\n",
      "  1.58470813e-02  6.29823282e-02  3.02402470e-02  1.99539438e-02\n",
      "  1.28335999e-02 -6.25512376e-02 -7.22569823e-02  3.83264311e-02\n",
      " -5.37748914e-03  2.94202939e-03  6.61651567e-02  4.53490950e-03\n",
      "  8.68646428e-03 -3.37273479e-02 -1.90926250e-02  6.05575033e-02\n",
      "  3.10163014e-02 -8.83600302e-03  4.05556597e-02  1.39546860e-02\n",
      " -4.25420962e-02 -8.38431297e-04 -3.23137492e-02 -2.25256686e-03\n",
      "  3.98349613e-02 -2.94450112e-02 -2.62961667e-02  3.25073041e-02\n",
      " -4.40992005e-02 -2.45306175e-02  1.00516062e-02  5.79220690e-02\n",
      " -3.27765308e-02 -2.93654315e-02 -3.95729318e-02 -4.18349653e-02\n",
      "  7.85630569e-03 -4.33267467e-02  2.57533565e-02  2.68054456e-02\n",
      " -6.66377274e-03 -4.73842137e-02  6.39312249e-03  4.41065468e-02\n",
      "  1.56671666e-02  5.44472365e-03 -3.62123512e-02 -4.10402678e-02\n",
      "  6.23032963e-03  1.95667278e-02 -2.18551792e-02  1.47492746e-02\n",
      " -1.70415044e-02 -1.90135296e-02 -7.92981009e-04 -3.69191058e-02\n",
      " -5.57348086e-03 -6.59317300e-02 -1.26405628e-02  1.65136363e-02\n",
      " -2.73985881e-02 -1.00930789e-02  3.60466130e-02  1.42403748e-02\n",
      "  5.45407459e-02 -1.75216980e-02 -6.13529272e-02  4.35160547e-02\n",
      " -3.46017182e-02 -6.04250655e-02  4.65894714e-02 -2.12061629e-02\n",
      "  9.17034522e-02  3.40809207e-03 -2.51403376e-02 -1.90434046e-02\n",
      "  6.32268097e-03  5.90704978e-02 -4.47293855e-02  4.29635420e-02\n",
      " -1.89008322e-02 -1.51639599e-02  2.63488255e-02  3.66073251e-02\n",
      " -1.13447336e-02 -2.83496045e-02 -2.78483350e-02  1.93125091e-03\n",
      " -3.16300467e-02  1.29435465e-01 -3.21113318e-02 -3.62546928e-02\n",
      " -2.17480026e-02 -3.75396758e-02  5.98449223e-02 -6.26928825e-03\n",
      " -4.33079489e-02 -3.18766907e-02 -3.86201846e-03  1.90646295e-02\n",
      "  5.81042804e-02 -1.06653431e-02 -2.37339688e-03 -1.19262924e-02\n",
      " -1.02159297e-02 -5.43072261e-02  3.86321209e-02 -2.44073048e-02\n",
      " -2.80684829e-02 -9.00446903e-03  1.16183236e-02 -6.98793828e-02\n",
      " -1.16777150e-02  3.22118625e-02  3.39050479e-02  4.55598384e-02\n",
      " -2.12054010e-02  8.16518720e-03  5.79289421e-02 -3.81503478e-02\n",
      " -4.43043094e-03  1.35337107e-03  1.10745169e-02 -2.10747700e-02\n",
      " -9.47950501e-03 -1.34912115e-02 -3.65765765e-03  4.97881956e-02\n",
      " -3.00230365e-02 -6.24600314e-02 -6.97974414e-02  1.93837881e-02\n",
      "  5.20540029e-02  3.08731049e-02  2.24625564e-33  9.80735291e-04\n",
      "  5.50294807e-03 -4.54840176e-02 -2.57982034e-03  5.53516671e-03\n",
      " -2.92656035e-03 -2.64072958e-02  2.00803187e-02  1.00955525e-02\n",
      " -1.00492714e-02  6.91963285e-02 -1.08639263e-02 -3.42691354e-02\n",
      "  1.46769136e-02 -6.40992522e-02 -3.62574086e-02  7.67651014e-03\n",
      "  4.40374948e-02  5.81924571e-03  7.66832288e-03 -2.25624572e-02\n",
      " -3.20991129e-02  7.86383152e-02 -1.35174394e-02  1.40205892e-02\n",
      " -2.88673514e-03  1.98903307e-02  1.72270220e-02  5.56149147e-03\n",
      " -5.56690851e-03 -1.97010413e-02  6.62108362e-02 -3.32453549e-02\n",
      " -7.76921958e-02  2.96486430e-02 -2.33435836e-02 -4.63043898e-03\n",
      "  4.05721813e-02  2.41771918e-02  4.01919335e-02 -1.88218318e-02\n",
      "  2.93938145e-02  1.52132721e-04  1.07136350e-02 -2.52685528e-02\n",
      "  5.90888644e-03  3.23564447e-02  9.34945513e-03 -3.90718784e-03\n",
      " -7.38908872e-02 -1.01162307e-02  3.66592593e-02 -4.23666947e-02\n",
      " -3.58617902e-02 -6.24363907e-02 -2.63717677e-02  1.18380487e-02\n",
      " -9.94323939e-03  9.82738379e-03  3.09555847e-02  3.50524555e-04\n",
      " -2.64366250e-02  4.91941422e-02  2.46406533e-02 -6.54783323e-02\n",
      " -9.38120205e-03  3.92585360e-02  5.31138619e-04  5.42595444e-05\n",
      " -2.70135738e-02  1.63600594e-02  1.97196919e-02  3.40841077e-02\n",
      "  3.38979028e-02 -3.54994118e-04  9.89753306e-02 -3.95267867e-02\n",
      "  4.47720252e-02 -2.20412761e-02 -4.98319929e-03  2.81845294e-02\n",
      " -3.76559347e-02 -3.21509279e-02 -9.70000587e-03  2.57118549e-02\n",
      " -5.88521548e-02  8.01836327e-02  8.49111006e-03  9.23721213e-03\n",
      "  1.74626708e-03  6.42001629e-02  6.52225837e-02  8.76695570e-03\n",
      "  1.72040449e-03 -1.04399752e-02 -2.06091739e-02 -7.64427707e-02\n",
      " -4.34605516e-02  9.45790485e-03  8.01980868e-03  7.25219995e-02\n",
      " -5.29142236e-03  2.94390861e-02 -1.78188588e-02  1.70220342e-02\n",
      " -1.51242304e-03 -1.51023148e-02  3.52623612e-02  1.94637533e-02\n",
      " -2.31502131e-02  1.07675018e-02 -5.00552682e-03 -1.44519377e-02\n",
      " -3.32182124e-02  6.20088279e-02  2.69659813e-02 -2.86766998e-02\n",
      "  2.92606074e-02 -4.01196033e-02 -1.31186005e-02 -3.34521681e-02\n",
      " -3.49293016e-02  5.56185246e-02  9.72344652e-02  5.45154745e-03\n",
      "  6.41311035e-02 -6.03784025e-02  4.45316284e-04  1.98593736e-02\n",
      " -2.06964882e-03  5.98608342e-04 -2.28845496e-02 -9.25562158e-03\n",
      "  3.43566798e-02  3.78562696e-02 -1.90134998e-02  1.34337712e-02\n",
      " -8.56055412e-03 -2.03252602e-02 -5.80456853e-02 -4.44501191e-02\n",
      "  1.63507974e-03 -4.48045991e-02 -1.19424034e-02 -3.38625088e-02\n",
      " -1.80710237e-02 -8.19313247e-03 -1.93339062e-03 -2.00767405e-02\n",
      " -3.53051946e-02 -1.53205253e-03 -3.26330471e-03 -5.33324778e-02\n",
      "  3.20943557e-02 -1.85389221e-02  3.17645632e-02  1.44379381e-02\n",
      "  7.38357753e-02 -7.68864341e-03 -1.10554351e-02  2.80031804e-02\n",
      "  5.94301336e-02 -2.12085415e-02 -3.47336456e-02  4.52917442e-02\n",
      "  6.40740106e-03 -1.46802757e-02  5.43359816e-02  4.42648456e-02\n",
      "  5.18207162e-05  5.64107671e-03  9.62776039e-03  1.64936930e-02\n",
      "  4.85213511e-02  7.52792358e-02 -3.76901925e-02  3.03656105e-02\n",
      "  9.27514024e-03 -2.65770108e-02 -5.47038205e-02 -7.40861346e-04\n",
      " -4.13001981e-03 -3.90815623e-02 -3.94909456e-02  3.02188341e-02\n",
      " -5.57051860e-02 -2.89235227e-02  5.76529885e-03 -2.58935266e-03\n",
      " -3.77671956e-03 -5.43502122e-02  6.19166810e-03  3.05840019e-02\n",
      " -2.51964829e-03 -3.82508077e-02 -1.11505864e-02  2.72627198e-03\n",
      "  9.51769948e-03 -4.39575426e-02 -1.25080987e-03 -4.34929691e-02\n",
      "  7.37353042e-03 -7.62937218e-03 -8.97918269e-02 -1.56955682e-02\n",
      " -2.55346261e-02  2.24405080e-02 -1.02870737e-03 -4.20513563e-02\n",
      "  1.38474144e-02  4.52309754e-03  3.81816961e-02  1.47458240e-02\n",
      "  4.98733446e-02 -4.20057622e-04 -3.25220055e-03  2.64411159e-02\n",
      " -2.03602016e-02  1.64234582e-02  1.23465266e-02 -2.38930341e-02\n",
      "  3.35369669e-02  2.92663602e-03 -1.94603913e-02 -2.28902845e-05\n",
      " -7.32492330e-03  7.77280657e-03  1.34164542e-02 -1.38965501e-02\n",
      " -2.82231029e-02 -3.17339785e-03  6.02337532e-02  5.67685366e-02\n",
      " -6.63361140e-03 -2.28223968e-02  4.38112468e-02  1.01254229e-02\n",
      "  3.78390774e-02  5.23660369e-02  1.65555123e-02  8.97497907e-02\n",
      " -5.78293391e-02 -1.89209655e-02 -2.17298102e-02 -2.32200846e-02\n",
      " -1.33678894e-02  3.87906097e-02  6.46334738e-02  2.98444182e-02\n",
      "  1.30476337e-02 -7.83440247e-02 -3.41035463e-02  1.34989358e-02\n",
      "  3.43383104e-02 -4.79843430e-02  8.87369215e-02  4.88414057e-02\n",
      " -5.89989312e-02 -4.80948761e-02 -2.57466938e-02 -4.63863537e-02\n",
      " -1.43878506e-02  5.75019931e-03 -1.98178878e-03  3.82988937e-02\n",
      " -1.79926306e-02  4.21510674e-02  7.12391734e-03  6.54197335e-02\n",
      "  7.99310505e-02  4.33803350e-02 -1.09676495e-02 -5.29075339e-02]\n",
      "Embedding size: (768,)\n"
     ]
    }
   ],
   "source": [
    "single_sentence = \"Yo! How cool are embeddings?\"\n",
    "single_embedding = embedding_model.encode(single_sentence)\n",
    "print(f\"Sentence: {single_sentence}\")\n",
    "print(f\"Embedding:\\n{single_embedding}\")\n",
    "print(f\"Embedding size: {single_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We've now got a way to numerically represent each of our chunks.\n",
    "\n",
    "Our embedding has a shape of `(768,)` meaning it's a vector of 768 numbers which represent our text in high-dimensional space, too many for a human to comprehend but machines love high-dimensional space.\n",
    "\n",
    "> **Note:** No matter the size of the text input to our `all-mpnet-base-v2` model, it will be turned into an embedding size of `(768,)`. This value is fixed. So whether a sentence is 1 token long or 1000 tokens long, it will be truncated/padded with zeros to size 384 and then turned into an embedding vector of size `(768,)`. Of course, other embedding models may have different input/output shapes.\n",
    "\n",
    "How about we add an embedding field to each of our chunk items?\n",
    "\n",
    "Let's start by trying to create embeddings on the CPU, we'll time it with the `%%time` magic to see how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Uncomment to see how long it takes to create embeddings on CPU\n",
    "# # Make sure the model is on the CPU\n",
    "# embedding_model.to(\"cpu\")\n",
    "\n",
    "# # Embed each chunk one by one\n",
    "# for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "#     item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok not too bad... but this would take a *really* long time if we had a larger dataset.\n",
    "\n",
    "Now let's see how long it takes to create the embeddings with a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 359/359 [00:04<00:00, 80.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 531 ms\n",
      "Wall time: 4.48 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Send the model to the GPU\n",
    "embedding_model.to(\"cuda\") \n",
    "\n",
    "# Create embeddings one by one on the GPU\n",
    "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! Looks like the embeddings get created much faster (~10x faster on my machine) on the GPU!\n",
    "\n",
    "You'll likely notice this trend with many of your deep learning workflows. If you have access to a GPU, especially a NVIDIA GPU, you should use one if you can.\n",
    "\n",
    "But what if I told you we could go faster again?\n",
    "\n",
    "You see many modern models can handle batched predictions.\n",
    "\n",
    "This means computing on multiple samples at once.\n",
    "\n",
    "Those are the types of operations where a GPU flourishes!\n",
    "\n",
    "We can perform batched operations by turning our target text samples into a single list and then passing that list to our embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn text chunks into a single list\n",
    "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1 s\n",
      "Wall time: 2.77 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0367,  0.0008, -0.0005,  ...,  0.0627, -0.0265,  0.0243],\n",
       "        [ 0.0100, -0.0654, -0.0008,  ..., -0.0387, -0.0191,  0.0307],\n",
       "        [ 0.0251, -0.0942,  0.0069,  ..., -0.0791, -0.0405, -0.0309],\n",
       "        ...,\n",
       "        [-0.0005, -0.0835,  0.0177,  ...,  0.0777, -0.0475, -0.0174],\n",
       "        [ 0.0092, -0.0777,  0.0093,  ...,  0.0333, -0.0162,  0.0147],\n",
       "        [-0.0340, -0.0509, -0.0146,  ...,  0.0073, -0.0708,  0.0157]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Embed all texts in batches\n",
    "text_chunk_embeddings = embedding_model.encode(text_chunks,\n",
    "                                               batch_size=32, # you can use different batch sizes here for speed/performance, I found 32 works well for this use case\n",
    "                                               convert_to_tensor=True) # optional to return embeddings as tensor instead of array\n",
    "\n",
    "text_chunk_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's what I'm talking about!\n",
    "\n",
    "A ~4x improvement (on my GPU) in speed thanks to batched operations.\n",
    "\n",
    "So the tip here is to use a GPU when you can and use batched operations if you can too.\n",
    "\n",
    "Now let's save our chunks and their embeddings so we could import them later if we wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save embeddings to file\n",
    "\n",
    "Since creating embeddings can be a timely process (not so much for our case but it can be for more larger datasets), let's turn our `pages_and_chunks_over_min_token_len` list of dictionaries into a DataFrame and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to file\n",
    "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)\n",
    "embeddings_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
    "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can make sure it imports nicely by loading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>HP 1 - Harry Potter and the Sorcerer's Stone H...</td>\n",
       "      <td>139</td>\n",
       "      <td>29</td>\n",
       "      <td>34.75</td>\n",
       "      <td>[ 3.67058441e-02  8.42662819e-04 -4.71975218e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>CHAPTER ONE  THE BOY WHO LIVED     M r. and Mr...</td>\n",
       "      <td>2359</td>\n",
       "      <td>440</td>\n",
       "      <td>589.75</td>\n",
       "      <td>[ 9.96456947e-03 -6.54456615e-02 -7.66414392e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>something peculiar — a cat reading a map. For ...</td>\n",
       "      <td>2068</td>\n",
       "      <td>389</td>\n",
       "      <td>517.00</td>\n",
       "      <td>[ 2.51424816e-02 -9.42336842e-02  6.89432491e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Most of them had never seen an owl even at nig...</td>\n",
       "      <td>812</td>\n",
       "      <td>155</td>\n",
       "      <td>203.00</td>\n",
       "      <td>[ 1.36089241e-02 -7.33762309e-02  4.56689438e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>“ — yes, their son, Harry —”    Mr. Dursley st...</td>\n",
       "      <td>1863</td>\n",
       "      <td>355</td>\n",
       "      <td>465.75</td>\n",
       "      <td>[-1.35985250e-02 -7.10061342e-02 -1.67870596e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number                                     sentence_chunk  \\\n",
       "0            2  HP 1 - Harry Potter and the Sorcerer's Stone H...   \n",
       "1            4  CHAPTER ONE  THE BOY WHO LIVED     M r. and Mr...   \n",
       "2            5  something peculiar — a cat reading a map. For ...   \n",
       "3            5  Most of them had never seen an owl even at nig...   \n",
       "4            6  “ — yes, their son, Harry —”    Mr. Dursley st...   \n",
       "\n",
       "   chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
       "0               139                29              34.75   \n",
       "1              2359               440             589.75   \n",
       "2              2068               389             517.00   \n",
       "3               812               155             203.00   \n",
       "4              1863               355             465.75   \n",
       "\n",
       "                                           embedding  \n",
       "0  [ 3.67058441e-02  8.42662819e-04 -4.71975218e-...  \n",
       "1  [ 9.96456947e-03 -6.54456615e-02 -7.66414392e-...  \n",
       "2  [ 2.51424816e-02 -9.42336842e-02  6.89432491e-...  \n",
       "3  [ 1.36089241e-02 -7.33762309e-02  4.56689438e-...  \n",
       "4  [-1.35985250e-02 -7.10061342e-02 -1.67870596e-...  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import saved file and view\n",
    "text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\n",
    "text_chunks_and_embedding_df_load.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking and embedding questions\n",
    "\n",
    "> **Which embedding model should I use?**\n",
    "\n",
    "This depends on many factors. My best advice is to experiment, experiment, experiment! \n",
    "\n",
    "If you want the model to run locally, you'll have to make sure it's feasible to run on your own hardware. \n",
    "\n",
    "A good place to see how different models perform on a wide range of embedding tasks is the [Hugging Face Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n",
    "\n",
    "> **What other forms of text chunking/splitting are there?**\n",
    "\n",
    "There are a fair few options here too. We've kept it simple with groups of sentences.\n",
    "\n",
    "For more, [Pinecone has a great guide on different kinds of chunking](https://www.pinecone.io/learn/chunking-strategies/) including for different kinds of data such as markdown and LaTeX.\n",
    "\n",
    "Libraries such as [LangChain also have a good amount of in-built text splitting options](https://python.langchain.com/docs/modules/data_connection/document_transformers/).\n",
    "\n",
    "> **What should I think about when creating my embeddings?**\n",
    "\n",
    "Our model turns text inputs up to 384 tokens long in embedding vectors of size 768.\n",
    "\n",
    "Generally, the larger the vector size, the more information that gets encoded into the embedding (however, this is not always the case, as smaller, better models can outperform larger ones).\n",
    "\n",
    "Though with larger vector sizes comes larger storage and compute requirements.\n",
    "\n",
    "Our model is also relatively small (420MB) in size compared to larger models that are available.\n",
    "\n",
    "Larger models may result in better performance but will also require more compute.\n",
    "\n",
    "So some things to think about:\n",
    "* Size of input - If you need to embed longer sequences, choose a model with a larger input capacity.\n",
    "* Size of embedding vector - Larger is generally a better representation but requires more compute/storage.\n",
    "* Size of model - Larger models generally result in better embeddings but require more compute power/time to run.\n",
    "* Open or closed - Open models allow you to run them on your own hardware whereas closed models can be easier to setup but require an API call to get embeddings.\n",
    "\n",
    "> **Where should I store my embeddings?**\n",
    "\n",
    "If you've got a relatively small dataset, for example, under 100,000 examples (this number is rough and only based on first hand experience), `np.array` or `torch.tensor` can work just fine as your dataset.\n",
    "\n",
    "But if you've got a production system and want to work with 100,000+ embeddings, you may want to look into a [vector database]( https://en.wikipedia.org/wiki/Vector_database) (these have become very popular lately and there are many offerings).\n",
    "\n",
    "### Document Ingestion and Embedding Creation Extensions\n",
    "\n",
    "One major extension to the workflow above would to functionize it.\n",
    "\n",
    "Or turn it into a script.\n",
    "\n",
    "As in, take all the functionality we've created and package it into a single process (e.g. go from document -> embeddings file).\n",
    "\n",
    "So you could input a document on one end and have embeddings come out the other end. The hardest part of this is knowing what kind of preprocessing your text may need before it's turned into embeddings. Cleaner text generally means better results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RAG - Search and Answer\n",
    "\n",
    "We discussed RAG briefly in the beginning but let's quickly recap.\n",
    "\n",
    "RAG stands for Retrieval Augmented Generation.\n",
    "\n",
    "Which is another way of saying \"given a query, search for relevant resources and answer based on those resources\".\n",
    "\n",
    "Let's breakdown each step:\n",
    "* **Retrieval** - Get relevant resources given a query. For example, if the query is \"what are the macronutrients?\" the ideal results will contain information about protein, carbohydrates and fats (and possibly alcohol) rather than information about which tractors are the best for farming (though that is also cool information).\n",
    "* **Augmentation** - LLMs are capable of generating text given a prompt. However, this generated text is designed to *look* right. And it often has some correct information, however, they are prone to hallucination (generating a result that *looks* like legit text but is factually wrong). In augmentation, we pass relevant information into the prompt and get an LLM to use that relevant information as the basis of its generation.\n",
    "* **Generation** - This is where the LLM will generate a response that has been flavoured/augmented with the retrieved resources. In turn, this not only gives us a potentially more correct answer, it also gives us resources to investigate more (since we know which resources went into the prompt).\n",
    "\n",
    "The whole idea of RAG is to get an LLM to be more factually correct based on your own input as well as have a reference to where the generated output may have come from.\n",
    "\n",
    "This is an incredibly helpful tool.\n",
    "\n",
    "Let's say you had 1000s of customer support documents.\n",
    "\n",
    "You could use RAG to generate direct answers to questions with links to relevant documentation.\n",
    "\n",
    "Or you were an insurance company with large chains of claims emails.\n",
    "\n",
    "You could use RAG to answer questions about the emails with sources.\n",
    "\n",
    "One helpful analogy is to think of LLMs as calculators for words.\n",
    "\n",
    "With good inputs, the LLM can sort them into helpful outputs.\n",
    "\n",
    "How? \n",
    "\n",
    "It starts with better search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity search\n",
    "\n",
    "Similarity search or semantic search or vector search is the idea of searching on *vibe*.\n",
    "\n",
    "If this sounds like woo, woo. It's not.\n",
    "\n",
    "Perhaps searching via *meaning* is a better analogy.\n",
    "\n",
    "With keyword search, you are trying to match the string \"apple\" with the string \"apple\".\n",
    "\n",
    "Whereas with similarity/semantic search, you may want to search \"macronutrients functions\".\n",
    "\n",
    "And get back results that don't necessarily contain the words \"macronutrients functions\" but get back pieces of text that match that meaning.\n",
    "\n",
    "> **Example:** Using similarity search on our textbook data with the query \"macronutrients function\" returns a paragraph that starts with: \n",
    ">\n",
    ">*There are three classes of macronutrients: carbohydrates, lipids, and proteins. These can be metabolically processed into cellular energy. The energy from macronutrients comes from their chemical bonds. This chemical energy is converted into cellular energy that is then utilized to perform work, allowing our bodies to conduct their basic functions.*\n",
    "> \n",
    "> as the first result. How cool!\n",
    "\n",
    "If you've ever used Google, you know this kind of workflow.\n",
    "\n",
    "But now we'd like to perform that across our own data.\n",
    "\n",
    "Let's import our embeddings we created earlier (tk -link to embedding file) and prepare them for use by turning them into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([359, 768])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Import texts and embedding df\n",
    "text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
    "\n",
    "# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
    "embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>HP 1 - Harry Potter and the Sorcerer's Stone H...</td>\n",
       "      <td>139</td>\n",
       "      <td>29</td>\n",
       "      <td>34.75</td>\n",
       "      <td>[0.0367058441, 0.000842662819, -0.000471975218...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>CHAPTER ONE  THE BOY WHO LIVED     M r. and Mr...</td>\n",
       "      <td>2359</td>\n",
       "      <td>440</td>\n",
       "      <td>589.75</td>\n",
       "      <td>[0.00996456947, -0.0654456615, -0.000766414392...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>something peculiar — a cat reading a map. For ...</td>\n",
       "      <td>2068</td>\n",
       "      <td>389</td>\n",
       "      <td>517.00</td>\n",
       "      <td>[0.0251424816, -0.0942336842, 0.00689432491, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Most of them had never seen an owl even at nig...</td>\n",
       "      <td>812</td>\n",
       "      <td>155</td>\n",
       "      <td>203.00</td>\n",
       "      <td>[0.0136089241, -0.0733762309, 0.00456689438, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>“ — yes, their son, Harry —”    Mr. Dursley st...</td>\n",
       "      <td>1863</td>\n",
       "      <td>355</td>\n",
       "      <td>465.75</td>\n",
       "      <td>[-0.013598525, -0.0710061342, -0.0167870596, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number                                     sentence_chunk  \\\n",
       "0            2  HP 1 - Harry Potter and the Sorcerer's Stone H...   \n",
       "1            4  CHAPTER ONE  THE BOY WHO LIVED     M r. and Mr...   \n",
       "2            5  something peculiar — a cat reading a map. For ...   \n",
       "3            5  Most of them had never seen an owl even at nig...   \n",
       "4            6  “ — yes, their son, Harry —”    Mr. Dursley st...   \n",
       "\n",
       "   chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
       "0               139                29              34.75   \n",
       "1              2359               440             589.75   \n",
       "2              2068               389             517.00   \n",
       "3               812               155             203.00   \n",
       "4              1863               355             465.75   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.0367058441, 0.000842662819, -0.000471975218...  \n",
       "1  [0.00996456947, -0.0654456615, -0.000766414392...  \n",
       "2  [0.0251424816, -0.0942336842, 0.00689432491, -...  \n",
       "3  [0.0136089241, -0.0733762309, 0.00456689438, -...  \n",
       "4  [-0.013598525, -0.0710061342, -0.0167870596, -...  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks_and_embedding_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.6706e-02,  8.4266e-04, -4.7198e-04, -2.9431e-02, -1.0796e-02,\n",
       "         7.6247e-02,  1.8942e-02,  2.3837e-02,  5.1251e-02,  7.5838e-02,\n",
       "        -5.4138e-03, -1.8103e-03, -3.3685e-04,  7.7356e-02, -1.6724e-02,\n",
       "        -2.4026e-02, -2.7088e-02, -1.1976e-03,  1.5567e-02, -1.4751e-04,\n",
       "         8.3372e-02,  1.9163e-03,  4.3073e-03, -3.3794e-02, -1.1898e-02,\n",
       "         7.5453e-02,  3.7420e-03, -3.0911e-02,  8.0322e-04, -4.7493e-02,\n",
       "        -7.3041e-03, -2.3919e-02, -2.1128e-02,  4.5088e-02,  6.8876e-02,\n",
       "         1.6760e-02,  5.0734e-02, -1.3727e-02,  1.7147e-02, -7.6768e-02,\n",
       "         1.9313e-02, -1.7395e-02, -1.6936e-02,  2.0743e-02, -1.5015e-02,\n",
       "        -8.1425e-03, -2.5242e-02,  3.9910e-02,  4.6012e-02,  3.6986e-02,\n",
       "         4.2021e-02, -1.7712e-02,  4.5784e-02,  3.6894e-02, -5.5929e-03,\n",
       "         7.3783e-02, -2.9931e-02,  1.0030e-02, -5.6403e-02, -9.8046e-03,\n",
       "         3.6481e-02, -1.9683e-02,  3.5150e-03,  1.0299e-02, -1.2798e-02,\n",
       "        -2.2385e-02,  1.3475e-02,  5.0692e-03, -1.8019e-03,  2.9440e-03,\n",
       "         4.1556e-02,  5.0111e-02,  1.3665e-02, -7.1933e-03, -1.6524e-03,\n",
       "         1.0884e-02, -5.2589e-03, -3.8146e-02, -4.0482e-02, -3.2536e-02,\n",
       "        -2.5408e-02, -5.5934e-02, -1.4523e-02,  3.6443e-03,  6.8758e-03,\n",
       "        -1.5000e-02, -1.1894e-02, -8.8668e-02,  1.0562e-02,  5.5761e-03,\n",
       "         4.5247e-02,  8.4463e-02,  1.7576e-02,  4.0510e-02, -8.5732e-03,\n",
       "         2.7793e-02, -6.5828e-02, -4.3947e-02, -1.5224e-02,  7.2441e-02,\n",
       "         6.2851e-04, -2.7230e-02, -1.5316e-02,  3.2509e-02,  4.4348e-02,\n",
       "         5.3317e-02, -4.1752e-02, -1.8720e-02,  7.4707e-03,  3.2195e-02,\n",
       "        -5.9824e-03, -2.8020e-02, -7.7960e-03, -4.7892e-02,  6.0073e-02,\n",
       "         1.1704e-02,  1.7842e-03, -1.4242e-02,  4.8778e-03, -6.1064e-02,\n",
       "         1.2854e-02,  1.1965e-02,  1.8228e-02,  7.4337e-02, -7.6111e-02,\n",
       "         4.7557e-02, -1.4091e-02, -1.5274e-02,  8.8875e-03, -3.1711e-02,\n",
       "         2.1776e-02, -7.1625e-02,  1.8098e-02, -6.7427e-02, -5.3700e-02,\n",
       "        -1.1280e-03,  1.9678e-02,  3.0558e-02,  4.5060e-03,  2.1210e-02,\n",
       "        -2.5800e-02,  4.0802e-03,  2.8010e-02,  6.2744e-02, -1.5697e-02,\n",
       "        -3.0718e-02,  2.1471e-02,  6.2458e-03,  6.3488e-02, -2.2462e-02,\n",
       "         3.8460e-02,  3.5285e-02, -1.1553e-02,  1.6796e-02, -2.1352e-02,\n",
       "        -3.4023e-02, -7.4491e-02, -4.2403e-02, -3.1447e-02, -4.7305e-02,\n",
       "         1.5002e-02,  1.0692e-02, -1.6819e-02, -2.4667e-03,  1.0869e-02,\n",
       "        -2.5708e-02,  1.0473e-02,  3.7567e-03, -8.8376e-03, -4.3755e-03,\n",
       "        -3.1857e-02,  1.4935e-02,  2.8728e-02, -6.3609e-02,  4.4492e-02,\n",
       "         2.8060e-02, -8.2294e-02, -1.5873e-02, -1.2188e-02,  1.1811e-02,\n",
       "        -3.2482e-02,  4.2984e-02,  2.7778e-02, -2.0380e-02, -3.5679e-03,\n",
       "         1.2928e-02,  1.5654e-02, -9.4817e-03, -1.4982e-02, -4.8533e-02,\n",
       "         1.3440e-02, -3.9054e-02,  1.0310e-03,  2.7688e-02,  3.0940e-02,\n",
       "         1.5393e-02, -9.9843e-03,  2.7498e-03,  2.9970e-02, -1.8371e-02,\n",
       "         2.8652e-03,  3.9914e-03, -2.8579e-02,  3.0013e-03,  4.1545e-04,\n",
       "        -3.2937e-02, -1.5991e-02,  3.0668e-02, -3.8942e-02, -2.2686e-03,\n",
       "        -3.0809e-02,  2.9602e-02, -3.0139e-02,  6.0927e-03, -2.6942e-02,\n",
       "        -5.8875e-02,  1.4464e-02, -1.0560e-01, -5.4293e-02, -5.4156e-02,\n",
       "        -7.1416e-03,  7.1902e-03,  1.7036e-02,  1.6277e-02, -1.8135e-03,\n",
       "        -5.8119e-03,  1.1434e-03,  6.2994e-02, -2.6290e-02,  5.9226e-03,\n",
       "        -2.6017e-02,  3.2559e-02, -5.4551e-02, -7.2957e-02, -2.7334e-02,\n",
       "        -2.7208e-02, -2.0141e-02, -1.4128e-02, -9.6194e-03,  3.7590e-02,\n",
       "        -8.6935e-03,  2.0703e-03, -1.8756e-02, -2.9089e-03,  5.6249e-03,\n",
       "         1.6514e-02,  3.3579e-02,  2.7547e-02,  5.0674e-03, -4.3507e-02,\n",
       "         7.0019e-02,  1.1883e-02, -3.3594e-02,  3.3816e-03, -2.4888e-02,\n",
       "        -2.3024e-02, -2.2604e-02,  7.5638e-02, -2.6287e-02,  5.5858e-02,\n",
       "         4.1151e-02, -4.2720e-03, -3.2855e-02, -9.7393e-03,  1.1955e-02,\n",
       "         3.8034e-02,  1.1007e-02,  1.2986e-02,  8.3679e-03, -6.8344e-03,\n",
       "        -2.6600e-02,  1.7901e-02, -5.7299e-02,  5.5392e-02, -6.0218e-02,\n",
       "        -1.4965e-02,  3.6193e-02,  3.8051e-02,  1.3528e-02, -1.0784e-02,\n",
       "         3.6351e-02, -4.9521e-02, -6.9752e-02,  1.7146e-03, -2.5028e-02,\n",
       "         6.2114e-02, -2.4142e-02,  5.3135e-02, -2.1577e-02,  4.3093e-03,\n",
       "        -2.1578e-02, -4.1794e-02, -2.7726e-02,  4.1025e-02,  6.0369e-02,\n",
       "         3.3412e-02, -2.1838e-02, -2.7139e-02,  2.8500e-02, -8.8996e-03,\n",
       "        -7.1629e-02, -8.1359e-03,  3.4779e-02,  1.6387e-02, -3.0666e-02,\n",
       "        -2.4538e-02, -5.2155e-03, -7.4435e-02, -7.1403e-02, -2.6410e-02,\n",
       "        -4.9569e-02,  3.6975e-02, -2.2159e-03, -2.0932e-02, -6.3467e-02,\n",
       "        -3.5354e-02, -5.0593e-02, -2.7205e-02,  2.3943e-02,  2.3171e-02,\n",
       "         3.5228e-02, -3.6419e-02, -2.3012e-02, -9.7764e-03,  6.3020e-02,\n",
       "        -7.4694e-03,  3.5262e-02, -3.2553e-02, -7.0962e-02,  4.2926e-02,\n",
       "         3.1433e-02, -6.8944e-02, -9.3362e-02, -2.8681e-04,  1.9541e-02,\n",
       "         6.6541e-02, -1.1476e-02, -1.1099e-02,  8.5747e-03,  6.9369e-02,\n",
       "        -3.5877e-02,  1.9674e-02, -7.9000e-02, -1.5936e-02,  4.1913e-02,\n",
       "         3.9899e-02,  3.0866e-02, -4.7186e-02, -1.4897e-02,  1.3443e-02,\n",
       "         2.5645e-02, -9.4772e-03,  5.5663e-02,  3.6020e-02, -5.0740e-02,\n",
       "         4.4585e-02,  4.4349e-02, -6.0562e-02, -5.7476e-02, -1.4792e-02,\n",
       "         3.5922e-02,  3.4099e-02, -4.6478e-03,  2.4586e-02,  4.7727e-02,\n",
       "        -3.5858e-02,  2.2462e-03,  3.3963e-02, -2.9070e-02, -4.3485e-02,\n",
       "         6.0922e-03,  2.6413e-02,  4.7105e-02,  3.2280e-02,  4.8342e-03,\n",
       "        -1.0616e-02,  2.2307e-02, -6.8148e-02,  6.7505e-02, -8.3360e-03,\n",
       "        -1.1405e-02, -3.2761e-02,  2.9086e-02,  6.8654e-03, -2.3265e-02,\n",
       "        -3.0705e-02, -1.9515e-02, -6.7785e-03,  5.0891e-02, -8.1423e-03,\n",
       "        -3.3446e-03, -2.7151e-02,  5.8398e-02, -3.9770e-02,  2.6231e-02,\n",
       "         4.1504e-02, -6.9336e-03,  1.2754e-04,  1.0206e-02, -4.0813e-02,\n",
       "         5.9591e-02,  6.6757e-02, -2.7181e-02,  8.3081e-02, -3.5283e-02,\n",
       "         1.1223e-01, -1.7489e-03, -1.5404e-02,  1.6661e-02, -4.5081e-02,\n",
       "         7.8907e-02, -1.4317e-02, -1.1624e-02, -4.9360e-02, -6.3197e-03,\n",
       "         1.7460e-02, -2.3866e-02,  2.0590e-02,  4.9436e-02, -4.5624e-02,\n",
       "         2.3360e-02, -3.4373e-02, -3.0566e-02,  4.2996e-02, -2.9903e-02,\n",
       "         3.5784e-02, -1.9280e-02, -2.7803e-02,  8.9327e-03,  1.9357e-02,\n",
       "        -1.8866e-02, -1.7709e-02,  9.8190e-02, -2.7200e-03,  5.0591e-03,\n",
       "        -2.4585e-02,  1.3839e-03, -3.8166e-02,  8.6449e-03, -6.2454e-03,\n",
       "        -6.9058e-02, -2.1228e-02,  6.7247e-02, -6.5057e-02,  2.3572e-02,\n",
       "         2.7597e-02, -1.2389e-02, -6.8281e-03, -9.7497e-03, -3.4680e-02,\n",
       "         2.2221e-03, -9.2858e-02, -3.5698e-02, -8.4085e-02, -4.0406e-02,\n",
       "         7.7389e-02, -6.2979e-03,  8.8136e-03,  1.2824e-02,  6.2132e-02,\n",
       "         1.1017e-02, -5.5175e-02,  1.6489e-02,  5.5418e-02,  4.1691e-02,\n",
       "        -3.6605e-02, -1.1064e-02, -5.5547e-02,  4.8154e-02,  2.6515e-02,\n",
       "         5.9399e-03,  4.0624e-03, -1.4616e-02, -3.4146e-02, -6.1257e-03,\n",
       "         5.5866e-04,  2.6579e-02,  6.1932e-02,  2.1087e-03,  1.9794e-02,\n",
       "         5.2761e-02,  1.9393e-02,  7.6975e-03,  3.3980e-02, -4.6619e-03,\n",
       "        -2.1420e-02, -4.5101e-02,  1.5604e-02, -1.4993e-02,  3.2164e-02,\n",
       "        -2.1774e-02,  1.2788e-02, -5.3727e-02,  9.2795e-03,  2.2714e-33,\n",
       "         4.1723e-02, -2.7220e-02, -4.2596e-03,  9.8646e-02, -4.6896e-02,\n",
       "         6.0887e-03, -9.3181e-03,  1.5235e-02,  4.4021e-04,  5.3802e-03,\n",
       "         4.2119e-02,  2.5354e-02,  1.9168e-02,  3.3586e-02,  2.0460e-02,\n",
       "         3.6513e-02, -1.5756e-02, -5.0785e-02,  8.5700e-03,  1.2741e-02,\n",
       "        -2.3780e-02,  3.3348e-02, -2.5550e-02,  6.6126e-04, -4.4554e-03,\n",
       "        -1.6404e-02,  1.2757e-02,  3.7083e-02,  2.8509e-02, -4.2693e-03,\n",
       "        -2.1220e-02, -4.6020e-03,  4.3447e-02, -1.4105e-03, -2.8626e-02,\n",
       "         3.5910e-03, -2.7887e-02,  9.8307e-03, -3.2523e-02, -1.5991e-02,\n",
       "        -3.0948e-02, -5.3711e-02, -2.4474e-02,  1.7433e-02,  3.1687e-02,\n",
       "         1.7487e-02, -1.3442e-02, -1.0072e-01,  2.8883e-02,  1.0414e-01,\n",
       "        -9.0930e-03,  9.1957e-02,  9.8584e-03,  3.4985e-03, -2.2455e-02,\n",
       "        -9.7886e-03, -2.8654e-02,  1.4152e-03,  1.0216e-02,  5.4533e-02,\n",
       "        -6.8371e-03,  3.1499e-02, -3.7245e-03,  1.1571e-02, -1.7336e-02,\n",
       "        -1.6922e-02,  7.1038e-02,  9.0368e-04, -1.4311e-02,  5.8175e-02,\n",
       "         5.5606e-02, -4.6722e-03, -4.4712e-02, -1.8121e-02, -7.7471e-03,\n",
       "        -1.2341e-02,  4.1125e-02, -4.2657e-03, -1.7205e-03, -2.6631e-03,\n",
       "        -4.3931e-02,  3.1801e-03,  2.8319e-02,  7.7164e-03,  2.2675e-03,\n",
       "         1.0647e-02,  3.2048e-02,  3.2543e-02, -8.1933e-02, -1.8686e-02,\n",
       "        -4.0261e-02,  5.2736e-02,  6.5435e-02,  6.0797e-04,  5.1491e-02,\n",
       "         1.0174e-02, -3.4584e-02,  1.6686e-02, -8.0405e-03,  2.0678e-02,\n",
       "         5.9038e-02, -1.9006e-02, -2.6725e-02, -2.1368e-02, -2.0049e-02,\n",
       "         2.2500e-02, -3.0439e-02, -1.6194e-02, -2.4520e-03, -2.9317e-03,\n",
       "        -3.6410e-02, -1.7148e-02, -1.0576e-02, -1.1795e-01,  1.0216e-01,\n",
       "         4.5381e-03,  2.3600e-02, -1.2365e-02,  2.1734e-02,  5.9063e-02,\n",
       "         4.2993e-02,  1.6618e-03,  4.3947e-03,  3.0426e-02,  2.7920e-02,\n",
       "        -1.9189e-02, -1.7213e-02, -3.1873e-02,  8.3914e-03, -4.2360e-03,\n",
       "        -2.7246e-02, -2.1348e-02, -2.3083e-02, -3.9077e-02, -9.0884e-03,\n",
       "         2.5843e-02,  6.3585e-02,  8.0046e-03, -1.3764e-02,  2.7611e-02,\n",
       "         2.5692e-02, -3.2443e-02,  5.9631e-02,  3.9278e-02, -4.0940e-02,\n",
       "        -3.3522e-02,  5.5524e-02, -4.7594e-02, -1.0157e-01,  3.5075e-03,\n",
       "        -4.8733e-02, -4.1413e-02, -5.3022e-02,  2.9831e-03,  2.9924e-02,\n",
       "         3.2130e-02, -2.2280e-02,  1.0267e-02, -4.6799e-02,  5.2447e-02,\n",
       "        -3.2205e-03,  5.2530e-03, -3.5439e-02, -2.3573e-02,  1.4596e-02,\n",
       "         1.9397e-02, -2.2896e-02,  3.8344e-02,  1.5291e-02, -1.5584e-02,\n",
       "        -4.4700e-02,  1.9237e-02,  2.1013e-02,  3.6023e-02,  3.5979e-02,\n",
       "         8.2766e-02, -1.9611e-02,  6.9202e-02, -7.1913e-03, -2.9025e-02,\n",
       "        -3.1605e-04, -1.9122e-02, -3.7672e-02, -7.3448e-03, -2.6329e-02,\n",
       "        -4.9891e-02,  1.1035e-03, -4.2399e-02, -4.4314e-02, -1.6289e-03,\n",
       "        -2.7257e-02,  1.7621e-02,  3.6052e-02, -1.2659e-02, -3.6596e-02,\n",
       "        -1.6046e-02, -3.3983e-02, -3.6778e-02, -2.1169e-02, -6.3166e-02,\n",
       "         1.5738e-02, -1.0633e-02,  5.5971e-04, -1.3377e-03,  8.3947e-02,\n",
       "        -3.5592e-02,  4.0018e-02, -2.6303e-02, -1.0947e-02, -1.0635e-02,\n",
       "        -2.8671e-02,  4.9702e-02, -5.0926e-02, -1.6510e-02, -2.3270e-02,\n",
       "        -1.0278e-02,  3.8460e-02,  1.0760e-02, -8.8866e-03,  6.3674e-02,\n",
       "        -1.0777e-02, -7.4490e-02,  9.1801e-03, -1.7284e-02,  1.3648e-02,\n",
       "         7.9827e-02, -9.6743e-02, -5.3156e-03, -1.1123e-02,  2.1572e-02,\n",
       "        -1.2875e-02,  4.5737e-02, -1.5559e-02, -4.1203e-02,  6.7797e-04,\n",
       "         2.8429e-02,  5.3035e-03, -2.2954e-02,  3.4440e-02,  8.9063e-02,\n",
       "        -2.1328e-02,  2.3114e-02, -4.6949e-03, -5.2872e-02, -2.8635e-02,\n",
       "        -2.9524e-02, -2.7655e-02,  2.1471e-02, -2.2357e-02, -1.0139e-04,\n",
       "         2.2196e-02,  2.7230e-02,  1.9672e-02, -2.8806e-02, -4.3341e-02,\n",
       "         3.4299e-02,  5.6328e-02,  6.5463e-02,  3.0887e-02, -4.5565e-02,\n",
       "         6.0258e-03,  2.9562e-02, -5.2674e-03, -1.1537e-02, -1.5966e-02,\n",
       "        -1.4903e-02, -1.7805e-02, -3.6417e-03,  4.7051e-02, -5.0384e-03,\n",
       "         6.2693e-02, -2.6542e-02,  2.4255e-02], device='cuda:0')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!\n",
    "\n",
    "Now let's prepare another instance of our embedding model. Not because we have to but because we'd like to make it so you can start the notebook from the cell above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=device) # choose the device to load the model to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding model ready!\n",
    "\n",
    "Time to perform a semantic search.\n",
    "\n",
    "Let's say you were studying the macronutrients.\n",
    "\n",
    "And wanted to search your textbook for \"macronutrients functions\".\n",
    "\n",
    "Well, we can do so with the following steps:\n",
    "1. Define a query string (e.g. `\"macronutrients functions\"`) - note: this could be anything, specific or not.\n",
    "2. Turn the query string in an embedding with same model we used to embed our text chunks.\n",
    "3. Perform a [dot product](https://pytorch.org/docs/stable/generated/torch.dot.html) or [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) function between the text embeddings and the query embedding (we'll get to what these are shortly) to get similarity scores.\n",
    "4. Sort the results from step 3 in descending order (a higher score means more similarity in the eyes of the model) and use these values to inspect the texts. \n",
    "\n",
    "Easy!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: macronutrients functions\n",
      "Time take to get scores on 359 embeddings: 0.07552 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.0831, 0.0817, 0.0754, 0.0695, 0.0680], device='cuda:0'),\n",
       "indices=tensor([168, 339,  54, 169, 219], device='cuda:0'))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Define the query\n",
    "# Note: This could be anything. But since we're working with a nutrition textbook, we'll stick with nutrition-based queries.\n",
    "query = \"macronutrients functions\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# 2. Embed the query to the same numerical space as the text examples \n",
    "# Note: It's important to embed your query with the same model you embedded your examples with.\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "# 3. Get similarity scores with the dot product (we'll time this for fun)\n",
    "from time import perf_counter as timer\n",
    "\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "# 4. Get the top-k results (we'll keep this to 5)\n",
    "top_results_dot_product = torch.topk(dot_scores, k=5)\n",
    "top_results_dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah!! Now that was fast!\n",
    "\n",
    "~0.00008 seconds to perform a dot product comparison across 1680 embeddings on my machine (NVIDIA RTX 4090 GPU).\n",
    "\n",
    "GPUs are optimized for these kinds of operations.\n",
    "\n",
    "So even if you we're to increase our embeddings by 100x (1680 -> 168,000), an exhaustive dot product operation would happen in ~0.008 seconds (assuming linear scaling).\n",
    "\n",
    "Heck, let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([35900, 768])\n",
      "Time take to get scores on 35900 embeddings: 0.00060 seconds.\n"
     ]
    }
   ],
   "source": [
    "larger_embeddings = torch.randn(100*embeddings.shape[0], 768).to(device)\n",
    "print(f\"Embeddings shape: {larger_embeddings.shape}\")\n",
    "\n",
    "# Perform dot product across 168,000 embeddings\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=larger_embeddings)[0]\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"Time take to get scores on {len(larger_embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. That's quick!\n",
    "\n",
    "That means we can get pretty far by just storing our embeddings in `torch.tensor` for now.\n",
    "\n",
    "However, for *much* larger datasets, we'd likely look at a dedicated vector database/indexing libraries such as [Faiss](https://github.com/facebookresearch/faiss).\n",
    "\n",
    "Let's check the results of our original similarity search.\n",
    "\n",
    "[`torch.topk`](https://pytorch.org/docs/stable/generated/torch.topk.html) returns a tuple of values (scores) and indicies for those scores.\n",
    "\n",
    "The indicies relate to which indicies in the `embeddings` tensor have what scores in relation to the query embedding (higher is better).\n",
    "\n",
    "We can use those indicies to map back to our text chunks.\n",
    "\n",
    "First, we'll define a small helper function to print out wrapped text (so it doesn't print a whole text chunk as a single line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function to print wrapped text \n",
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can loop through the `top_results_dot_product` tuple and match up the scores and indicies and then use those indicies to index on our `pages_and_chunks` variable to get the relevant text chunk.\n",
    "\n",
    "Sounds like a lot but we can do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'macronutrients functions'\n",
      "\n",
      "Results:\n",
      "Score: 0.0831\n",
      "Text:\n",
      "out at odd angles.   “Stick out your right hand over your broom,” called Madam\n",
      "Hooch at the front, “and say ‘Up!’”   “UP” everyone shouted.   Harry’s broom\n",
      "jumped into his hand at once, but it was one of the few that did. Hermione\n",
      "Granger’s had simply rolled over on the ground, and Neville’s hadn’t moved at\n",
      "all. Perhaps brooms, like horses, could tell when you were afraid, thought\n",
      "Harry; there was a quaver in Neville’s voice that said only too clearly that he\n",
      "wanted to keep his feet on the ground.   Madam Hooch then showed them how to\n",
      "mount their brooms without sliding off the end, and walked up and down the rows\n",
      "correcting their grips. Harry and Ron were delighted when she told Malfoy he’d\n",
      "been doing it wrong for years.   “Now, when I blow my whistle, you kick off from\n",
      "the ground, hard,” said Madam Hooch. “Keep your brooms steady, rise a few feet,\n",
      "and then come straight back down by leaning forward slightly. On my whistle —\n",
      "three — two —”    But Neville, nervous and jumpy and frightened of being left on\n",
      "the ground, pushed off hard before the whistle had touched Madam Hooch’s lips.\n",
      "“Come back, boy!”she shouted, but Neville was rising straight up like a cork\n",
      "shot out of a bottle — twelve feet — twenty feet. Harry saw his scared white\n",
      "face look down at the ground falling away, saw him gasp, slip sideways off the\n",
      "broom and —    WHAM — a thud and a nasty crack and Neville lay facedown on the\n",
      "grass in a heap. His broomstick was still rising higher and higher, and started\n",
      "to drift lazily toward the forbidden forest and out of sight.   Madam Hooch was\n",
      "bending over Neville, her face as white as his.   “Broken wrist,” Harry heard\n",
      "her mutter. “Come on, boy — it’s all right, up you get.”   She turned to the\n",
      "rest of the class.   “None of you is to move while I take this boy to the\n",
      "hospital wing!You leave those brooms where they are or you’ll be out of Hogwarts\n",
      "before you can say ‘Quidditch.’Come on, dear.”   Neville, his face tear-\n",
      "streaked, clutching his wrist, hobbled off with Madam Hooch, who had her arm\n",
      "around him.   No sooner were they out of earshot than Malfoy burst into\n",
      "laughter.   “Did you see his face, the great lump?”\n",
      "Page number: 106\n",
      "\n",
      "\n",
      "Score: 0.0817\n",
      "Text:\n",
      "Quirrell cursed again.   “Get out of the way,” he said. As Harry moved aside, he\n",
      "felt the Sorcerer’s Stone against his leg. Dare he make a break for it?   But he\n",
      "hadn’t walked five paces before a high voice spoke, though Quirrell wasn’t\n",
      "moving his lips.   “He lies…He lies.…”   “Potter, come back here!”Quirrell\n",
      "shouted. “Tell me the truth!What did you just see?”   The high voice spoke\n",
      "again.   “Let me speak to him…face-to-face…”    “Master, you are not strong\n",
      "enough!”   “I have strength enough…for this…”    Harry felt as if Devil’s Snare\n",
      "was rooting him to the spot. He couldn’t move a muscle. Petrified, he watched as\n",
      "Quirrell reached up and began to unwrap his turban. What was going on?The turban\n",
      "fell away. Quirrell’s head looked strangely small without it. Then he turned\n",
      "slowly on the spot.   Harry would have screamed, but he couldn’t make a sound.\n",
      "Where there should have been a back to Quirrell’s head, there was a face, the\n",
      "most terrible face Harry had ever seen. It was chalk white with glaring red eyes\n",
      "and slits for nostrils, like a snake.   “Harry Potter…” it whispered.   Harry\n",
      "tried to take a step backward but his legs wouldn’t move.   “See what I have\n",
      "become?”\n",
      "Page number: 210\n",
      "\n",
      "\n",
      "Score: 0.0754\n",
      "Text:\n",
      "CHAPTER FOUR  THE KEEPER OF THE KEYS  B OOM. They knocked again. Dudley jerked\n",
      "awake.   “Where’s the cannon?”he said stupidly.   There was a crash behind them\n",
      "and Uncle Vernon came skidding into the room. He was holding a rifle in his\n",
      "hands – now they knew what had been in the long, thin package he had brought\n",
      "with them.   “Who’s there?”he shouted. “I warn you — I’m armed!”   There was a\n",
      "pause. Then —    SMASH!   The door was hit with such force that it swung clean\n",
      "off its hinges and with a deafening crash landed flat on the floor.   A giant of\n",
      "a man was standing in the doorway. His face was almost completely hidden by a\n",
      "long, shaggy mane of hair and a wild, tangled beard, but you could make out his\n",
      "eyes, glinting like black beetles under all the hair.   The giant squeezed his\n",
      "way into the hut, stooping so that his head just brushed the ceiling. He bent\n",
      "down, picked up the door, and fitted it easily back into its frame. The noise of\n",
      "the storm outside dropped a little. He turned to look at them all.   “Couldn’t\n",
      "make us a cup o’ tea, could yeh?It’s not been an easy journey. …”   He strode\n",
      "over to the sofa where Dudley sat frozen with fear.   “Budge up, yeh great\n",
      "lump,” said the stranger.   Dudley squeaked and ran to hide behind his mother,\n",
      "who was crouching, terrified, behind Uncle Vernon.   “An’ here’s Harry!”\n",
      "Page number: 36\n",
      "\n",
      "\n",
      "Score: 0.0695\n",
      "Text:\n",
      "“Ooh, sticking up for Longbottom?”said Pansy Parkinson, a hard-faced Slytherin\n",
      "girl. “Never thought you’d like fat little crybabies, Parvati.”   “Look!”said\n",
      "Malfoy, darting forward and snatching something out of the grass. “It’s that\n",
      "stupid thing Longbottom’s gran sent him.”   The Remembrall glittered in the sun\n",
      "as he held it up.   “Give that here, Malfoy,” said Harry quietly. Everyone\n",
      "stopped talking to watch.   Malfoy smiled nastily.   “I think I’ll leave it\n",
      "somewhere for Longbottom to find — how about — up a tree?”   “Give it\n",
      "here!”Harry yelled, but Malfoy had leapt onto his broomstick and taken off. He\n",
      "hadn’t been lying, he could fly well. Hovering level with the topmost branches\n",
      "of an oak he called, “Come and get it, Potter!”   Harry grabbed his broom.\n",
      "“No!”shouted Hermione Granger. “Madam Hooch told us not to move — you’ll get us\n",
      "all into trouble.”   Harry ignored her. Blood was pounding in his ears. He\n",
      "mounted the broom and kicked hard against the ground and up, up he soared; air\n",
      "rushed through his hair, and his robes whipped out behind him — and in a rush of\n",
      "fierce joy he realized he’d found something he could do without being taught —\n",
      "this was easy, this was wonderful. He pulled his broomstick up a little to take\n",
      "it even higher, and heard screams and gasps of girls back on the ground and an\n",
      "admiring whoop from Ron.   He turned his broomstick sharply to face Malfoy in\n",
      "midair. Malfoy looked stunned.\n",
      "Page number: 107\n",
      "\n",
      "\n",
      "Score: 0.0680\n",
      "Text:\n",
      "obviously hoping to catch him if he fell. Marcus Flint seized the Quaffle and\n",
      "scored five times without anyone noticing.   “Come on, Hermione,” Ron muttered\n",
      "desperately.   Hermione had fought her way across to the stand where Snape\n",
      "stood, and was now racing along the row behind him; she didn’t even stop to say\n",
      "sorry as she knocked Professor Quirrell headfirst into the row in front.\n",
      "Reaching Snape, she crouched down, pulled out her wand, and whispered a few,\n",
      "well-chosen words. Bright blue flames shot from her wand onto the hem of Snape’s\n",
      "robes.   It took perhaps thirty seconds for Snape to realize that he was on\n",
      "fire. A sudden yelp told her she had done her job. Scooping the fire off him\n",
      "into a little jar in her pocket, she scrambled back along the row — Snape would\n",
      "never know what had happened.   It was enough. Up in the air, Harry was suddenly\n",
      "able to clamber back on to his broom.   “Neville, you can look!”Ron said.\n",
      "Neville had been sobbing into Hagrid’s jacket for the last five minutes.   Harry\n",
      "was speeding toward the ground when the crowd saw him clap his hand to his mouth\n",
      "as though he was about to be sick — he hit the field on all fours — coughed —\n",
      "and something gold fell into his hand.   “I’ve got the Snitch!”he shouted,\n",
      "waving it above his head, and the game ended in complete confusion.   “He didn’t\n",
      "catch it, he nearly swallowed it,” Flint was still howling twenty minutes later,\n",
      "but it made no difference — Harry hadn’t broken any rules and Lee Jordan was\n",
      "still happily shouting the results — Gryffindor had won by one hundred and\n",
      "seventy points to sixty. Harry heard none of this, though. He was being made a\n",
      "cup of strong tea back in Hagrid’s hut, with Ron and Hermione.   “It was Snape,”\n",
      "Ron was explaining, “Hermione and I saw him. He was cursing your broomstick,\n",
      "muttering, he wouldn’t take his eyes off you.”   “Rubbish,” said Hagrid, who\n",
      "hadn’t heard a word of what had gone on next to him in the stands. “Why would\n",
      "Snape do somethin’ like that?”   Harry, Ron, and Hermione looked at one another,\n",
      "wondering what to tell him.\n",
      "Page number: 137\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Results:\")\n",
    "# Loop through zipped together scores and indicies from torch.topk\n",
    "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "    print(\"Text:\")\n",
    "    print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n",
    "    # Print the page number too so we can reference the textbook further (and check the results)\n",
    "    print(f\"Page number: {pages_and_chunks[idx]['page_number']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first result looks to have nailed it!\n",
    "\n",
    "We get a very relevant answer to our query `\"macronutrients functions\"` even though its quite vague.\n",
    "\n",
    "That's the power of semantic search!\n",
    "\n",
    "And even better, if we wanted to inspect the result further, we get the page number where the text appears.\n",
    "\n",
    "How about we check the page to verify?\n",
    "\n",
    "We can do so by loading the page number containing the highest result (page 5 but really page 5 + 41 since our PDF page numbers start on page 41)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!\n",
    "\n",
    "Now we can do extra research if we'd like.\n",
    "\n",
    "We could repeat this workflow for any kind of query we'd like on our textbook.\n",
    "\n",
    "And it would also work for other datatypes too.\n",
    "\n",
    "We could use semantic search on customer support documents.\n",
    "\n",
    "Or email threads.\n",
    "\n",
    "Or company plans.\n",
    "\n",
    "Or our old journal entries.\n",
    "\n",
    "Almost anything!\n",
    "\n",
    "The workflow is the same:\n",
    "\n",
    "`ingest documents -> split into chunks -> embed chunks -> make a query -> embed the query -> compare query embedding to chunk embeddings`\n",
    "\n",
    "And we get relevant resources *along with* the source they came from!\n",
    "\n",
    "That's the **retrieval** part of Retrieval Augmented Generation (RAG).\n",
    "\n",
    "Before we get to the next two steps, let's take a small aside and discuss similarity measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity measures: dot product and cosine similarity \n",
    "\n",
    "Let's talk similarity measures between vectors.\n",
    "\n",
    "Specifically, embedding vectors which are representations of data with magnitude and direction in high dimensional space (our embedding vectors have 768 dimensions).\n",
    "\n",
    "Two of the most common you'll across are the dot product and cosine similarity.\n",
    "\n",
    "They are quite similar.\n",
    "\n",
    "The main difference is that cosine similarity has a normalization step.\n",
    "\n",
    "| Similarity measure | Description | Code |\n",
    "| ----- | ----- | ----- |\n",
    "| [Dot Product](https://en.wikipedia.org/wiki/Dot_product) | - Measure of magnitude and direction between two vectors<br>- Vectors that are aligned in direction and magnitude have a higher positive value<br>- Vectors that are opposite in direction and magnitude have a higher negative value | [`torch.dot`](https://pytorch.org/docs/stable/generated/torch.dot.html), [`np.dot`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html), [`sentence_transformers.util.dot_score`](https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.dot_score) | \n",
    "| [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) | - Vectors get normalized by magnitude/[Euclidean norm](https://en.wikipedia.org/wiki/Norm_(mathematics))/L2 norm so they have unit length and are compared more so on direction<br>- Vectors that are aligned in direction have a value close to 1<br>- Vectors that are opposite in direction have a value close to -1 | [`torch.nn.functional.cosine_similarity`](https://pytorch.org/docs/stable/generated/torch.nn.functional.cosine_similarity.html), [`1 - scipy.spatial.distance.cosine`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html) (subtract the distance from 1 for similarity measure), [`sentence_transformers.util.cos_sim`](https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.cos_sim) |\n",
    "\n",
    "For text similarity, you generally want to use cosine similarity as you are after the semantic measurements (direction) rather than magnitude. \n",
    "\n",
    "In our case, our embedding model `all-mpnet-base-v2` outputs normalized outputs (see the [Hugging Face model card](https://huggingface.co/sentence-transformers/all-mpnet-base-v2#usage-huggingface-transformers) for more on this) so dot product and cosine similarity return the same results. However, dot product is faster due to not need to perform a normalize step.\n",
    "\n",
    "To make things bit more concrete, let's make simple dot product and cosine similarity functions and view their results on different vectors.\n",
    "\n",
    "> **Note:** Similarity measures between vectors and embeddings can be used on any kind of embeddings, not just text embeddings. For example, you could measure image embedding similarity or audio embedding similarity. Or with text and image models like [CLIP](https://github.com/mlfoundations/open_clip), you can measure the similarity between text and image embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product between vector1 and vector2: tensor(14.)\n",
      "Dot product between vector1 and vector3: tensor(32.)\n",
      "Dot product between vector1 and vector4: tensor(-14.)\n",
      "Cosine similarity between vector1 and vector2: tensor(1.0000)\n",
      "Cosine similarity between vector1 and vector3: tensor(0.9746)\n",
      "Cosine similarity between vector1 and vector4: tensor(-1.0000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def dot_product(vector1, vector2):\n",
    "    return torch.dot(vector1, vector2)\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = torch.dot(vector1, vector2)\n",
    "\n",
    "    # Get Euclidean/L2 norm of each vector (removes the magnitude, keeps direction)\n",
    "    norm_vector1 = torch.sqrt(torch.sum(vector1**2))\n",
    "    norm_vector2 = torch.sqrt(torch.sum(vector2**2))\n",
    "\n",
    "    return dot_product / (norm_vector1 * norm_vector2)\n",
    "\n",
    "# Example tensors\n",
    "vector1 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "vector2 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "vector3 = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
    "vector4 = torch.tensor([-1, -2, -3], dtype=torch.float32)\n",
    "\n",
    "# Calculate dot product\n",
    "print(\"Dot product between vector1 and vector2:\", dot_product(vector1, vector2))\n",
    "print(\"Dot product between vector1 and vector3:\", dot_product(vector1, vector3))\n",
    "print(\"Dot product between vector1 and vector4:\", dot_product(vector1, vector4))\n",
    "\n",
    "# Calculate cosine similarity\n",
    "print(\"Cosine similarity between vector1 and vector2:\", cosine_similarity(vector1, vector2))\n",
    "print(\"Cosine similarity between vector1 and vector3:\", cosine_similarity(vector1, vector3))\n",
    "print(\"Cosine similarity between vector1 and vector4:\", cosine_similarity(vector1, vector4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice for both dot product and cosine similarity the comparisons of `vector1` and `vector2` are the opposite of `vector1` and `vector4`.\n",
    "\n",
    "Comparing `vector1` and `vector2` both equations return positive values (14 for dot product and 1.0 for cosine similarity). \n",
    "\n",
    "But comparing `vector1` and `vector4` the result is in the negative direction.\n",
    "\n",
    "This makes sense because `vector4` is the negative version of `vector1`.\n",
    "\n",
    "Whereas comparing `vector1` and `vector3` shows a different outcome.\n",
    "\n",
    "For the dot product, the value is positive and larger then the comparison of two exactly the same vectors (32 vs 14).\n",
    "\n",
    "However, for the cosine similarity, thanks to the normalization step, comparing `vector1` and `vector3` results in a postive value close to 1 but not exactly 1.\n",
    "\n",
    "It is because of this that when comparing text embeddings, cosine similarity is generally favoured as it measures the difference in direction of a pair of vectors rather than difference in magnitude.\n",
    "\n",
    "And it is this difference in direction that is more generally considered to capture the semantic meaning/vibe of the text.\n",
    "\n",
    "The good news is that as mentioned before, the outputs of our embedding model `all-mpnet-base-v2` are already normalized.\n",
    "\n",
    "So we can continue using the dot product (cosine similarity is dot product + normalization).\n",
    "\n",
    "With similarity measures explained, let's functionize our semantic search steps from above so we can repeat them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionizing our semantic search pipeline\n",
    "\n",
    "Let's put all of the steps from above for semantic search into a function or two so we can repeat the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model: SentenceTransformer=embedding_model,\n",
    "                                n_resources_to_return: int=5,\n",
    "                                print_time: bool=True):\n",
    "    \"\"\"\n",
    "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed the query\n",
    "    query_embedding = model.encode(query, \n",
    "                                   convert_to_tensor=True) \n",
    "\n",
    "    # Get dot product scores on embeddings\n",
    "    start_time = timer()\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "    end_time = timer()\n",
    "\n",
    "    if print_time:\n",
    "        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "    scores, indices = torch.topk(input=dot_scores, \n",
    "                                 k=n_resources_to_return)\n",
    "\n",
    "    return scores, indices\n",
    "\n",
    "def print_top_results_and_scores(query: str,\n",
    "                                 embeddings: torch.tensor,\n",
    "                                 pages_and_chunks: list[dict]=pages_and_chunks,\n",
    "                                 n_resources_to_return: int=5):\n",
    "    \"\"\"\n",
    "    Takes a query, retrieves most relevant resources and prints them out in descending order.\n",
    "\n",
    "    Note: Requires pages_and_chunks to be formatted in a specific way (see above for reference).\n",
    "    \"\"\"\n",
    "    \n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings,\n",
    "                                                  n_resources_to_return=n_resources_to_return)\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Results:\")\n",
    "    # Loop through zipped together scores and indicies\n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n",
    "        # Print the page number too so we can reference the textbook further and check the results\n",
    "        print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Now let's test our functions out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Time taken to get scores on 359 embeddings: 0.00022 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.0710, 0.0566, 0.0560, 0.0541, 0.0510], device='cuda:0'),\n",
       " tensor([244, 166, 238,  78,  72], device='cuda:0'))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"harry, ron and hermoine\"\n",
    "\n",
    "# Get just the scores and indices of top related results\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Time taken to get scores on 359 embeddings: 0.00006 seconds.\n",
      "Query: harry, ron and hermoine\n",
      "\n",
      "Results:\n",
      "Score: 0.0710\n",
      "It shows us nothing more or less than the deepest, most desperate desire of our\n",
      "hearts. You, who have never known your family, see them standing around you.\n",
      "Ronald Weasley, who has always been overshadowed by his brothers, sees himself\n",
      "standing alone, the best of all of them. However, this mirror will give us\n",
      "neither knowledge or truth. Men have wasted away before it, entranced by what\n",
      "they have seen, or been driven mad, not knowing if what it shows is real or even\n",
      "possible.   “The Mirror will be moved to a new home tomorrow, Harry, and I ask\n",
      "you not to go looking for it again. If you ever do run across it, you will now\n",
      "be\n",
      "Page number: 152\n",
      "\n",
      "\n",
      "Score: 0.0566\n",
      "CHAPTER NINE  THE MIDNIGHT DUEL  H arry had never believed he would meet a boy\n",
      "he hated more than Dudley, but that was before he met Draco Malfoy. Still,\n",
      "first-year Gryffindors only had Potions with the Slytherins, so they didn’t have\n",
      "to put up with Malfoy much. Or at least, they didn’t until they spotted a notice\n",
      "pinned up in the Gryffindor common room that made them all groan. Flying lessons\n",
      "would be starting on Thursday — and Gryffindor and Slytherin would be learning\n",
      "together.   “Typical,” said Harry darkly. “Just what I always wanted. To make a\n",
      "fool of myself on a broomstick in front of Malfoy.”   He had been looking\n",
      "forward to learning to fly more than anything else.   “You don’t know that\n",
      "you’ll make a fool of yourself,” said Ron reasonably. “Anyway, I know Malfoy’s\n",
      "always going on about how good he is at Quidditch, but I bet that’s all talk.”\n",
      "Malfoy certainly did talk about flying a lot. He complained loudly about first\n",
      "years never getting on the house Quidditch teams and told long, boastful stories\n",
      "that always seemed to end with him narrowly escaping Muggles in helicopters. He\n",
      "wasn’t the only one, though: the way Seamus Finnigan told it, he’d spent most of\n",
      "his childhood zooming around the countryside on his broomstick. Even Ron would\n",
      "tell anyone who’d listen about the time he’d almost hit a hang glider on\n",
      "Charlie’s old broom. Everyone from wizarding families talked about Quidditch\n",
      "constantly. Ron had already had a big argument with Dean Thomas, who shared\n",
      "their dormitory, about soccer. Ron couldn’t see what was exciting about a game\n",
      "with only one ball where no one was allowed to fly. Harry had caught Ron\n",
      "prodding Dean’s poster of West Ham soccer team, trying to make the players move.\n",
      "Neville had never been on a broomstick in his life, because his grandmother had\n",
      "never let him near one. Privately, Harry felt she’d had good reason, because\n",
      "Neville managed to have an extraordinary number of accidents even with both feet\n",
      "on the ground.   Hermione Granger was almost as nervous about flying as Neville\n",
      "was. This was something you couldn’t learn by heart out of a book — not that she\n",
      "hadn’t tried. At breakfast on Thursday she bored them all stupid with flying\n",
      "tips she’d gotten out of a library book called Quidditch Through the Ages.\n",
      "Neville\n",
      "Page number: 104\n",
      "\n",
      "\n",
      "Score: 0.0560\n",
      "He had a powerful kind of ache inside him, half joy, half terrible sadness.\n",
      "How long he stood there, he didn’t know. The reflections did not fade and he\n",
      "looked and looked until a distant noise brought him back to his senses. He\n",
      "couldn’t stay here, he had to find his way back to bed. He tore his eyes away\n",
      "from his mother’s face, whispered, “I’ll come back,” and hurried from the room.\n",
      "“You could have woken me up,” said Ron, crossly.   “You can come tonight, I’m\n",
      "going back, I want to show you the mirror.   “I’d like to see your mom and dad,”\n",
      "Ron said eagerly.\n",
      "Page number: 149\n",
      "\n",
      "\n",
      "Score: 0.0541\n",
      "Nah, we’re best left alone.”   At this moment the boat bumped gently into the\n",
      "harbor wall. Hagrid folded up his newspaper, and they clambered up the stone\n",
      "steps onto the street.   Passersby stared a lot at Hagrid as they walked through\n",
      "the little town to the station. Harry couldn’t blame them. Not only was Hagrid\n",
      "twice as tall as anyone else, he kept pointing at perfectly ordinary things like\n",
      "parking meters and saying loudly, “See that, Harry?Things these Muggles dream\n",
      "up, eh?”   “Hagrid,” said Harry, panting a bit as he ran to keep up, “did you\n",
      "say there are dragons at Gringotts?”   “Well, so they say,” said Hagrid.\n",
      "“Crikey, I’d like a dragon.”\n",
      "Page number: 49\n",
      "\n",
      "\n",
      "Score: 0.0510\n",
      "“Gotta get up ter town, get all yer books an’ that.”   He took off his thick\n",
      "black coat and threw it to Harry.   “You can kip under that,” he said. “Don’\n",
      "mind if it wriggles a bit, I think I still got a couple o’ doormice in one o’\n",
      "the pockets.”\n",
      "Page number: 45\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out the texts of the top scores\n",
    "print_top_results_and_scores(query=query,\n",
    "                             embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic search/vector search extensions \n",
    "\n",
    "We've covered an exmaple of using embedding vector search to find relevant results based on a query.\n",
    "\n",
    "However, you could also add to this pipeline with traditional keyword search.\n",
    "\n",
    "Many modern search systems use keyword and vector search in tandem.\n",
    "\n",
    "Our dataset is small and allows for an exhaustive search (comparing the query to *every* possible result) but if you start to work with large scale datasets with hundred of thousands, millions or even billions of vectors, you'll want to implement an index.\n",
    "\n",
    "You can think of an index as sorting your embeddings before you search through them.\n",
    "\n",
    "So it narrows down the search space.\n",
    "\n",
    "For example, it would be inefficient to search every word in the dictionary to find the word \"duck\", instead you'd go straight to the letter D, perhaps even straight to the back half of the letter D, find words close to \"duck\" before finding it.\n",
    "\n",
    "That's how an index can help search through many examples without comprimising too much on speed or quality (for more on this, check out [nearest neighbour search](https://en.wikipedia.org/wiki/Nearest_neighbor_search)).\n",
    "\n",
    "One of the most popular indexing libraries is [Faiss](https://github.com/facebookresearch/faiss). \n",
    "\n",
    "Faiss is open-source and was originally created by Facebook to deal with internet-scale vectors and implements many algorithms such as [HNSW](https://arxiv.org/abs/1603.09320) (Hierarchical Naviganle Small Worlds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting an LLM for local generation\n",
    "\n",
    "We're got our retrieval pipeline ready, let's now get the generation side of things happening.\n",
    "\n",
    "To perform generation, we're going to use a Large Language Model (LLM).\n",
    "\n",
    "LLMs are designed to generate an output given an input.\n",
    "\n",
    "In our case, we want our LLM to generate and output of text given a input of text.\n",
    "\n",
    "And more specifically, we want the output of text to be generated based on the context of relevant information to the query.\n",
    "\n",
    "The input to an LLM is often referred to as a prompt.\n",
    "\n",
    "We'll augment our prompt with a query as well as context from our textbook related to that query.\n",
    "\n",
    "> **Which LLM should I use?**\n",
    "\n",
    "There are many LLMs available.\n",
    "\n",
    "Two of the main questions to ask from this is:\n",
    "1. Do I want it to run locally? \n",
    "2. If yes, how much compute power can I dedicate?\n",
    "\n",
    "If you're after the absolute best performance, you'll likely want to use an API (not running locally) such as GPT-4 or Claude 3. However, this comes with the tradeoff of sending your data away and then awaiting a response.\n",
    "\n",
    "For our case, since we want to set up a local pipeline and run it on our own GPU, we'd answer \"yes\" to the first question and then the second question will depend on what hardware we have available.\n",
    "\n",
    "To find open-source LLMs, one great resource is the [Hugging Face open LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n",
    "\n",
    "The leaderboard compares many of the latest and greatest LLMs on various benchmarks.\n",
    "\n",
    "Another great resource is [TheBloke on Hugging Face](https://huggingface.co/TheBloke), an account which provides an extensive range of quantized (models that have been made smaller) LLMs.\n",
    "\n",
    "A rule of thumb for LLMs (and deep learning models in general) is that the higher the number of parameters, the better the model performs. \n",
    "\n",
    "It may be tempting to go for the largest size model (e.g. a 70B parameter model rather than a 7B parameter model) but a larger size model may not be able to run on your available hardware.\n",
    "\n",
    "The following table gives an insight into how much GPU memory you'll need to load an LLM with different sizes and different levels of [numerical precision](https://en.wikipedia.org/wiki/Precision_(computer_science)).\n",
    "\n",
    "They are based on the fact that 1 float32 value (e.g. `0.69420`) requires 4 bytes of memory and 1GB is approximately 1,000,000,000 (one billion) bytes.\n",
    "\n",
    "| Model Size (Billion Parameters) | Float32 VRAM (GB) | Float16 VRAM (GB) | 8-bit VRAM (GB) | 4-bit VRAM (GB) |\n",
    "|-----|-----|-----|-----|-----|\n",
    "| 1B                              | ~4                | ~2                | ~1              | ~0.5            |\n",
    "| 7B (e.g., [Llama 2 7B](https://huggingface.co/meta-llama/Llama-2-7b), [Gemma 7B](https://huggingface.co/google/gemma-7b-it), [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1))             | ~28               | ~14               | ~7              | ~3.5            |\n",
    "| 10B                             | ~40               | ~20               | ~10             | ~5              |\n",
    "| 70B (e.g, Llama 2 70B)          | ~280              | ~140              | ~70             | ~35             |\n",
    "| 100B                            | ~400              | ~200              | ~100            | ~50             |\n",
    "| 175B                            | ~700              | ~350              | ~175            | ~87.5           |\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Note:** Loading a model in a lower precision (e.g. 8-bit instead of float16) generally lowers performance. Lower precision can help to reduce computing requirements, however sometimes the performance degradation in terms of model output can be substantial. Finding the right speed/performance tradeoff will often require many experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking local GPU memory availability\n",
    "\n",
    "Let's find out what hardware we've got available and see what kind of model(s) we'll be able to load.\n",
    "\n",
    "> **Note:** You can also check this with the `!nvidia-smi` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "# Get GPU available memory\n",
    "import torch\n",
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
    "print(f\"Available GPU memory: {gpu_memory_gb} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok wonderful!\n",
    "\n",
    "I'm running this notebook with a NVIDIA RTX 4090, so I've got 24GB of VRAM available.\n",
    "\n",
    "However, this may be different on your end.\n",
    "\n",
    "Looking at the table above, it seems we can run a ~7-10B parameter model in float16 precision pretty comfortably.\n",
    "\n",
    "But we could also run a smaller one if we'd like.\n",
    "\n",
    "Let's try out the recently released (at the time of writing, March 2024) LLM from Google, [Gemma](https://huggingface.co/blog/gemma).\n",
    "\n",
    "Specifically, we'll use the `gemma-7b-it` version which stands for Gemma 7B Instruction-Tuned.\n",
    "\n",
    "Instruction tuning is the process of tuning a raw language model to follow instructions.\n",
    "\n",
    "These are the kind of models you'll find in most chat-based assistants such as ChatGPT, Gemini or Claude.\n",
    "\n",
    "The following table shows different amounts of GPU memory requirements for different verions of the Gemma LLMs with varying levels of precision.\n",
    "\n",
    "| Model             | Precision | Min-Memory (Bytes) | Min-Memory (MB) | Min-Memory (GB) | Recommended Memory (GB) | Hugging Face ID |\n",
    "|-------------------|-----------|----------------|-------------|-------------| ----- | ----- |\n",
    "| [Gemma 2B](https://huggingface.co/google/gemma-2b-it)          | 4-bit     | 2,106,749,952  | 2009.15     | 1.96        | ~5.0 | [`gemma-2b`](https://huggingface.co/google/gemma-2b) or [`gemma-2b-it`](https://huggingface.co/google/gemma-2b-it) for instruction tuned version | \n",
    "| Gemma 2B          | Float16   | 5,079,453,696  | 4844.14     | 4.73        | ~8.0 | Same as above |\n",
    "| [Gemma 7B](https://huggingface.co/google/gemma-7b-it)          | 4-bit     | 5,515,859,968  | 5260.33     | 5.14        | ~8.0 | [`gemma-7b`](https://huggingface.co/google/gemma-7b) or [`gemma-7b-it`](https://huggingface.co/google/gemma-7b-it) for instruction tuned version |\n",
    "| Gemma 7B          | Float16   | 17,142,470,656 | 16348.33    | 15.97       | ~19 | Same as above |\n",
    "\n",
    "> **Note:** `gemma-7b-it` means \"instruction tuned\", as in, a base LLM (`gemma-7b`) has been fine-tuned to follow instructions, similar to [`Mistral-7B-v0.1`](https://huggingface.co/mistralai/Mistral-7B-v0.1) and [`Mistral-7B-Instruct-v0.1`](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1).\n",
    "> \n",
    "> There are also further quantized and smaller variants of Gemma (and other LLMs) available in various formats such as GGUF. You can see many of these on [TheBloke account on Hugging Face](https://huggingface.co/TheBloke).\n",
    "> \n",
    "> The version of LLM you choose to use will be largely based on project requirements and experimentation.\n",
    "\n",
    "Based on the table above, let's write a simple if/else statement which recommends which Gemma variant we should look into using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 8 | Recommended model: Gemma 2B in 4-bit precision.\n",
      "use_quantization_config set to: True\n",
      "model_id set to: google/gemma-2b-it\n"
     ]
    }
   ],
   "source": [
    "# Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.\n",
    "if gpu_memory_gb < 5.1:\n",
    "    print(f\"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n",
    "elif gpu_memory_gb < 8.1:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.\")\n",
    "    use_quantization_config = True \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb < 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb > 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-7b-it\"\n",
    "\n",
    "print(f\"use_quantization_config set to: {use_quantization_config}\")\n",
    "print(f\"model_id set to: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading an LLM locally\n",
    "\n",
    "Alright! Looks like `gemma-7b-it` it is (for my local machine with an RTX 4090, change the `model_id` and `use_quantization_config` values to suit your needs)! \n",
    "\n",
    "There are plenty of examples of how to load the model on the `gemma-7b-it` [Hugging Face model card](https://huggingface.co/google/gemma-7b-it).\n",
    "\n",
    "Good news is, the Hugging Face [`transformers`](https://huggingface.co/docs/transformers/) library has all the tools we need.\n",
    "\n",
    "To load our LLM, we're going to need a few things:\n",
    "1. A quantization config (optional) - This will determine whether or not we load the model in 4bit precision for lower memory usage. The we can create this with the [`transformers.BitsAndBytesConfig`](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/quantization#transformers.BitsAndBytesConfig) class (requires installing the [`bitsandbytes` library](https://github.com/TimDettmers/bitsandbytes)).\n",
    "2. A model ID - This is the reference Hugging Face model ID which will determine which tokenizer and model gets used. For example `gemma-7b-it`.\n",
    "3. A tokenzier - This is what will turn our raw text into tokens ready for the model. We can create it using the [`transformers.AutoTokenzier.from_pretrained`](https://huggingface.co/docs/transformers/v4.38.2/en/model_doc/auto#transformers.AutoTokenizer) method and passing it our model ID.\n",
    "4. An LLM model - Again, using our model ID we can load a specific LLM model. To do so we can use the [`transformers.AutoModelForCausalLM.from_pretrained`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained) method and passing it our model ID as well as other various parameters.\n",
    "\n",
    "As a bonus, we'll check if [Flash Attention 2](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2) is available using `transformers.utils.is_flash_attn_2_available()`. Flash Attention 2 speeds up the attention mechanism in Transformer architecture models (which is what many modern LLMs are based on, including Gemma). So if it's available and the model is supported (not all models support Flash Attention 2), we'll use it. If it's not available, you can install it by following the instructions on the [GitHub repo](https://github.com/Dao-AILab/flash-attention). \n",
    "\n",
    "> **Note:** Flash Attention 2 currently works on NVIDIA GPUs with a compute capability score of 8.0+ (Ampere, Ada Lovelace, Hopper architectures). We can check our GPU compute capability score with [`torch.cuda.get_device_capability(0)`](https://pytorch.org/docs/stable/generated/torch.cuda.get_device_capability.html). \n",
    "\n",
    "> **Note:** To get access to the Gemma models, you will have to [agree to the terms & conditions](https://huggingface.co/google/gemma-7b-it) on the Gemma model page on Hugging Face. You will then have to authorize your local machine via the [Hugging Face CLI/Hugging Face Hub `login()` function](https://huggingface.co/docs/huggingface_hub/en/quick-start#authentication). Once you've done this, you'll be able to download the models. If you're using Google Colab, you can add a [Hugging Face token](https://huggingface.co/docs/hub/en/security-tokens) to the \"Secrets\" tab.\n",
    ">\n",
    "> Downloading an LLM locally can take a fair bit of time depending on your internet connection. Gemma 7B is about a 16GB download and Gemma 2B is about a 6GB download.\n",
    "\n",
    "Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using attention implementation: sdpa\n",
      "[INFO] Using model_id: google/gemma-2b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.11s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available \n",
    "\n",
    "# 1. Create quantization config for smaller model loading (optional)\n",
    "# Requires !pip install bitsandbytes accelerate, see: https://github.com/TimDettmers/bitsandbytes, https://huggingface.co/docs/accelerate/\n",
    "# For models that require 4-bit quantization (use this if you have low GPU memory available)\n",
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "# Bonus: Setup Flash Attention 2 for faster inference, default to \"sdpa\" or \"scaled dot product attention\" if it's not available\n",
    "# Flash Attention 2 requires NVIDIA GPU compute capability of 8.0 or above, see: https://developer.nvidia.com/cuda-gpus\n",
    "# Requires !pip install flash-attn, see: https://github.com/Dao-AILab/flash-attention \n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "  attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "  attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
    "\n",
    "# 2. Pick a model we'd like to use (this will depend on how much GPU memory you have available)\n",
    "#model_id = \"google/gemma-7b-it\"\n",
    "model_id = model_id # (we already set this above)\n",
    "print(f\"[INFO] Using model_id: {model_id}\")\n",
    "\n",
    "# 3. Instantiate tokenizer (tokenizer turns text into numbers ready for the model) \n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "\n",
    "# 4. Instantiate the model\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                                 torch_dtype=torch.float16, # datatype to use, we want float16\n",
    "                                                 quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                                 low_cpu_mem_usage=False, # use full memory \n",
    "                                                 attn_implementation=attn_implementation) # which attention version to use\n",
    "\n",
    "if not use_quantization_config: # quantization takes care of device setting automatically, so if it's not used, send model to GPU \n",
    "    llm_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got an LLM!\n",
    "\n",
    "Let's check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, ok a bunch of layers ranging from embedding layers to attention layers (see the `GemmaFlashAttention2` layers!) to MLP and normalization layers.\n",
    "\n",
    "The good news is that we don't have to know too much about these to use the model.\n",
    "\n",
    "How about we get the number of parameters in our model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1515268096"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "get_model_num_params(llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, turns out that Gemma 7B is really Gemma 8.5B.\n",
    "\n",
    "It pays to do your own investigations!\n",
    "\n",
    "How about we get the models memory requirements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_mem_bytes': 2039631872, 'model_mem_mb': 1945.14, 'model_mem_gb': 1.9}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Get how much memory a PyTorch model takes up.\n",
    "\n",
    "    See: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822\n",
    "    \"\"\"\n",
    "    # Get model parameters and buffer sizes\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate various model sizes\n",
    "    model_mem_bytes = mem_params + mem_buffers # in bytes\n",
    "    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes\n",
    "    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes\n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes,\n",
    "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "get_model_mem_size(llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text with our LLM\n",
    "\n",
    "We can generate text with our LLM `model` instance by calling the [`generate()` method](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/text_generation#transformers.GenerationConfig) (this method has plenty of options to pass into it alongside the text) on it and passing it a tokenized input.\n",
    "\n",
    "The tokenized input comes from passing a string of text to our `tokenizer`.\n",
    "\n",
    "It's important to note that you should use a tokenizer that has been paired with a model.\n",
    "\n",
    "Otherwise if you try to use a different tokenizer and then pass those inputs to a model, you will likely get errors/strange results.\n",
    "\n",
    "For some LLMs, there's a specific template you should pass to them for ideal outputs.\n",
    "\n",
    "For example, the `gemma-7b-it` model has been trained in a dialogue fashion (instruction tuning).\n",
    "\n",
    "In this case, our `tokenizer` has a [`apply_chat_template()` method](https://huggingface.co/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template) which can prepare our input text in the right format for the model.\n",
    "\n",
    "Let's try it out.\n",
    "\n",
    "> **Note:** The following demo has been modified from the Hugging Face model card for [Gemma 7B](https://huggingface.co/google/gemma-7b-it). Many similar demos of usage are available on the model cards of similar models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "What are the macronutrients, and what roles do they play in the human body?\n",
      "\n",
      "Prompt (formatted):\n",
      "<bos><start_of_turn>user\n",
      "What are the macronutrients, and what roles do they play in the human body?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What are the macronutrients, and what roles do they play in the human body?\"\n",
    "print(f\"Input text:\\n{input_text}\")\n",
    "\n",
    "# Create prompt template for instruction-tuned model\n",
    "dialogue_template = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": input_text}\n",
    "]\n",
    "\n",
    "# Apply the chat template\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False, # keep as raw text (not tokenized)\n",
    "                                       add_generation_prompt=True)\n",
    "print(f\"\\nPrompt (formatted):\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the scaffolding around our input text, this is the kind of turn-by-turn instruction tuning our model has gone through.\n",
    "\n",
    "Our next step is to tokenize this formatted text and pass it to our model's `generate()` method.\n",
    "\n",
    "We'll make sure our tokenized text is on the same device as our model (GPU) using `to(\"cuda\")`.\n",
    "\n",
    "Let's generate some text! \n",
    "\n",
    "We'll time it for fun with the `%%time` magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input (tokenized):\n",
      "{'input_ids': tensor([[     2,      2,    106,   1645,    108,   1841,    708,    573, 186809,\n",
      "         184592, 235269,    578,   1212,  16065,    749,    984,   1554,    575,\n",
      "            573,   3515,   2971, 235336,    107,    108,    106,   2516,    108]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n",
      "\n",
      "Model output (tokens):\n",
      "tensor([     2,      2,    106,   1645,    108,   1841,    708,    573, 186809,\n",
      "        184592, 235269,    578,   1212,  16065,    749,    984,   1554,    575,\n",
      "           573,   3515,   2971, 235336,    107,    108,    106,   2516,    108,\n",
      "         21404, 235269,   1517, 235303, 235256,    476,  25497,    576,    573,\n",
      "        186809, 184592,    578,   1024,  16065,    575,    573,   3515,   2971,\n",
      "        235292,    109,    688,  12298,   1695, 184592,  66058,    109, 235287,\n",
      "          5231, 156615,  56227,  66058,   5626,   2971,   7177,  72780,    604,\n",
      "          4134, 235265,   2365,    708,    573,   7920,   9719,    604,   1167,\n",
      "          5999,    578,  29703, 235265,    108, 235287,   5231,  49471,  66058,\n",
      "         33849,    603,   8727,    604,   4547,    578,  68808,  29703, 235269,\n",
      "          3547,  44760, 235269,    578,  17839,  53186, 235265,    108, 235287,\n",
      "          5231,  33690,  66058,  22904,   6572,   4134, 235269,   7154,  33398,\n",
      "         48765, 235269,    578,   7154,    577,   2029,   7459,    573,   2971,\n",
      "        235265,    109,    688,  12298,   1695,   7208,    579, 152614,  66058,\n",
      "           109, 235287,   5231, 156615,  56227,  66058,   5626,  13266,   1476,\n",
      "          2449, 235248, 235310, 235308, 235290, 235318, 235308, 235358,    576,\n",
      "          1167,   3051,  34366,    774,  72780, 235265,    108, 235287,   5231,\n",
      "         49471,  66058,   1448,   1476,   2449, 235248, 235274, 235265, 235318,\n",
      "        235290, 235284, 235265, 235284,  27491,    576,   9646,    842,  77180,\n",
      "           576,   2971,   5171,    842,   1744, 235265,    108, 235287,   5231,\n",
      "         33690,  66058,   1448,   1476,   2449, 235248, 235284, 235276, 235290,\n",
      "        235304, 235276, 235358,    576,   1167,   3051,  34366,    774,   6181,\n",
      "        235265,    109,    688,   2299,  97586, 184592,   5624,  32119,  66058,\n",
      "           109,  12298,   1695, 184592,   1160,   3584,    577,   3658,    573,\n",
      "          2971,    675,    573,   4134,    578,   4547,  13854,    665,   4026,\n",
      "           577,   1411,  10338, 235265,   1699,   3287, 235292,    109, 235287,\n",
      "        110165,  56227,    708,  10127,   1706,   1280,  30859, 235269,    948,\n",
      "           603,   1671,    731,   5999,    604,   4134, 235265,    108, 235287,\n",
      "         33849,    603,   1671,    577,   2500,    578,  12158,  29703, 235269,\n",
      "          7872,  44760, 235269,    578,   7872,  53186, 235265,    108, 235287,\n",
      "         22904,    603,   1671,    577,   4659,   4134,    578,   1707,    577,\n",
      "          2029,   7459,    573,   2971], device='cuda:0')\n",
      "\n",
      "CPU times: total: 1.3 s\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Tokenize the input text (turn it into numbers) and send it to GPU\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(f\"Model input (tokenized):\\n{input_ids}\\n\")\n",
    "\n",
    "# Generate outputs passed on the tokenized input\n",
    "# See generate docs: https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/text_generation#transformers.GenerationConfig \n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             max_new_tokens=256) # define the maximum number of new tokens to create\n",
    "print(f\"Model output (tokens):\\n{outputs[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! We just generated some text on our local GPU!\n",
    "\n",
    "Well not just yet...\n",
    "\n",
    "Our LLM accepts tokens in and sends tokens back out.\n",
    "\n",
    "We can conver the output tokens to text using [`tokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output (decoded):\n",
      "<bos><bos><start_of_turn>user\n",
      "What are the macronutrients, and what roles do they play in the human body?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here's a breakdown of the macronutrients and their roles in the human body:\n",
      "\n",
      "**Macronutrients:**\n",
      "\n",
      "* **Carbohydrates:** Our body uses carbohydrates for energy. They are the primary fuel for our cells and tissues.\n",
      "* **Protein:** Protein is essential for building and repairing tissues, making enzymes, and producing hormones.\n",
      "* **Fat:** Fat provides energy, helps absorb vitamins, and helps to insulate the body.\n",
      "\n",
      "**Macronutrient Ratios:**\n",
      "\n",
      "* **Carbohydrates:** Our bodies need around 45-65% of our total calories from carbohydrates.\n",
      "* **Protein:** We need around 1.6-2.2 grams of protein per kilogram of body weight per day.\n",
      "* **Fat:** We need around 20-30% of our total calories from fat.\n",
      "\n",
      "**How Macronutrients Work Together:**\n",
      "\n",
      "Macronutrients work together to provide the body with the energy and building blocks it needs to function properly. For example:\n",
      "\n",
      "* Carbohydrates are broken down into glucose, which is used by cells for energy.\n",
      "* Protein is used to build and repair tissues, produce enzymes, and produce hormones.\n",
      "* Fat is used to store energy and help to insulate the body\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decode the output tokens to text\n",
    "outputs_decoded = tokenizer.decode(outputs[0])\n",
    "print(f\"Model output (decoded):\\n{outputs_decoded}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! That looks like a pretty good answer.\n",
    "\n",
    "But notice how the output contains the prompt text as well?\n",
    "\n",
    "How about we do a little formatting to replace the prompt in the output text?\n",
    "\n",
    "> **Note:** `\"<bos>\"` and `\"<eos>\"` are special tokens to denote \"beginning of sentence\" and \"end of sentence\" respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: What are the macronutrients, and what roles do they play in the human body?\n",
      "\n",
      "Output text:\n",
      "Sure, here's a breakdown of the macronutrients and their roles in the human body:\n",
      "\n",
      "**Macronutrients:**\n",
      "\n",
      "* **Carbohydrates:** Our body uses carbohydrates for energy. They are the primary fuel for our cells and tissues.\n",
      "* **Protein:** Protein is essential for building and repairing tissues, making enzymes, and producing hormones.\n",
      "* **Fat:** Fat provides energy, helps absorb vitamins, and helps to insulate the body.\n",
      "\n",
      "**Macronutrient Ratios:**\n",
      "\n",
      "* **Carbohydrates:** Our bodies need around 45-65% of our total calories from carbohydrates.\n",
      "* **Protein:** We need around 1.6-2.2 grams of protein per kilogram of body weight per day.\n",
      "* **Fat:** We need around 20-30% of our total calories from fat.\n",
      "\n",
      "**How Macronutrients Work Together:**\n",
      "\n",
      "Macronutrients work together to provide the body with the energy and building blocks it needs to function properly. For example:\n",
      "\n",
      "* Carbohydrates are broken down into glucose, which is used by cells for energy.\n",
      "* Protein is used to build and repair tissues, produce enzymes, and produce hormones.\n",
      "* Fat is used to store energy and help to insulate the body\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input text: {input_text}\\n\")\n",
    "print(f\"Output text:\\n{outputs_decoded.replace(prompt, '').replace('<bos>', '').replace('<eos>', '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How cool is that!\n",
    "\n",
    "We just officially generated text from an LLM running locally.\n",
    "\n",
    "So we've covered the R (retrieval) and G (generation) of RAG.\n",
    "\n",
    "How about we check out the last step?\n",
    "\n",
    "Augmentation.\n",
    "\n",
    "First, let's put together a list of queries we can try out with our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nutrition-style questions generated with GPT4\n",
    "gpt4_questions = [\n",
    "    \"Who is Harry Potter?\",\n",
    "    \"Describe Hermione Granger.\",\n",
    "    \"What are the characteristics of Ron Weasley?\",\n",
    "    \"What house does Harry get sorted into, and why?\",\n",
    "    \"Describe the character of Severus Snape.\",\n",
    "    \"How does Harry discover he is a wizard?\",\n",
    "    \"Describe Harry's first experience at Platform 9 3/4.\",\n",
    "    \"What is the significance of Harry's lightning-shaped scar?\",\n",
    "    \"What is the Mirror of Erised?\"\n",
    "]\n",
    "\n",
    "# Manually created question list\n",
    "manual_questions = [\n",
    "    \"What are the characteristics of a Nimbus 2000 broomstick?\",\n",
    "    \"How does Harry become the Seeker for the Gryffindor Quidditch team?\",\n",
    "    \"What challenges do Harry, Ron, and Hermione face to reach the Philosopher's Stone?\",\n",
    "    \"How do they get past Fluffy?\",\n",
    "    \"Describe the encounter with the Devil's Snare.\",\n",
    "    \"What happens in the life-sized wizard chess game?\",\n",
    "    \"How does Harry confront Professor Quirrell and Voldemort?\",\n",
    "    \"How is friendship portrayed in the book?\",\n",
    "    \"What does the story suggest about bravery?\",\n",
    "    \"Who is Fluffy, and what role does he play?\",\n",
    "    \"How do Harry, Ron, and Hermione become friends?\"\n",
    "]\n",
    "\n",
    "query_list = gpt4_questions + manual_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's check if our `retrieve_relevant_resources()` function works with our list of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Describe the character of Severus Snape.\n",
      "[INFO] Time taken to get scores on 359 embeddings: 0.00015 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.0375, 0.0306, 0.0263, 0.0260, 0.0222], device='cuda:0'),\n",
       " tensor([263, 268, 160,  67, 267], device='cuda:0'))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "query = random.choice(query_list)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Get just the scores and indices of top related results\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful!\n",
    "\n",
    "Let's augment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting our prompt with context items\n",
    "\n",
    "What we'd like to do with augmentation is take the results from our search for relevant resources and put them into the prompt that we pass to our LLM.\n",
    "\n",
    "In essence, we start with a base prompt and update it with context text.\n",
    "\n",
    "Let's write a function called `prompt_formatter` that takes in a query and our list of context items (in our case it'll be select indices from our list of dictionaries inside `pages_and_chunks`) and then formats the query with text from the context items.\n",
    "\n",
    "We'll apply the dialogue and chat template to our prompt before returning it as well.\n",
    "\n",
    "> **Note:** The process of augmenting or changing a prompt to an LLM is known as prompt engineering. And the best way to do it is an active area of research. For a comprehensive guide on different prompt engineering techniques, I'd recommend the Prompt Engineering Guide ([promptingguide.ai](https://www.promptingguide.ai/)), [Brex's Prompt Engineering Guide](https://github.com/brexhq/prompt-engineering) and the paper [Prompt Design and Engineering: Introduction and Advanced Models](https://arxiv.org/abs/2401.14423)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str, \n",
    "                     context_items: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    # Join context items into one dotted paragraph\n",
    "    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "\n",
    "    # Create a base prompt with examples to help the model\n",
    "    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n",
    "    # We could also write this in a txt file and import it in if we wanted.\n",
    "    base_prompt = \"\"\"In response to the following prompt, provide relevant information retrieved from the documents: \"{context}\"\n",
    "\n",
    "Prompt: {query}\"\"\"\n",
    "\n",
    "    # Update base prompt with context items and query   \n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    # Create prompt template for instruction-tuned model\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good! Let's try our function out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who is Fluffy, and what role does he play?\n",
      "[INFO] Time taken to get scores on 359 embeddings: 0.00005 seconds.\n",
      "<bos><start_of_turn>user\n",
      "In response to the following prompt, provide relevant information retrieved from the documents: \"- He hurried to his car and set off for home, hoping he was imagining things, which he had never hoped before, because he didn’t approve of imagination.   As he pulled into the driveway of number four, the first thing he saw — and it didn’t improve his mood — was the tabby cat he’d spotted that morning. It was now sitting on his garden wall. He was sure it was the same one; it had the same markings around its eyes.   “Shoo!”said Mr. Dursley loudly.   The cat didn’t move. It just gave him a stern look. Was this normal cat behavior?Mr. Dursley wondered. Trying to pull himself together, he let himself into the house. He was still determined not to mention anything to his wife.   Mrs. Dursley had had a nice, normal day. She told him over dinner all about Mrs. Next Door’s problems with her daughter and how Dudley had\n",
      "- Most of them had never seen an owl even at nighttime. Mr. Dursley, however, had a perfectly normal, owl-free morning. He yelled at five different people. He made several important telephone calls and shouted a bit more. He was in a very good mood until lunchtime, when he thought he’d stretch his legs and walk across the road to buy himself a bun from the bakery.   He’d for gotten all about the people in cloaks until he passed a group of them next to the baker’s. He eyed them angrily as he passed. He didn’t know why, but they made him uneasy. This bunch were whispering excitedly, too, and he couldn’t see a single collecting tin. It was on his way back past them, clutching a large doughnut in a bag, that he caught a few words of what they were saying.   “The Potters, that’s right, that’s what I heard —”\n",
      "- “Could do with some of those letters now, eh?”he said cheerfully.   He was in a very good mood. Obviously he thought nobody stood a chance of reaching them here in a storm to deliver mail. Harry privately agreed, though the thought didn’t cheer him up at all.   As night fell, the promised storm blew up around them. Spray from the high waves splattered the walls of the hut and a fierce wind rattled the filthy windows. Aunt Petunia found a few moldy blankets in the second room and made up a bed for Dudley on the moth-eaten sofa. She and Uncle Vernon went off to the lumpy bed next door, and Harry was left to find the softest bit of floor he could and to curl up under the thinnest, most ragged blanket.   The storm raged more and more ferociously as the night went on. Harry couldn’t sleep. He shivered and turned over, trying to get comfortable, his stomach rumbling with hunger. Dudley’s snores were drowned by the low rolls\n",
      "- He was busy rummaging in his cloak, looking for something. But he did seem to realize he was being watched, because he looked up suddenly at the cat, which was still staring at him from the other end of the street. For some reason, the sight of the cat seemed to amuse him. He chuckled and muttered, “I should have known.”   He found what he was looking for in his inside pocket. It seemed to be a silver cigarette lighter. He flicked it open, held it up in the air, and clicked it. The\n",
      "- As they couldn’t go through the mail slot they had been pushed under the door, slotted through the sides, and a few even forced through the small window in the downstairs bathroom.   Uncle Vernon stayed at home again. After burning all the letters, he got out a hammer and nails and boarded up the cracks around the front and back\"\n",
      "\n",
      "Prompt: Who is Fluffy, and what role does he play?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = random.choice(query_list)\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Get relevant resources\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "    \n",
    "# Create a list of context items\n",
    "context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "# Format prompt with context items\n",
    "prompt = prompt_formatter(query=query,\n",
    "                          context_items=context_items)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a good looking prompt!\n",
    "\n",
    "We can tokenize this and pass it straight to our LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How does Harry confront Professor Quirrell and Voldemort?\n",
      "RAG answer:\n",
      "<bos>The context does not provide any information about Fluffy, so I cannot answer this question from the provided context.<eos>\n",
      "CPU times: total: 234 ms\n",
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate an output of tokens\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             temperature=0.7, # lower temperature = more deterministic outputs, higher temperature = more creative outputs\n",
    "                             do_sample=True, # whether or not to use sampling, see https://huyenchip.com/2024/01/16/sampling.html for more\n",
    "                             max_new_tokens=256) # how many new tokens to generate from prompt \n",
    "\n",
    "# Turn the output tokens into text\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"RAG answer:\\n{output_text.replace(prompt, '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yesssssss!!!\n",
    "\n",
    "Our RAG pipeline is complete!\n",
    "\n",
    "We just Retrieved, Augmented and Generated!\n",
    "\n",
    "And all on our own local GPU!\n",
    "\n",
    "How about we functionize the generation step to make it easier to use?\n",
    "\n",
    "We can put a little formatting on the text being returned to make it look nice too.\n",
    "\n",
    "And we'll make an option to return the context items if needed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query, \n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True, \n",
    "        return_answer_only=True):\n",
    "    \"\"\"\n",
    "    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get just the scores and indices of top related results\n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings)\n",
    "    \n",
    "    # Create a list of context items\n",
    "    context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "    # Add score to context item\n",
    "    for i, item in enumerate(context_items):\n",
    "        item[\"score\"] = scores[i].cpu() # return score back to CPU \n",
    "        \n",
    "    # Format the prompt with context items\n",
    "    prompt = prompt_formatter(query=query,\n",
    "                              context_items=context_items)\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate an output of tokens\n",
    "    outputs = llm_model.generate(**input_ids,\n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=True,\n",
    "                                 max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    # Turn the output tokens into text\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    if format_answer_text:\n",
    "        # Replace special tokens and unnecessary help message\n",
    "        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
    "\n",
    "    # Only return the answer without the context items\n",
    "    if return_answer_only:\n",
    "        return output_text\n",
    "    \n",
    "    return output_text, context_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How does Harry confront Professor Quirrell and Voldemort?\n",
      "[INFO] Time taken to get scores on 359 embeddings: 0.00006 seconds.\n",
      "Answer:\n",
      "\n",
      "The context does not provide any information about how Harry confronts Professor\n",
      "Quirrell or Voldemort, so I cannot answer this question from the provided\n",
      "context.\n",
      "Context items:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': 14,\n",
       "  'sentence_chunk': 'happen. Harry Potter rolled over inside his blankets without waking up. One small hand closed on the letter beside him and he slept on, not knowing he was special, not knowing he was famous, not knowing he would be woken in a few hours’ time by Mrs. Dursley’s scream as she opened the front door to put out the milk bottles, nor that he would spend the next few weeks being prodded and pinched by his cousin Dudley.... He couldn’t know that at this very moment, people meeting in secret all over the country were holding up their glasses and saying in hushed voices: “To Harry Potter — the boy who lived!”',\n",
       "  'chunk_char_count': 605,\n",
       "  'chunk_word_count': 112,\n",
       "  'chunk_token_count': 151.25,\n",
       "  'embedding': array([-1.39567652e-03, -8.05436745e-02, -9.37172445e-04, -3.32539063e-03,\n",
       "          2.43339259e-02,  5.33219725e-02, -1.63650569e-02, -3.73153744e-05,\n",
       "          9.93958022e-03,  1.06289508e-02,  3.19287069e-02,  1.41762465e-03,\n",
       "         -1.89624093e-02, -8.28961283e-03, -6.07005432e-02,  1.37118790e-02,\n",
       "         -1.30167778e-03, -5.37809022e-02, -3.94776836e-02, -1.42993145e-02,\n",
       "          7.10962489e-02, -5.94034009e-02, -1.16435317e-02,  3.03415470e-02,\n",
       "         -5.35300700e-04, -1.27449892e-02,  1.92893762e-02,  9.66342911e-03,\n",
       "         -3.51565666e-02,  4.71151173e-02, -8.80426820e-03, -3.44116539e-02,\n",
       "          1.01701729e-02,  1.21971546e-02,  1.21037317e-02,  7.94139691e-03,\n",
       "          5.61638735e-02, -2.78549772e-02,  7.31265694e-02, -1.38388816e-02,\n",
       "         -5.86181358e-02, -1.88637758e-03, -1.36325574e-02,  1.89675440e-04,\n",
       "          1.72390195e-03,  9.40787327e-03, -5.85987270e-02,  2.14887224e-02,\n",
       "          2.63919961e-02, -1.55507261e-02,  4.71111014e-02, -3.51493582e-02,\n",
       "         -8.38660449e-03,  2.11394392e-02,  2.65732221e-03,  2.98289452e-02,\n",
       "          2.75587142e-02,  1.18205771e-02,  4.00439749e-04, -5.70587255e-02,\n",
       "          8.95338133e-02, -2.74363663e-02, -2.52318662e-02,  2.78576072e-02,\n",
       "         -1.39615918e-02, -2.52918573e-03, -1.87176540e-02, -4.37779464e-02,\n",
       "         -3.37571017e-02, -2.72980258e-02,  1.77101940e-02, -3.27963158e-02,\n",
       "         -9.90030076e-03,  2.29377039e-02, -2.86457296e-02,  5.76189123e-02,\n",
       "         -1.44648151e-02, -4.26768921e-02, -3.59275267e-02,  1.78998653e-02,\n",
       "         -2.65590586e-02,  2.49812603e-02, -2.44441889e-02, -2.60401033e-02,\n",
       "         -4.47961502e-03,  2.15422828e-02,  2.00376976e-02, -8.19066837e-02,\n",
       "         -3.27895023e-02,  2.01709606e-02,  1.46707073e-02,  8.53058472e-02,\n",
       "          3.30488682e-02,  1.26884669e-01, -4.76393849e-02,  1.38381924e-02,\n",
       "         -4.24905308e-02,  6.90743029e-02, -8.78335722e-03,  3.28378119e-02,\n",
       "          2.39956249e-02, -5.06911147e-03, -7.03456625e-02,  5.74892852e-03,\n",
       "          3.39120664e-02, -9.38156806e-03, -3.82405482e-02, -2.66395845e-02,\n",
       "         -1.97021011e-02, -2.25756853e-03, -5.32941744e-02, -2.20114980e-02,\n",
       "          2.07150038e-02,  4.49821865e-03,  8.90996028e-03,  2.26524211e-02,\n",
       "          2.23349836e-02,  3.51126678e-03, -6.84032589e-03, -3.73034505e-03,\n",
       "          8.50690622e-03, -3.84522863e-02,  2.13092044e-02,  3.84368114e-02,\n",
       "         -2.63576116e-03,  4.38308418e-02,  3.49003859e-02, -4.19330299e-02,\n",
       "         -3.60298814e-04, -2.89536677e-02, -6.55205771e-02, -2.08455902e-02,\n",
       "          1.04581024e-02, -6.72042966e-02, -3.15452367e-02, -6.47750869e-03,\n",
       "          2.06225030e-02,  8.97617862e-02,  1.27643701e-02, -8.07368662e-03,\n",
       "         -4.96896207e-02,  2.57651526e-02,  1.06635038e-02,  3.98729518e-02,\n",
       "          7.09239976e-04,  3.94633152e-02,  3.60535346e-02,  5.12345545e-02,\n",
       "         -3.00470404e-02,  5.69194220e-02,  5.15412018e-02, -2.70048324e-02,\n",
       "         -8.39918386e-03, -4.23844084e-02,  8.47562123e-03, -3.08302441e-03,\n",
       "         -1.89371426e-02,  1.22986287e-02, -2.34219618e-02,  4.18273769e-02,\n",
       "         -4.09880327e-03,  6.08324260e-02,  3.92452441e-02,  2.15188526e-02,\n",
       "         -1.85820106e-02, -4.06914614e-02,  5.09753041e-02,  1.83378849e-02,\n",
       "          4.92875762e-02,  4.29761538e-04,  6.51443098e-03,  4.51961011e-02,\n",
       "          4.59998958e-02,  4.19367664e-02,  1.35182971e-02,  7.86985364e-03,\n",
       "         -5.31570353e-02, -2.66651809e-03,  1.75752006e-02,  2.54924390e-02,\n",
       "         -1.48716355e-02,  2.44616270e-02, -4.87313984e-04, -1.21727604e-02,\n",
       "          1.90174859e-02,  1.73220336e-02, -2.12976635e-02, -1.48836169e-02,\n",
       "         -6.75591081e-02,  9.89790657e-04, -4.35689352e-02,  3.64882313e-02,\n",
       "          3.64883207e-02,  8.14830959e-02,  3.51215713e-02,  2.62196437e-02,\n",
       "          4.78027528e-03,  1.85250759e-03,  6.01308234e-02, -4.71557006e-02,\n",
       "         -6.81501231e-04,  7.61962915e-03, -2.19189581e-02, -2.48896312e-02,\n",
       "         -3.17252390e-02, -2.60109380e-02,  2.23547090e-02,  5.97266108e-03,\n",
       "         -1.64584778e-02,  3.34651433e-02, -2.42461171e-02,  5.46621066e-03,\n",
       "         -7.15190836e-04,  3.23705412e-02,  2.18290929e-02, -2.87356451e-02,\n",
       "          4.38680016e-02,  1.42040886e-02, -8.73774514e-02,  6.96281297e-03,\n",
       "         -1.88383367e-02,  5.12083666e-03, -3.91119011e-02,  3.44510167e-03,\n",
       "         -3.16200182e-02, -8.56406912e-02, -2.19740979e-02,  2.71829823e-03,\n",
       "         -4.63759676e-02,  3.52283381e-03, -5.00144772e-02,  4.35461774e-02,\n",
       "         -7.15673938e-02, -7.23604485e-02,  6.62485044e-03, -3.40167172e-02,\n",
       "         -8.84116685e-04, -1.34889018e-02,  2.35183150e-04,  2.62066312e-02,\n",
       "          1.84437465e-02,  8.17209110e-03,  8.34591407e-03,  1.88693795e-02,\n",
       "          5.13433246e-03, -2.68599894e-02,  3.41093652e-02,  6.44756481e-02,\n",
       "          7.73328468e-02, -7.23345950e-02,  4.69859652e-02,  1.32063245e-02,\n",
       "         -3.51631455e-02, -2.55244412e-02,  2.17042640e-02, -1.07662287e-02,\n",
       "          2.61682887e-02,  5.71184829e-02,  1.55558595e-02,  1.16775431e-01,\n",
       "          3.08187250e-02,  1.30236689e-02, -5.09215370e-02,  9.60995909e-03,\n",
       "         -1.51120657e-02,  1.51119311e-03, -4.72531505e-02,  3.41313221e-02,\n",
       "         -3.04723773e-02, -4.77797957e-03, -9.69570130e-03,  7.98391923e-03,\n",
       "         -1.61892883e-02,  2.49253735e-02, -1.63489114e-02,  2.17315219e-02,\n",
       "          2.56150849e-02,  6.59529045e-02,  5.49052879e-02, -4.37605530e-02,\n",
       "         -2.09040195e-02, -3.91680077e-02, -3.90557759e-02, -6.99258875e-03,\n",
       "         -3.35880672e-03,  3.16273682e-02,  2.18941341e-03,  2.46972218e-02,\n",
       "          1.60484035e-02, -3.28492485e-02, -3.58333252e-02,  7.82082602e-03,\n",
       "          2.72512771e-02,  4.26070690e-02,  3.90052162e-02,  3.76464576e-02,\n",
       "         -1.73292344e-03, -7.01008439e-02, -3.44617967e-03, -3.51712257e-02,\n",
       "         -7.71563500e-02, -4.60095815e-02, -5.09846024e-02,  1.98573954e-02,\n",
       "          1.35933340e-03, -1.73102990e-02,  6.82269782e-03, -1.93462390e-02,\n",
       "         -3.32448669e-02,  3.37171778e-02, -1.63029376e-02,  4.16335128e-02,\n",
       "         -1.63959665e-03, -1.67597027e-03, -6.92017153e-02,  1.66267920e-02,\n",
       "         -1.71082448e-02, -4.35357727e-02,  4.08815630e-02,  4.10262980e-02,\n",
       "         -1.27690705e-02, -3.69162187e-02, -2.91825421e-02, -3.85723747e-02,\n",
       "          2.65015904e-02, -5.14295101e-02,  4.38545123e-02, -4.43646917e-03,\n",
       "         -4.67322432e-02, -3.79845151e-03, -8.63750055e-02, -2.42888015e-02,\n",
       "         -2.13743374e-02, -1.57292690e-02,  1.03527782e-02,  2.77353171e-02,\n",
       "          2.59772502e-02,  8.64287373e-03,  2.05577165e-02,  3.64550836e-02,\n",
       "         -3.66688855e-02, -1.22763170e-02, -4.83581349e-02,  2.43252851e-02,\n",
       "          1.70014352e-02,  2.15077642e-02, -2.90579274e-02,  1.11336100e-04,\n",
       "         -7.99260475e-03, -2.20586639e-02,  2.28292719e-02,  1.06794341e-02,\n",
       "         -1.02331443e-02,  2.49314252e-02, -4.68781739e-02,  3.38469967e-02,\n",
       "         -1.26771284e-02, -1.26155373e-02, -1.67760719e-02, -4.36873967e-03,\n",
       "         -4.12878320e-02,  3.75006273e-02,  3.67046483e-02, -3.91811132e-03,\n",
       "          1.48918293e-02, -6.66721612e-02,  1.57811735e-02,  1.03274383e-01,\n",
       "          1.96180623e-02, -2.07846835e-02, -1.15214167e-02,  4.95548286e-02,\n",
       "          3.87670174e-02, -1.62611320e-03, -6.23144500e-04, -1.03898952e-02,\n",
       "         -4.99160178e-02, -7.56416172e-02,  3.80157307e-02,  6.01555640e-03,\n",
       "          4.38971957e-03, -4.23307940e-02, -2.82523781e-02, -1.69693436e-02,\n",
       "          3.28647047e-02, -4.83923815e-02, -1.46302385e-02,  1.96027365e-02,\n",
       "          7.27966353e-02, -8.57899990e-03, -3.26545388e-02, -4.57671620e-02,\n",
       "         -9.47446004e-03,  2.10508350e-02,  6.89301267e-02,  8.82178172e-03,\n",
       "          9.82462429e-03,  3.58882882e-02, -2.64645386e-02,  4.10350524e-02,\n",
       "          3.46079022e-02,  9.28774774e-02, -1.32199181e-02,  3.22440639e-02,\n",
       "         -4.38785888e-02,  5.65574691e-02, -5.37456572e-03, -5.00223599e-04,\n",
       "          8.83793645e-03, -1.02638498e-01,  4.85209599e-02,  8.90434012e-02,\n",
       "         -1.75666288e-02, -6.18026555e-02,  5.55193834e-02, -4.01248932e-02,\n",
       "         -4.13151979e-02,  7.26209767e-03,  1.92567538e-02, -2.28829663e-02,\n",
       "         -7.19918543e-03,  3.89786400e-02, -5.83262928e-02,  2.37755124e-02,\n",
       "          4.23929334e-04,  5.70488023e-03,  3.91320400e-02, -4.08010446e-02,\n",
       "          1.29060019e-02, -5.29167168e-02, -5.63080870e-02, -8.26216862e-03,\n",
       "          1.40719078e-02, -3.24340425e-02, -5.22283954e-04,  9.64799710e-03,\n",
       "          3.40624377e-02,  2.50704307e-03, -1.96976773e-02,  3.74490395e-03,\n",
       "         -5.49142659e-02,  2.19440460e-02,  4.97080721e-02, -9.56051871e-02,\n",
       "          1.80204902e-02,  2.59094294e-02, -1.67499483e-02,  1.03125591e-02,\n",
       "          8.79039336e-03,  1.73347332e-02, -6.14440031e-02, -6.17545433e-02,\n",
       "          2.01498922e-02, -1.45242602e-01, -7.28862956e-02,  2.90459432e-02,\n",
       "         -7.58842984e-03, -8.42034351e-03, -1.00682974e-02, -1.14917252e-02,\n",
       "          6.74859527e-03, -3.36335301e-02, -2.15983726e-02,  5.89598790e-02,\n",
       "          3.48309912e-02,  1.08352005e-02, -3.23922932e-02,  7.76197948e-03,\n",
       "          3.02584190e-02,  5.09217978e-02,  3.55966538e-02,  4.18521911e-02,\n",
       "         -2.99431831e-02, -2.76229181e-03,  2.11952850e-02,  3.41853648e-02,\n",
       "          1.85332552e-03,  4.18884158e-02, -6.12671599e-02,  2.23653894e-02,\n",
       "          2.01027803e-02,  8.61240551e-03,  6.21940754e-03, -3.10663097e-02,\n",
       "         -2.01321505e-02,  2.80187540e-02, -3.62946950e-02,  9.02382731e-02,\n",
       "         -4.05067205e-02, -2.41800789e-02,  1.73180103e-02,  5.82917733e-03,\n",
       "         -9.54506081e-03,  2.72738710e-02,  3.11950597e-33, -3.63336019e-02,\n",
       "          4.76940488e-03,  1.66475796e-03,  2.97846142e-02, -6.00362048e-02,\n",
       "          8.88592098e-03, -8.08905810e-03,  2.25126054e-02,  1.94568168e-02,\n",
       "         -2.47536576e-03, -5.61033078e-02,  2.10874341e-02, -1.75963733e-02,\n",
       "          4.76139132e-03,  8.98815598e-03,  3.44473682e-02,  6.64369836e-02,\n",
       "         -1.34415263e-02,  1.38182798e-02,  3.35199609e-02, -2.38843057e-02,\n",
       "          6.78353384e-02, -5.58573566e-02,  5.99359162e-02,  3.31897917e-03,\n",
       "         -1.16780910e-04,  2.86796391e-02, -1.27085056e-02, -1.90350891e-03,\n",
       "          2.45958548e-02,  9.33409203e-03, -3.28246914e-02,  4.70857322e-02,\n",
       "         -3.24213393e-02, -7.93349296e-02, -9.32171568e-03, -1.42075019e-02,\n",
       "          2.67225131e-02, -4.09394838e-02, -5.67651214e-03, -3.68513018e-02,\n",
       "          1.90522603e-03,  1.93354469e-02, -1.82694945e-04, -1.16811497e-02,\n",
       "          1.71094667e-02,  4.83021513e-03, -9.93700698e-03,  1.93131790e-02,\n",
       "          2.70538554e-02,  4.51455265e-02,  8.84373263e-02, -1.86922085e-02,\n",
       "         -6.53925017e-02,  8.32512975e-03,  3.33461724e-02,  5.27268834e-02,\n",
       "         -2.57962174e-03,  4.75928038e-02,  1.52473394e-02, -1.86375342e-02,\n",
       "          4.80023064e-02,  7.48488121e-03,  2.91424040e-02, -3.69106494e-02,\n",
       "          2.48198751e-02,  5.44093773e-02,  2.26798025e-03, -1.25175230e-02,\n",
       "         -7.85608776e-03,  5.16143553e-02, -3.89947854e-02,  1.84424855e-02,\n",
       "          1.57879181e-02, -1.73570216e-02, -2.44213790e-02, -4.79174517e-02,\n",
       "         -1.01745632e-02,  6.13048971e-02,  1.92714781e-02, -4.02991660e-02,\n",
       "          6.18485315e-03,  3.32306363e-02,  2.98986454e-02, -1.95417628e-02,\n",
       "         -6.84700906e-03,  2.70952913e-03,  2.36747526e-02, -7.33484998e-02,\n",
       "          9.57725942e-03, -4.14116792e-02,  1.72447953e-02,  3.71905771e-04,\n",
       "          6.12378411e-04, -2.09680200e-02,  7.70257087e-03,  5.21599799e-02,\n",
       "         -9.67817754e-03, -2.81517599e-02, -4.90725636e-02, -1.36421267e-02,\n",
       "         -5.82893677e-02,  1.32457316e-02,  6.65942207e-03,  1.56433433e-02,\n",
       "          3.42107229e-02, -1.12065598e-02, -1.09915501e-02,  3.17139402e-02,\n",
       "         -1.58448853e-02,  3.96739319e-02,  6.04577065e-02,  1.74680687e-02,\n",
       "         -1.08043224e-01,  7.08139390e-02,  1.29164914e-02, -6.50838464e-02,\n",
       "         -5.21948328e-03,  2.78184935e-02,  4.60246466e-02, -1.43105509e-02,\n",
       "         -4.13356014e-02, -1.85460020e-02, -5.53340279e-02,  3.17115039e-02,\n",
       "          1.23265209e-02, -3.02022658e-02,  2.86878459e-03, -6.02438785e-02,\n",
       "         -3.80979851e-02, -2.28195786e-02,  6.93815760e-04,  1.64449513e-02,\n",
       "         -1.78227965e-02, -1.16444109e-02,  5.36978245e-03,  1.29819410e-02,\n",
       "          1.47337010e-02, -3.89139280e-02,  2.87043280e-03,  1.07887667e-02,\n",
       "         -1.59554277e-02,  3.77161913e-02, -1.64611824e-02,  7.45177921e-03,\n",
       "         -2.54323538e-02, -6.42184615e-02, -4.14391719e-02, -5.05285561e-02,\n",
       "         -1.29122464e-02,  1.90369375e-02, -3.65216620e-02,  4.64086309e-02,\n",
       "         -1.13357399e-02, -4.05308306e-02,  1.51399020e-02, -1.98527556e-02,\n",
       "         -1.53480014e-02, -6.78803101e-02,  6.66062832e-02, -1.27740912e-02,\n",
       "         -2.25903504e-02,  3.96175198e-02, -5.51122054e-03, -1.90023670e-03,\n",
       "          4.13136557e-02,  4.73425798e-02, -3.10462643e-03, -1.09912166e-02,\n",
       "          4.34594601e-02, -7.82800168e-02,  3.49835195e-02, -6.65667513e-03,\n",
       "          5.38449362e-02,  6.59144595e-02,  1.88663993e-02,  1.50952982e-02,\n",
       "         -8.12370144e-03, -6.11059507e-03, -5.70987463e-02,  2.60248892e-02,\n",
       "          6.25847802e-02, -3.12743671e-02, -3.48197948e-03, -2.84044873e-02,\n",
       "         -1.47744045e-02, -3.13568115e-02,  5.77738509e-02, -1.07379034e-01,\n",
       "         -1.83452088e-02, -1.83265712e-02, -1.74119044e-02,  2.59826109e-02,\n",
       "          4.34077391e-03,  6.44154549e-02, -1.56886335e-02, -7.21805636e-03,\n",
       "          2.91913245e-02,  1.33885797e-02, -2.74799056e-02,  6.99996576e-02,\n",
       "          1.99718233e-02, -1.05245700e-02, -2.78080795e-02,  4.32029106e-02,\n",
       "         -1.76503658e-02,  2.31801011e-02,  3.34977768e-02, -6.85520610e-03,\n",
       "          2.96639241e-02, -5.05210683e-02,  6.32019341e-02, -8.19220096e-02,\n",
       "          1.74884945e-02, -2.67111901e-02, -3.08947526e-02, -1.41628720e-02,\n",
       "          3.62661593e-02, -1.16048567e-02, -6.15122030e-03, -2.39813738e-02,\n",
       "         -1.15950713e-02,  3.76372784e-03, -6.72957897e-02,  4.08176444e-02,\n",
       "          4.49898317e-02, -2.36425884e-02, -2.35455148e-02, -9.12160426e-03,\n",
       "          3.55143938e-03, -1.82506572e-02,  7.54506290e-02, -5.34225022e-03,\n",
       "         -3.14903483e-02,  2.08372511e-02,  4.75350246e-02, -2.33663246e-02,\n",
       "         -1.31540131e-02,  5.32095581e-02,  3.21847163e-02,  5.27074561e-02,\n",
       "          2.66344361e-02,  8.01291317e-03, -3.19054760e-02, -2.74557918e-02,\n",
       "          5.95795363e-02, -4.21774760e-02,  9.00587961e-02, -4.82902117e-02,\n",
       "          7.55110290e-03,  6.05651131e-03,  3.39591056e-02,  1.74855068e-02,\n",
       "          1.85979996e-02,  1.83240846e-02, -4.48569208e-02,  7.07706809e-02,\n",
       "         -2.99481256e-03,  3.97566743e-02, -1.28740883e-02, -5.88763170e-02,\n",
       "         -5.25267906e-02,  1.97848901e-02,  1.54086128e-02, -3.45867709e-03,\n",
       "         -5.39885238e-02, -3.11298072e-02, -4.58589792e-02, -8.33362900e-03,\n",
       "         -3.43854278e-02, -1.11869974e-02, -1.22531848e-02,  4.75810617e-02]),\n",
       "  'score': tensor(0.0743)},\n",
       " {'page_number': 107,\n",
       "  'sentence_chunk': '“Give it here,” Harry called, “or I’ll knock you off that broom!”   “Oh, yeah?”said Malfoy, trying to sneer, but looking worried.   Harry knew, somehow, what to do. He leaned forward and grasped the broom tightly in both hands, and it shot toward Malfoy like a javelin. Malfoy only just got out of the way in time; Harry made a sharp about-face and held the broom steady. A few people below were clapping.   “No Crabbe and Goyle up here to save your neck, Malfoy,” Harry called.   The same thought seemed to have struck Malfoy.   “Catch it if you can, then!”he shouted, and he threw the glass ball high into the air and streaked back toward the ground.   Harry saw, as though in slow motion, the ball rise up in the air and then start to fall. He leaned forward and pointed his broom handle down — next second he was gathering speed in a steep dive, racing the ball — wind whistled in his ears, mingled with the screams of people watching — he stretched out his',\n",
       "  'chunk_char_count': 961,\n",
       "  'chunk_word_count': 188,\n",
       "  'chunk_token_count': 240.25,\n",
       "  'embedding': array([-3.88489924e-02, -4.66598794e-02,  3.81800234e-02, -5.99504274e-05,\n",
       "          4.26387824e-02,  3.70429046e-02,  1.58957820e-02,  6.51510358e-02,\n",
       "         -1.35576194e-02,  2.99763493e-02,  4.55701686e-02, -1.96552407e-02,\n",
       "         -1.94966067e-02, -2.97682360e-02, -3.61204520e-02, -2.13971473e-02,\n",
       "          7.15207634e-03, -7.85880834e-02, -3.01716067e-02, -5.30178174e-02,\n",
       "          7.11328816e-03, -5.75678796e-02,  3.98821682e-02, -5.50768077e-02,\n",
       "          1.61535088e-02, -1.84744447e-02, -3.61190066e-02,  2.24996358e-02,\n",
       "         -1.49929225e-02,  7.59510370e-03, -3.00240237e-02, -2.34736572e-03,\n",
       "          5.74136060e-03,  4.49767485e-02,  1.27563030e-02,  9.65766422e-03,\n",
       "          4.67305165e-03, -9.19611542e-04,  1.79395191e-02,  1.05630117e-03,\n",
       "         -7.57266432e-02,  5.47930598e-03,  8.75567831e-03,  1.29590658e-02,\n",
       "          1.12574818e-02, -1.48635521e-03, -7.50548989e-02, -4.48572673e-02,\n",
       "         -2.93060727e-02, -2.60267258e-02,  4.20435667e-02,  3.78279276e-02,\n",
       "         -3.53592671e-02, -2.19615921e-02,  1.64872874e-02,  5.74154109e-02,\n",
       "          5.18580899e-02,  4.77478886e-03, -4.89406660e-03, -7.12530762e-02,\n",
       "          7.18287453e-02, -2.34671403e-02, -3.47780623e-02,  5.58087453e-02,\n",
       "         -5.62499538e-02, -2.83215269e-02, -3.10765505e-02, -7.32829468e-03,\n",
       "         -4.78026867e-02,  1.88656878e-02,  1.26145789e-02, -3.55607308e-02,\n",
       "         -5.51738357e-03,  2.32605692e-02,  6.22199895e-03,  7.46868402e-02,\n",
       "          4.90040705e-02,  1.85033828e-02, -8.53419118e-03,  3.37630101e-02,\n",
       "         -3.49348150e-02,  6.53917193e-02, -3.32572460e-02, -4.04648781e-02,\n",
       "         -9.10585467e-03, -5.48335500e-02,  1.68843009e-02,  1.53929116e-02,\n",
       "         -1.00904023e-02, -3.25892754e-02,  5.01008425e-03,  7.33118206e-02,\n",
       "          1.15042049e-02,  9.28731337e-02, -2.22483575e-02, -7.67736183e-03,\n",
       "         -1.66314375e-02,  9.03372169e-02, -4.70344722e-02,  9.27003399e-02,\n",
       "          1.39925452e-02, -1.77101754e-02, -4.26089428e-02, -2.69113854e-02,\n",
       "          2.55212877e-02,  2.42249724e-02,  4.30501327e-02,  1.15749147e-02,\n",
       "         -3.04304548e-02,  1.76394582e-02, -5.55298617e-03,  1.15446895e-02,\n",
       "          7.69237289e-03,  3.36136520e-02,  3.12132556e-02,  5.30538056e-03,\n",
       "         -1.56235034e-02, -2.39679944e-02, -1.36813289e-03, -3.40807140e-02,\n",
       "          4.19429652e-02, -6.34598685e-03,  5.00794835e-02,  3.03471349e-02,\n",
       "          5.03063016e-03, -1.37182521e-02,  2.22048964e-02, -4.32135239e-02,\n",
       "          3.07796374e-02, -4.37779650e-02, -4.79406305e-02, -1.67703852e-02,\n",
       "          5.63692264e-02, -1.23880617e-02, -3.99255715e-02,  2.20032781e-03,\n",
       "          3.03254742e-02, -3.26748826e-02, -3.84482346e-03,  2.95736012e-03,\n",
       "         -2.61553098e-02, -1.85759477e-02, -1.66309886e-02,  6.86966907e-03,\n",
       "         -3.49737331e-02, -1.27523225e-02,  8.94150697e-03,  3.51312645e-02,\n",
       "         -1.54396612e-02,  6.95763603e-02,  4.31202240e-02, -3.07243839e-02,\n",
       "          3.52129037e-03, -2.26999428e-02,  2.02773735e-02,  2.51664985e-02,\n",
       "         -4.62204590e-02,  1.16750542e-02, -6.13963902e-02,  5.90363378e-03,\n",
       "         -9.33112949e-03,  8.67546871e-02, -3.37439887e-02,  2.90443539e-03,\n",
       "         -1.15994662e-02,  1.69837642e-02,  5.68673620e-03,  2.87197679e-02,\n",
       "          3.45855914e-02,  1.97394248e-02, -6.87388005e-03,  6.48027062e-02,\n",
       "          4.48642857e-02,  3.49462614e-04,  1.11825988e-02, -5.37754549e-03,\n",
       "         -2.73321178e-02, -2.27417555e-02,  4.22455079e-04, -1.08670071e-02,\n",
       "         -6.85624480e-02,  4.34238613e-02,  1.98760871e-02, -2.74078827e-02,\n",
       "          1.29962368e-02,  1.23114302e-03, -3.13382559e-02, -4.08314988e-02,\n",
       "         -1.16337083e-01,  6.39770478e-02,  2.86259921e-03, -6.06168527e-03,\n",
       "          3.81432511e-02,  3.17548998e-02,  5.66479117e-02, -1.52093568e-03,\n",
       "         -6.62970096e-02, -2.06756890e-02,  5.32489680e-02,  1.41655346e-02,\n",
       "         -1.92944854e-02, -5.69772199e-02, -3.96957658e-02, -2.02967953e-02,\n",
       "          2.19056234e-02, -3.02915964e-02, -2.78734230e-02,  9.79007967e-03,\n",
       "         -1.76779442e-02,  2.52945907e-02,  2.27112137e-02, -2.72246357e-02,\n",
       "         -5.01638278e-02,  3.11958436e-02,  5.66986687e-02,  2.43413858e-02,\n",
       "          2.54768934e-02, -1.60045251e-02, -3.70870307e-02, -1.29624316e-02,\n",
       "         -5.59940236e-03,  1.72248557e-02, -4.84735556e-02, -9.03271046e-03,\n",
       "          3.65443416e-02, -5.62820546e-02, -9.40928515e-03, -3.14167403e-02,\n",
       "         -2.14950349e-02,  3.12497448e-02,  2.25451961e-03,  1.91167649e-02,\n",
       "         -3.94838341e-02, -3.79952230e-02, -6.47900952e-03, -1.77405011e-02,\n",
       "         -1.59361283e-03,  5.39018512e-02,  3.67359482e-02,  2.76448149e-02,\n",
       "          1.68121103e-02,  2.49156933e-02,  6.92752283e-03,  4.70908955e-02,\n",
       "          7.00206356e-03, -3.55819836e-02,  2.21090652e-02,  1.41001511e-02,\n",
       "          4.71835844e-02, -5.94224930e-02, -2.85191722e-02, -2.02849284e-02,\n",
       "         -3.19465585e-02, -2.70495676e-02,  2.28455830e-02,  8.78891349e-03,\n",
       "         -3.59016843e-02,  5.04488265e-03,  3.18695977e-02,  6.76682666e-02,\n",
       "          4.07063738e-02, -6.38180003e-02, -1.26791745e-02,  3.81064974e-02,\n",
       "         -3.98356421e-03,  4.32252400e-02, -1.50392717e-02,  3.16381119e-02,\n",
       "         -4.18667495e-02, -2.52380166e-02,  8.62639688e-04, -5.02186269e-02,\n",
       "          1.05862971e-02,  2.05465443e-02,  3.46182398e-02,  2.51181563e-03,\n",
       "         -4.28560004e-02,  7.34659284e-02, -2.66096797e-02, -1.90908369e-02,\n",
       "          4.35899422e-02, -6.67717084e-02, -1.77318463e-03, -5.37323058e-02,\n",
       "          6.31050614e-04,  4.11608703e-02, -5.52849993e-02,  2.79549193e-02,\n",
       "          1.33049600e-02, -3.44567895e-02,  4.68231086e-03,  4.11982164e-02,\n",
       "          8.91768315e-04,  4.34903195e-03, -1.31444475e-02,  1.43453153e-03,\n",
       "         -3.85652892e-02, -2.40312833e-02,  2.03136764e-02, -7.99931411e-04,\n",
       "         -1.56851709e-02,  9.04410519e-03, -2.28728596e-02,  3.29997763e-02,\n",
       "         -8.34458321e-03,  2.63076499e-02, -3.20271514e-02,  5.00448514e-03,\n",
       "         -9.50737670e-03,  9.89175290e-02, -1.11633111e-02,  4.05135937e-02,\n",
       "         -6.36379793e-02, -1.17834378e-02, -5.28182797e-02,  6.10876502e-03,\n",
       "         -3.07012685e-02, -4.78430605e-03, -2.52761994e-03, -1.45128341e-02,\n",
       "         -3.22313118e-03, -5.08921361e-03, -3.71663794e-02,  3.90494205e-02,\n",
       "          5.17926961e-02, -4.22884934e-02,  1.03369020e-02, -2.34500449e-02,\n",
       "          1.82182733e-02, -3.47349653e-03, -4.83377464e-02,  4.90119942e-02,\n",
       "         -2.84641869e-02, -4.40283632e-03,  3.59408222e-02,  3.71733680e-03,\n",
       "          2.25765705e-02, -1.20602846e-02, -1.61313880e-02, -1.15583502e-02,\n",
       "         -4.63779718e-02, -1.72098745e-02, -5.22211641e-02,  4.13022265e-02,\n",
       "          4.39420417e-02,  3.24088372e-02, -6.31646886e-02,  3.61193046e-02,\n",
       "         -4.35162969e-02,  2.24036258e-02, -1.13595985e-02, -3.34410369e-02,\n",
       "         -3.17631364e-02,  4.93620336e-02, -1.59823000e-02,  5.49151115e-02,\n",
       "         -1.82861648e-02, -1.74341556e-02, -2.47102752e-02,  3.28706615e-02,\n",
       "          6.81030564e-03,  7.32718632e-02,  3.10463868e-02, -3.43217552e-02,\n",
       "         -2.73060258e-02, -4.06222558e-03, -5.46118096e-02,  2.39076614e-02,\n",
       "         -5.11066103e-03,  1.27278799e-02,  3.78021114e-02,  3.09288241e-02,\n",
       "          3.43710072e-02,  1.88967120e-02, -4.38814722e-02, -1.10634929e-02,\n",
       "         -6.13228790e-02, -7.06316307e-02,  1.91313643e-02,  4.43355239e-04,\n",
       "         -2.02484685e-03, -3.72358263e-02, -3.33293006e-02,  7.30009936e-03,\n",
       "          8.92206933e-03, -2.02179570e-02, -7.98902363e-02, -1.03616901e-02,\n",
       "         -3.05164661e-02,  8.47343821e-04,  3.63345332e-02, -1.89892687e-02,\n",
       "         -2.69277226e-02, -4.17940179e-03, -3.22952010e-02, -2.98527535e-02,\n",
       "         -2.20156051e-02,  1.50928190e-02, -1.17367320e-02,  6.89545348e-02,\n",
       "          3.43918912e-02,  1.95998792e-02,  4.67283390e-02, -2.18156427e-02,\n",
       "         -2.46628281e-02,  5.73220290e-02,  1.95075795e-02, -2.57922220e-03,\n",
       "         -1.84128080e-02, -6.03380464e-02, -1.52889332e-02,  3.23798656e-02,\n",
       "         -1.71350576e-02, -2.56284233e-02,  5.38284294e-02, -5.92296124e-02,\n",
       "         -4.20822315e-02,  3.06160208e-02,  5.69133312e-02,  8.60859305e-02,\n",
       "         -7.43426569e-03,  3.58269140e-02,  3.89900780e-03, -1.46207586e-03,\n",
       "         -3.67935300e-02, -1.95085444e-02,  6.49845786e-03, -4.59631011e-02,\n",
       "          4.09721024e-02, -6.81746230e-02, -1.15944415e-01, -4.94286697e-03,\n",
       "          1.90975536e-02, -4.61995676e-02,  1.39490957e-03,  2.09524892e-02,\n",
       "          6.69626072e-02,  4.52361554e-02,  2.86107324e-02,  8.60214140e-03,\n",
       "         -3.68105471e-02,  4.57501523e-02, -6.46039436e-04, -3.80492732e-02,\n",
       "          3.23291160e-02, -4.82159061e-03,  1.75225735e-02,  4.55392450e-02,\n",
       "         -1.23406462e-02, -3.93432193e-02, -4.48305942e-02, -3.06870025e-02,\n",
       "         -2.85075810e-02, -9.04742777e-02, -5.91461062e-02,  7.66852945e-02,\n",
       "         -3.60219181e-02, -9.98252258e-03, -1.67131554e-02,  3.84722464e-02,\n",
       "          1.60827916e-02,  1.62664836e-03,  2.59883422e-03,  2.85594296e-02,\n",
       "         -1.19491499e-02, -9.32791643e-03, -2.78533995e-02,  6.36740169e-03,\n",
       "         -4.92764749e-02,  1.84921883e-02,  1.91803649e-02,  2.78382245e-02,\n",
       "         -4.98308688e-02,  2.15594489e-02, -2.19518133e-02,  2.36496739e-02,\n",
       "         -9.90638733e-02, -4.05773427e-03, -5.91321774e-02,  2.28156410e-02,\n",
       "         -1.53296292e-02, -2.80479435e-02,  1.35252737e-02, -2.60791276e-02,\n",
       "         -1.59148704e-02,  6.06566332e-02, -5.64961089e-03,  6.11201748e-02,\n",
       "         -1.50453430e-02,  8.94205365e-03, -8.50029290e-03, -1.90205891e-02,\n",
       "          2.84268381e-03, -3.16175148e-02,  3.37897725e-33,  1.67053696e-02,\n",
       "          5.28977811e-02,  8.97079930e-02,  2.31441706e-02, -4.46875244e-02,\n",
       "          9.02133714e-03,  1.22463051e-02,  3.00265849e-02,  7.66836514e-04,\n",
       "         -1.56630557e-02, -6.25982042e-03, -1.29955988e-02, -2.26992611e-02,\n",
       "         -1.08657191e-02, -2.42822785e-02, -2.00122055e-02,  3.49329636e-02,\n",
       "          4.50064689e-02, -1.79280471e-02,  3.88419675e-03,  8.40738788e-03,\n",
       "         -1.71288699e-02, -1.49595290e-02,  5.26304208e-02,  4.58814390e-02,\n",
       "          1.19508114e-02,  4.12119925e-02,  6.01823628e-03, -1.93832684e-02,\n",
       "          2.83493660e-02,  4.76528183e-02, -5.86194843e-02,  2.68319156e-02,\n",
       "         -5.41351512e-02, -7.09625855e-02, -2.59748194e-02,  1.81842446e-02,\n",
       "          1.19816428e-02,  7.59901106e-03,  2.59678215e-02, -1.49256699e-02,\n",
       "          4.64367568e-02,  3.87589238e-03, -2.45569330e-02, -2.15628482e-02,\n",
       "         -7.57539868e-02,  1.12802908e-02, -7.28711206e-03, -6.84293360e-03,\n",
       "          8.61576479e-03,  4.41129357e-02,  1.98366232e-02, -1.86773762e-02,\n",
       "         -1.70360524e-02, -4.14246656e-02,  1.78090273e-03,  8.71755630e-02,\n",
       "         -3.66003774e-02,  3.74308340e-02,  2.16235965e-02, -1.87453795e-02,\n",
       "          5.64789660e-02, -2.22605523e-02,  5.96452355e-02, -5.85421100e-02,\n",
       "         -3.74157913e-02,  2.07521804e-02,  3.06901094e-02, -2.08957517e-03,\n",
       "          3.33796777e-02,  8.68040416e-03, -2.86713745e-02, -5.49499653e-02,\n",
       "          5.45640439e-02, -4.60680313e-02, -2.68980488e-02, -2.87499409e-02,\n",
       "         -2.27760915e-02,  3.93093824e-02, -5.88129042e-03,  2.59456085e-03,\n",
       "         -5.29233851e-02,  9.89551395e-02,  6.07262217e-02,  1.06476108e-02,\n",
       "         -1.22763319e-02, -1.66391283e-02,  1.61501020e-02,  9.61943623e-03,\n",
       "          6.45837113e-02, -3.12534943e-02,  4.18067835e-02, -3.89813795e-03,\n",
       "          1.72515307e-03, -3.71131338e-02, -2.46429741e-02,  5.71889654e-02,\n",
       "          3.04034427e-02, -1.56407189e-02,  3.44964117e-02,  7.33136684e-02,\n",
       "         -7.31465146e-02,  5.87406335e-03, -9.14433692e-03, -3.52467559e-02,\n",
       "          6.03293255e-03, -2.58303732e-02,  4.19840775e-03, -1.64991636e-02,\n",
       "         -3.99408825e-02, -1.60238929e-02,  3.52895968e-02, -7.56686646e-03,\n",
       "         -2.73828078e-02,  1.51975024e-02,  3.94340530e-02, -1.16191925e-02,\n",
       "         -6.69713551e-03,  4.17048372e-02,  3.27270515e-02,  7.96921551e-03,\n",
       "         -3.40649560e-02,  1.42004797e-02, -1.99661441e-02,  7.66695365e-02,\n",
       "          1.88655425e-02, -1.83978714e-02,  2.31290031e-02, -6.16729595e-02,\n",
       "         -5.21528535e-02, -2.04496197e-02,  3.57379727e-02,  4.20673601e-02,\n",
       "         -2.76083872e-02, -7.39951385e-03,  2.21766327e-02,  3.12963277e-02,\n",
       "         -1.86274759e-02,  1.41393682e-02, -8.93725641e-03,  5.32801934e-02,\n",
       "         -2.03833245e-02,  2.54047327e-02,  2.59501161e-04, -1.40553238e-02,\n",
       "         -1.94692111e-04, -2.95960717e-03,  2.61063613e-02, -4.35789935e-02,\n",
       "          7.93269137e-04,  5.84917665e-02, -8.26891419e-03,  5.12137711e-02,\n",
       "         -5.67731634e-03, -3.20947506e-02,  2.87511107e-02, -1.93644837e-02,\n",
       "         -6.60154149e-02, -3.17620598e-02,  3.72593813e-02, -2.30983365e-03,\n",
       "         -3.08993831e-02,  3.44580524e-02, -1.32789351e-02, -1.77084084e-03,\n",
       "          1.23803085e-02,  1.18453829e-02, -1.35744428e-02,  3.89335607e-03,\n",
       "          3.96317020e-02, -4.43088897e-02,  1.59994941e-02, -1.63949076e-02,\n",
       "          1.69703886e-02,  2.60989908e-02,  7.25049600e-02, -4.39670123e-02,\n",
       "         -3.65427788e-03,  2.32874695e-02,  3.28383110e-02,  3.47475149e-02,\n",
       "          2.30879765e-02, -1.87371969e-02,  5.88259175e-02,  8.60711038e-02,\n",
       "         -2.50663552e-02, -4.65359502e-02,  8.87195915e-02, -8.33607987e-02,\n",
       "          8.22945032e-03,  2.04467550e-02, -4.97923000e-03, -9.53825307e-04,\n",
       "          3.92459109e-02,  5.84977344e-02, -2.21292442e-03,  1.18201831e-02,\n",
       "          1.03460085e-02,  5.74030802e-02, -4.16315943e-02,  4.14234027e-02,\n",
       "         -2.83754282e-02,  6.51302980e-03, -4.33331728e-02,  8.14095065e-02,\n",
       "          3.17680161e-03, -2.80948523e-02, -8.31768941e-03, -5.25162891e-02,\n",
       "          6.50085928e-03, -4.91265357e-02,  4.31347638e-02, -7.12891966e-02,\n",
       "         -2.88558099e-02, -2.39068829e-02, -6.98041217e-03,  9.71812103e-03,\n",
       "          3.88517119e-02, -2.05617747e-03, -1.55535480e-02,  2.54373662e-02,\n",
       "         -9.60252527e-03, -3.76138352e-02, -4.18868475e-02,  4.35237512e-02,\n",
       "          8.21081102e-02, -5.28135290e-03, -4.28549451e-04, -5.02202399e-02,\n",
       "         -2.94653289e-02, -5.15069216e-02,  5.58156185e-02,  7.31203258e-02,\n",
       "         -2.77657676e-02, -1.65665932e-02,  1.15742004e-02, -2.91954242e-02,\n",
       "         -2.56153867e-02,  2.15119552e-02,  4.50936425e-03, -2.91713569e-02,\n",
       "          4.07738201e-02,  6.70889094e-02,  7.82244140e-04, -4.74125147e-02,\n",
       "         -4.83347960e-02, -1.43889459e-02, -1.68678556e-02, -3.85735221e-02,\n",
       "          3.56441699e-02, -1.57027002e-02,  3.37795019e-02,  7.54501969e-02,\n",
       "         -1.35595566e-02,  7.82905146e-02, -6.61873743e-02, -1.01228924e-02,\n",
       "         -7.26449043e-02,  1.76270362e-02,  6.07024208e-02, -4.56761429e-03,\n",
       "         -1.55271506e-02, -2.96067838e-02,  3.61165293e-02, -2.95282528e-02,\n",
       "         -2.23591644e-02, -7.69237876e-02, -1.36141852e-02, -4.77434173e-02,\n",
       "         -6.16167746e-02,  6.92077875e-02, -3.78290974e-02, -1.58137362e-02]),\n",
       "  'score': tensor(0.0720)},\n",
       " {'page_number': 198,\n",
       "  'sentence_chunk': 'have to drop.”   Harry, who was still playing the flute, waved at Ron to get his attention and pointed at himself.   “You want to go first?Are you sure?”said Ron. “I don’t know how deep this thing goes. Give the flute to Hermione so she can keep him asleep.”   Harry handed the flute over. In the few seconds’ silence, the dog growled and twitched, but the moment Hermione began to play, it fell back into its deep sleep.   Harry climbed over it and looked down through the trapdoor. There was no sign of the bottom.   He lowered himself through the hole until he was hanging on by his fingertips. Then he looked up at Ron and said, “If anything happens to me, don’t follow. Go straight to the owlery and send Hedwig to Dumbledore, right?”   “Right,” said Ron.   “See you in a minute, I hope …”    And Harry let go. Cold, damp air rushed past him as he fell down, down, down and —    FLUMP. With a funny, muffled sort of thump he landed on something soft. He sat up and felt around, his eyes not used to the gloom. It felt as though he was sitting on some sort of plant.   “It’s okay!”he called up to the light the size of a postage stamp, which was the open trapdoor, “it’s a soft landing, you can jump!”   Ron followed right away. He landed, sprawled next to Harry.   “What’s this stuff?”',\n",
       "  'chunk_char_count': 1290,\n",
       "  'chunk_word_count': 265,\n",
       "  'chunk_token_count': 322.5,\n",
       "  'embedding': array([-3.62833180e-02, -6.20290563e-02,  1.29176769e-02, -3.44376378e-02,\n",
       "          4.24421839e-02,  9.95079949e-02, -1.08881518e-02,  5.07487021e-02,\n",
       "          9.65148397e-03,  5.99839864e-03,  1.71271861e-02, -4.24314812e-02,\n",
       "         -2.19699256e-02, -4.35359869e-03, -1.62861180e-02, -3.20113935e-02,\n",
       "          2.31501944e-02, -6.53060377e-02, -7.05148801e-02, -1.76887624e-02,\n",
       "          2.50471346e-02, -3.76264267e-02, -3.35814618e-02,  5.63807599e-03,\n",
       "         -1.08644590e-02, -1.64132379e-02, -1.38370870e-02, -1.87157493e-04,\n",
       "         -4.67056520e-02,  3.87281086e-03, -1.53573956e-02, -1.80782378e-02,\n",
       "          2.74842456e-02,  4.00265828e-02, -1.34116719e-02,  2.12308746e-02,\n",
       "          2.97336560e-02, -4.68465080e-03,  3.32048126e-02, -6.47545420e-03,\n",
       "         -6.37409016e-02, -3.70915458e-02, -3.96589981e-03, -3.34019698e-02,\n",
       "          2.10069045e-02, -2.57552154e-02, -3.34141888e-02, -2.21235808e-02,\n",
       "         -1.12996092e-02,  1.56418607e-03,  4.80956584e-02,  4.41297106e-02,\n",
       "         -2.17050780e-02, -9.90394317e-03,  2.97171120e-02,  6.98502455e-03,\n",
       "          1.98313501e-02, -2.11843848e-02,  1.11177796e-02, -6.35371953e-02,\n",
       "          6.14314675e-02, -3.00136320e-02, -1.77602447e-03,  3.74323945e-03,\n",
       "         -6.19662814e-02, -3.88956480e-02, -1.37592042e-02, -1.47182057e-02,\n",
       "         -4.11857516e-02,  1.45238731e-02,  5.35443202e-02, -2.18550526e-02,\n",
       "         -3.30358855e-02, -2.12452710e-02, -8.66959710e-03,  3.17369588e-02,\n",
       "          4.50044274e-02,  2.41919179e-02, -7.64967827e-03,  4.26644422e-02,\n",
       "         -1.10855224e-02,  3.60833034e-02, -3.78228277e-02,  2.89178081e-02,\n",
       "          2.40313392e-02, -1.60447620e-02,  5.54673411e-02, -4.02533710e-02,\n",
       "          8.68886127e-04,  1.73244271e-02, -9.24961083e-03,  8.50208178e-02,\n",
       "          6.34805486e-03,  1.34193674e-01,  7.94534665e-03, -1.08885346e-02,\n",
       "         -2.86064707e-02,  4.61159721e-02, -3.51403318e-02,  2.34310944e-02,\n",
       "          3.77998203e-02,  1.87545468e-03, -1.01955302e-01, -3.52458693e-02,\n",
       "          3.05825174e-02, -3.04928632e-03,  2.09321417e-02,  1.70486537e-03,\n",
       "         -3.31785269e-02, -8.34676996e-03,  5.83826890e-03, -7.84107205e-03,\n",
       "          3.87245789e-02, -6.36210060e-03,  3.19306809e-03, -1.61787067e-02,\n",
       "         -1.41876470e-02, -3.19089857e-03,  1.36339385e-02,  1.19534610e-02,\n",
       "          2.85486015e-03, -2.08183955e-02,  4.33688089e-02,  6.72949478e-02,\n",
       "          1.18379164e-02, -3.75735424e-02,  7.59343207e-02, -3.45639028e-02,\n",
       "         -1.85853783e-02, -4.31857537e-03, -7.27789775e-02, -2.77225673e-02,\n",
       "          4.67305370e-02, -4.32418212e-02, -3.52372602e-02,  3.92620452e-02,\n",
       "          2.91536171e-02,  4.58679944e-02,  7.05666328e-03,  2.42461246e-02,\n",
       "          3.05640232e-03,  2.11599730e-02, -4.00033779e-02,  2.70239133e-02,\n",
       "         -2.14448888e-02,  9.85333789e-03, -2.70779375e-02,  1.69125348e-02,\n",
       "         -6.47582784e-02,  6.24623597e-02,  3.92822511e-02,  2.98973988e-03,\n",
       "         -9.10737272e-03, -1.99211370e-02,  4.66350019e-02,  2.52488926e-02,\n",
       "         -8.83550383e-04, -3.94346043e-02, -2.30182465e-02,  1.57314371e-02,\n",
       "          1.57109983e-02,  5.81864044e-02, -4.82359603e-02, -4.63888086e-02,\n",
       "         -2.92817745e-02, -1.62766539e-02, -1.09352963e-02,  2.40535960e-02,\n",
       "          5.12498394e-02, -5.05989231e-02, -3.06866746e-02,  6.08907901e-02,\n",
       "          3.03359311e-02,  4.08945307e-02, -5.99645264e-03, -8.93425662e-03,\n",
       "         -6.69232085e-02,  3.50314528e-02, -5.17143961e-03,  3.65871489e-02,\n",
       "         -3.43393795e-02,  4.70361598e-02,  3.78706530e-02, -1.94449481e-02,\n",
       "          2.43407656e-02,  3.29541750e-02, -3.46939787e-02, -4.68465872e-02,\n",
       "         -6.99266270e-02,  9.93968453e-03, -1.62882842e-02,  2.19612159e-02,\n",
       "          2.90791374e-02,  6.38982579e-02,  7.24497810e-02, -9.66323074e-03,\n",
       "         -2.95202527e-02, -1.61114316e-02,  5.61065748e-02, -2.89386865e-02,\n",
       "          1.13374088e-02, -3.18593532e-02, -4.08869376e-03, -3.78589146e-02,\n",
       "         -4.43135463e-02,  7.22464290e-04,  3.02532651e-02,  6.02820190e-03,\n",
       "         -8.65929667e-03,  6.29411777e-03, -1.00085111e-02,  6.48808386e-03,\n",
       "         -4.14021127e-02,  6.46955194e-03,  6.53558150e-02, -7.49034481e-03,\n",
       "          7.55889632e-04,  8.19995906e-03, -8.62221792e-03, -2.30013765e-03,\n",
       "         -1.22173568e-02, -6.69626147e-03, -4.09641303e-02, -8.88489652e-03,\n",
       "          2.53704488e-02,  5.58403973e-03,  1.20541384e-03, -2.31368281e-03,\n",
       "         -1.97537858e-02,  2.13167239e-02, -3.17943357e-02,  2.34881118e-02,\n",
       "         -3.15944850e-02, -3.91479805e-02, -2.03027558e-02, -3.99549399e-03,\n",
       "         -3.11672390e-02,  6.10863185e-03,  6.58947378e-02,  1.71253067e-02,\n",
       "         -2.81734820e-02, -9.90371336e-04,  2.73273289e-02, -1.64165851e-02,\n",
       "          1.03284633e-02, -3.02564371e-02,  2.93132905e-02, -3.97794975e-05,\n",
       "          6.25987798e-02, -6.63260445e-02,  2.07015723e-02,  1.33316126e-02,\n",
       "         -2.16282066e-02, -2.86377706e-02,  2.04815976e-02, -2.15961002e-02,\n",
       "         -1.00195110e-02,  2.85085756e-02,  2.92754993e-02,  3.47267687e-02,\n",
       "          3.78224663e-02, -5.69805689e-02, -3.91157717e-02,  3.58880684e-03,\n",
       "         -3.62234153e-02,  3.30448360e-03, -2.86029559e-02,  4.00153622e-02,\n",
       "         -3.30710337e-02,  7.76823889e-03, -1.76170021e-02, -5.58839664e-02,\n",
       "          2.46619657e-02,  2.46669557e-02,  4.17597182e-02,  4.08660769e-02,\n",
       "         -1.04297707e-02,  4.35966738e-02, -2.21681572e-03, -4.47922684e-02,\n",
       "          2.34535635e-02, -8.69101360e-02, -1.55493682e-02, -3.35168205e-02,\n",
       "         -1.87413637e-02,  1.72400605e-02, -4.45210859e-02,  5.45332395e-02,\n",
       "          5.19083114e-03, -2.83921473e-02, -5.99424727e-03,  4.32819612e-02,\n",
       "          1.97447594e-02, -1.93607353e-04, -2.10108906e-02,  4.35024202e-02,\n",
       "         -2.04995945e-02, -1.11831456e-01,  3.76482569e-02,  9.93060134e-03,\n",
       "         -2.22962573e-02, -3.54826339e-02, -1.14001743e-02,  2.53456309e-02,\n",
       "         -1.26743121e-02, -1.58402591e-03,  1.01281926e-02, -1.70241278e-02,\n",
       "          7.60768354e-03,  5.72280586e-02, -8.54304992e-03,  2.90226694e-02,\n",
       "         -3.99358645e-02,  4.28353027e-02, -4.47007157e-02, -6.17012242e-03,\n",
       "         -2.46317163e-02, -1.59383882e-02,  6.26442023e-03,  2.65553314e-02,\n",
       "         -3.49637046e-02, -1.69539470e-02, -7.90202916e-02,  1.96562987e-02,\n",
       "          5.36838220e-03, -7.61700422e-02, -6.12137420e-03,  6.40903367e-03,\n",
       "          4.67107855e-02, -1.00338059e-02, -6.35806993e-02,  3.73367295e-02,\n",
       "         -1.99653264e-02, -1.37530155e-02,  2.67170351e-02, -2.44914368e-02,\n",
       "         -1.17400382e-03, -2.65738666e-02,  9.27383173e-03,  1.25674019e-02,\n",
       "         -6.99187592e-02, -8.57728440e-03, -6.59093633e-02,  1.63980648e-02,\n",
       "          1.04374094e-02,  1.80675834e-02, -4.33622375e-02,  5.36389127e-02,\n",
       "          2.05268152e-02, -4.55193734e-03,  1.42121362e-02, -1.17562357e-02,\n",
       "         -1.85630061e-02,  2.18100101e-02, -4.96010184e-02,  4.10009399e-02,\n",
       "         -2.69140489e-02, -4.73272055e-02, -3.47519368e-02, -2.79537309e-02,\n",
       "         -3.69153433e-02,  6.67584091e-02,  2.11139396e-02, -3.77291180e-02,\n",
       "         -2.57344097e-02, -6.49562478e-02, -2.87408233e-02,  6.93659708e-02,\n",
       "          1.40416380e-02,  1.64961815e-02, -1.16764335e-02,  9.47311819e-02,\n",
       "          3.53867821e-02, -8.77244025e-03,  2.32866462e-02,  1.09029720e-02,\n",
       "         -4.74431552e-02, -3.43838409e-02,  2.62112040e-02, -1.71422865e-02,\n",
       "         -2.19355598e-02, -2.47422867e-02, -2.16425806e-02, -3.98091227e-03,\n",
       "         -1.92268863e-02,  1.91308428e-02, -4.03581262e-02,  1.08791841e-02,\n",
       "          3.91342444e-03, -4.14490625e-02, -6.68037031e-03, -5.61225899e-02,\n",
       "         -5.25754644e-03, -2.06467230e-02,  1.94707804e-03,  3.93876340e-03,\n",
       "         -5.78226335e-02, -2.93264482e-02,  1.56258494e-02,  3.83219682e-02,\n",
       "          3.09550408e-02,  5.57008125e-02,  2.72019133e-02,  8.40471499e-03,\n",
       "         -4.90761027e-02, -7.85265956e-03,  5.67848347e-02,  2.72356831e-02,\n",
       "         -7.22440705e-03, -5.72118312e-02, -3.56002874e-03,  9.78970528e-02,\n",
       "          1.60332222e-03, -6.17758520e-02,  5.03002517e-02, -3.68460529e-02,\n",
       "         -2.60282140e-02, -2.29441188e-02,  5.33744320e-02,  5.60714938e-02,\n",
       "          4.72130906e-03,  1.48824304e-02, -3.88902090e-02,  8.78652092e-03,\n",
       "         -2.36219838e-02, -3.44671868e-02,  1.14709092e-02, -6.68555722e-02,\n",
       "          1.72658674e-02, -7.86819384e-02, -9.67441872e-02, -1.61240660e-02,\n",
       "          1.71548296e-02, -3.00113708e-02, -3.94117609e-02, -8.00504070e-03,\n",
       "          5.79010621e-02,  1.68139692e-02, -1.63236782e-02, -2.73340866e-02,\n",
       "         -5.98798990e-02,  3.37187201e-03,  3.65991555e-02, -3.57502326e-02,\n",
       "          2.24117693e-02,  1.16460687e-02, -4.23250012e-02,  8.42995290e-03,\n",
       "         -1.76002327e-02, -3.22062406e-04, -4.76748832e-02, -3.56081221e-03,\n",
       "          1.41497999e-02, -1.36880279e-01, -1.45797404e-02,  7.11015165e-02,\n",
       "          1.84994508e-02, -2.19632983e-02,  7.36047886e-03,  3.48426886e-02,\n",
       "          4.34886664e-02, -4.21371534e-02, -1.66535769e-02, -1.49568217e-02,\n",
       "         -2.89856200e-03, -3.45926592e-03, -4.68201824e-02, -2.30198242e-02,\n",
       "         -3.55894379e-02,  2.00575218e-02, -1.26358252e-02,  2.25331802e-02,\n",
       "         -3.78838480e-02,  8.09326861e-03,  3.36896446e-05,  3.26694176e-02,\n",
       "         -2.82386430e-02,  2.63052247e-02, -5.74032478e-02,  2.29024817e-03,\n",
       "         -1.63636822e-02,  6.12159120e-03,  1.30102336e-02, -8.40868894e-03,\n",
       "         -4.13155966e-02,  7.81060848e-03, -4.19450291e-02,  8.27715769e-02,\n",
       "         -2.09658742e-02, -4.23234364e-04, -6.03834689e-02,  4.42756936e-02,\n",
       "         -4.09385981e-03, -1.06100161e-02,  3.57651540e-33, -8.20103753e-03,\n",
       "         -2.32946239e-02,  3.86954751e-03,  3.74404825e-02, -4.14213352e-02,\n",
       "         -9.31333099e-03, -2.96419673e-02,  4.86128330e-02,  6.56019747e-02,\n",
       "         -1.18998867e-02,  8.88490770e-03, -9.41734761e-03, -1.76409241e-02,\n",
       "          1.23791583e-02, -9.50556062e-03,  3.51680839e-03,  1.05916478e-01,\n",
       "          1.30798100e-02, -3.03307567e-02,  7.38158915e-03, -1.78725217e-02,\n",
       "          4.45234738e-02,  6.11957721e-03,  8.66723210e-02,  2.93004550e-02,\n",
       "          1.09343501e-02,  2.61896346e-02, -1.90850664e-02, -1.73671711e-02,\n",
       "          3.29241790e-02,  2.78380718e-02, -1.90714784e-02,  8.27150568e-02,\n",
       "         -3.81144248e-02, -8.75799358e-02, -1.82905849e-02,  4.27504070e-03,\n",
       "         -3.98792513e-03, -1.43735474e-02, -8.88122898e-03, -2.57721171e-03,\n",
       "          1.32922567e-02,  2.73804236e-02,  1.35970218e-02, -4.71378379e-02,\n",
       "          1.93150975e-02,  3.72082330e-02, -3.78201120e-02,  6.85087172e-03,\n",
       "         -7.09940633e-03,  5.50019182e-02,  7.93882832e-02, -6.75713550e-03,\n",
       "         -3.34385149e-02, -2.60723345e-02,  3.60593572e-02,  1.14569582e-01,\n",
       "         -3.48604247e-02,  5.25073074e-02,  1.51873128e-02, -3.01487632e-02,\n",
       "          7.17934743e-02,  3.26001868e-02,  1.61297321e-02, -7.62561336e-02,\n",
       "         -6.31598290e-03,  4.47618216e-02,  3.99388038e-02,  1.03099346e-02,\n",
       "          2.06805635e-02, -9.13979951e-03, -4.69036065e-02,  2.34748423e-02,\n",
       "          4.26892675e-02, -1.68461744e-02, -1.73916202e-02, -7.78093422e-03,\n",
       "          5.97193232e-03,  4.75127995e-03, -4.55407389e-02,  2.53493767e-02,\n",
       "         -2.39116251e-02,  1.56005779e-02,  2.28994153e-02,  1.03721563e-02,\n",
       "         -2.39373930e-02,  4.78119077e-03,  1.44162653e-02, -1.25665381e-03,\n",
       "          2.23904978e-02, -9.22975987e-02,  3.62018012e-02,  6.91679586e-03,\n",
       "          1.86431385e-03, -5.96505776e-03,  7.89557490e-03,  5.34096248e-02,\n",
       "         -1.20503968e-02, -1.12295030e-02,  1.60041067e-03,  3.79637294e-02,\n",
       "         -7.45396316e-02,  2.06498839e-02, -2.79434826e-02,  1.99137777e-02,\n",
       "         -1.74961966e-02, -6.00537024e-02,  8.37217085e-03, -2.42295838e-03,\n",
       "         -4.45537232e-02,  3.10243126e-02,  2.82927640e-02,  3.16670304e-03,\n",
       "         -8.93500149e-02,  2.45323610e-02,  4.38328013e-02, -1.20499786e-02,\n",
       "         -1.71310436e-02,  6.40543401e-02,  1.30315060e-02, -7.67307763e-04,\n",
       "         -1.91625170e-02, -2.33023171e-03, -3.76584828e-02,  1.08389467e-01,\n",
       "          7.01706186e-02, -6.04052767e-02,  2.80037499e-03,  9.34642274e-03,\n",
       "         -8.98935422e-02, -5.93898026e-03,  4.85005835e-03,  1.92636121e-02,\n",
       "         -4.28609587e-02,  5.26292901e-03,  8.19406379e-03,  6.03074171e-02,\n",
       "          6.70955866e-04,  8.49129166e-03,  3.32030952e-02,  3.31756957e-02,\n",
       "         -5.31411469e-02,  2.65790708e-02, -8.70706048e-03, -2.59099761e-03,\n",
       "         -3.61821651e-02, -7.16022309e-03, -2.27583162e-02, -5.27583882e-02,\n",
       "         -3.85574512e-02,  4.79550958e-02, -3.12520079e-02,  2.38211080e-02,\n",
       "          9.65527073e-03, -1.26926824e-02,  1.83342304e-02,  9.17689595e-03,\n",
       "         -3.66551522e-03, -6.42182827e-02,  2.26974878e-02, -2.63115149e-02,\n",
       "          3.20294574e-02,  1.71627626e-02, -3.74583490e-02, -2.25337222e-02,\n",
       "          1.36417979e-02, -1.16521325e-02,  1.52007407e-02, -2.26573590e-02,\n",
       "         -7.53508136e-03, -6.06311113e-02,  3.05231987e-03,  3.87971550e-02,\n",
       "          3.74902189e-02,  6.78007156e-02,  6.24525249e-02,  1.81243308e-02,\n",
       "          2.54712235e-02,  1.12771532e-02, -2.25574262e-02,  9.60331783e-03,\n",
       "         -1.25984810e-02, -3.84100042e-02,  5.73702753e-02,  5.82614802e-02,\n",
       "         -6.42425492e-02, -3.25994454e-02,  5.30997142e-02, -7.25160316e-02,\n",
       "          1.04378313e-02,  5.74700860e-03,  5.07589022e-04, -1.01524070e-04,\n",
       "          1.35524357e-02,  9.36673507e-02,  2.45624557e-02,  2.97169723e-02,\n",
       "          3.45733315e-02,  2.28480585e-02, -1.72923878e-02,  3.94348465e-02,\n",
       "         -3.98359587e-03, -8.65227077e-03, -1.47676002e-02,  5.89919873e-02,\n",
       "         -6.47176197e-03,  5.61045948e-03,  3.29223648e-02, -4.30475697e-02,\n",
       "          2.12260126e-03,  1.50488713e-03,  5.77022508e-02, -5.00993058e-02,\n",
       "          2.74916412e-03, -4.33358252e-02,  8.53011198e-03,  4.43727802e-03,\n",
       "          1.09825013e-02,  4.40896712e-02, -4.54111211e-02,  1.73465982e-02,\n",
       "         -3.51978913e-02, -3.61999217e-03,  4.31379452e-02,  2.57890914e-02,\n",
       "          7.74615034e-02,  3.60150822e-02, -1.55938175e-02, -4.63597178e-02,\n",
       "         -1.00974366e-02, -1.27672022e-02,  5.15260324e-02,  5.35627641e-02,\n",
       "         -6.88041598e-02, -7.49780855e-04,  1.64739769e-02,  1.13990726e-02,\n",
       "         -1.41084502e-02,  5.91510385e-02,  3.34143974e-02,  2.95455158e-02,\n",
       "         -3.10539827e-03,  9.71047506e-02, -3.90394330e-02, -5.72108924e-02,\n",
       "          6.58417260e-03, -3.45117524e-02,  9.15981904e-02, -1.18786506e-02,\n",
       "          4.01904322e-02,  2.83061340e-02,  1.54872413e-03,  4.31087650e-02,\n",
       "         -1.90295223e-02,  2.07914170e-02, -7.44959787e-02,  4.52524275e-02,\n",
       "         -4.49699387e-02,  3.22701260e-02,  1.00021465e-02, -2.67527737e-02,\n",
       "         -3.41767296e-02, -1.51194083e-02,  3.22644785e-02,  2.10199039e-02,\n",
       "         -3.26662101e-02, -5.18006198e-02, -5.26640043e-02, -1.52999694e-02,\n",
       "          3.06515004e-02, -3.80911428e-04, -3.76143716e-02, -1.38020916e-02]),\n",
       "  'score': tensor(0.0708)},\n",
       " {'page_number': 106,\n",
       "  'sentence_chunk': 'out at odd angles.   “Stick out your right hand over your broom,” called Madam Hooch at the front, “and say ‘Up!’”   “UP” everyone shouted.   Harry’s broom jumped into his hand at once, but it was one of the few that did. Hermione Granger’s had simply rolled over on the ground, and Neville’s hadn’t moved at all. Perhaps brooms, like horses, could tell when you were afraid, thought Harry; there was a quaver in Neville’s voice that said only too clearly that he wanted to keep his feet on the ground.   Madam Hooch then showed them how to mount their brooms without sliding off the end, and walked up and down the rows correcting their grips. Harry and Ron were delighted when she told Malfoy he’d been doing it wrong for years.   “Now, when I blow my whistle, you kick off from the ground, hard,” said Madam Hooch. “Keep your brooms steady, rise a few feet, and then come straight back down by leaning forward slightly. On my whistle — three — two —”    But Neville, nervous and jumpy and frightened of being left on the ground, pushed off hard before the whistle had touched Madam Hooch’s lips.   “Come back, boy!”she shouted, but Neville was rising straight up like a cork shot out of a bottle — twelve feet — twenty feet. Harry saw his scared white face look down at the ground falling away, saw him gasp, slip sideways off the broom and —    WHAM — a thud and a nasty crack and Neville lay facedown on the grass in a heap. His broomstick was still rising higher and higher, and started to drift lazily toward the forbidden forest and out of sight.   Madam Hooch was bending over Neville, her face as white as his.   “Broken wrist,” Harry heard her mutter. “Come on, boy — it’s all right, up you get.”   She turned to the rest of the class.   “None of you is to move while I take this boy to the hospital wing!You leave those brooms where they are or you’ll be out of Hogwarts before you can say ‘Quidditch.’Come on, dear.”   Neville, his face tear-streaked, clutching his wrist, hobbled off with Madam Hooch, who had her arm around him.   No sooner were they out of earshot than Malfoy burst into laughter.   “Did you see his face, the great lump?”',\n",
       "  'chunk_char_count': 2155,\n",
       "  'chunk_word_count': 423,\n",
       "  'chunk_token_count': 538.75,\n",
       "  'embedding': array([-1.57035422e-02, -5.26405685e-02,  1.91018209e-02, -3.45306173e-02,\n",
       "          4.69569750e-02,  4.26958576e-02,  8.92062765e-03,  6.96280673e-02,\n",
       "          1.99333578e-02,  4.35549542e-02,  2.93870457e-02, -2.12266389e-02,\n",
       "         -3.04580498e-02, -8.49288795e-03, -9.39515699e-03, -6.83798688e-03,\n",
       "          4.93757101e-03, -9.68041644e-02, -5.15577793e-02, -7.95273557e-02,\n",
       "          2.07803585e-02, -5.86726405e-02, -6.78968523e-03, -2.09540091e-02,\n",
       "          3.65903787e-02, -1.11879287e-02, -8.52342881e-03,  2.30302010e-02,\n",
       "         -2.54320260e-02,  3.59561807e-03, -2.42668744e-02,  1.11366622e-02,\n",
       "          3.32225189e-02,  2.45917048e-02,  1.92259171e-03, -6.30556652e-03,\n",
       "         -1.41996443e-02, -8.55155662e-03, -9.08064935e-03,  4.54387721e-03,\n",
       "         -5.09839617e-02, -2.28495318e-02, -6.52533397e-03, -1.48296859e-02,\n",
       "         -1.27670337e-02,  1.55388955e-02, -3.12041622e-02,  3.52886803e-02,\n",
       "         -1.84053760e-02, -4.03394885e-02,  8.56104642e-02,  4.06154804e-02,\n",
       "         -2.46871524e-02,  7.05379900e-03,  2.91147847e-02,  2.59894598e-02,\n",
       "          4.07354496e-02,  4.81153950e-02,  5.78554696e-04, -5.36955446e-02,\n",
       "          7.65272975e-02, -1.51172327e-02, -2.10808683e-02,  3.70740891e-02,\n",
       "         -5.07678129e-02, -5.28575666e-02, -1.92306917e-02, -4.47115563e-02,\n",
       "         -5.21013439e-02,  2.93811760e-03,  3.91833633e-02, -7.25386590e-02,\n",
       "         -4.80027450e-03, -6.09530089e-03,  4.64933133e-03,  4.58550826e-02,\n",
       "         -1.92757677e-02,  2.26883795e-02,  1.35758827e-02,  6.89233169e-02,\n",
       "         -1.35681508e-02,  3.92649248e-02, -3.30503322e-02, -3.45063023e-02,\n",
       "          8.37283116e-03, -5.89580275e-02,  2.94439681e-02, -1.86470412e-02,\n",
       "         -2.42215884e-03, -2.66756788e-02, -1.73113085e-02,  7.42612705e-02,\n",
       "          2.12462363e-03,  1.45953298e-01, -3.38272825e-02,  9.37315263e-03,\n",
       "         -4.53690514e-02,  9.69446376e-02, -1.86859034e-02,  2.88787242e-02,\n",
       "          5.50373904e-02, -5.13413548e-03, -5.00711910e-02, -1.08723994e-02,\n",
       "          3.88231911e-02,  2.60140281e-02,  3.62507664e-02,  1.91955213e-02,\n",
       "         -5.76526411e-02,  6.08027400e-03, -2.02046726e-02,  4.71184067e-02,\n",
       "         -4.89029195e-03,  6.94969948e-03, -1.11928936e-02, -3.38159166e-02,\n",
       "         -3.86072369e-03, -4.41102535e-02,  1.39250010e-02, -3.91062200e-02,\n",
       "          5.03663011e-02, -1.96249783e-02,  3.22559886e-02,  6.54831082e-02,\n",
       "          4.94452901e-02,  1.62555650e-02,  1.70945693e-02, -3.53154913e-02,\n",
       "         -2.59254198e-03, -1.18465461e-02, -6.45842105e-02, -1.36436177e-02,\n",
       "          7.22736195e-02, -4.46271077e-02, -3.84308696e-02,  2.51570549e-02,\n",
       "          3.43623050e-02,  1.64952446e-02,  2.11894512e-02,  1.10554798e-02,\n",
       "         -2.22110897e-02, -1.14614135e-02, -3.63803580e-02,  1.33960489e-02,\n",
       "         -2.16462649e-02,  2.46318411e-02, -3.30133960e-02,  5.12664169e-02,\n",
       "          1.03535354e-02,  7.17469156e-02,  7.12665021e-02, -2.60410123e-02,\n",
       "         -1.96785573e-02, -4.45790961e-03,  8.79385136e-03,  3.09811328e-02,\n",
       "         -2.61851754e-02, -2.35367799e-03, -1.08113894e-02,  1.34002883e-02,\n",
       "          8.06467421e-03,  2.50205770e-02, -3.08821462e-02, -7.77120749e-03,\n",
       "         -4.59925123e-02, -1.00705583e-04,  1.91687029e-02,  1.42827192e-02,\n",
       "          3.90950739e-02,  7.84711819e-03, -2.55709160e-02,  4.46200781e-02,\n",
       "          2.40943674e-02,  7.52565963e-03,  9.95083619e-03,  2.14470085e-02,\n",
       "         -4.16004509e-02,  1.33588929e-02,  1.93877742e-02,  3.29012312e-02,\n",
       "         -5.32106347e-02,  4.73906882e-02,  3.90395075e-02, -1.69779286e-02,\n",
       "          2.95736287e-02, -2.70478688e-02, -3.57467420e-02, -4.55690594e-03,\n",
       "         -1.10262528e-01,  4.52600466e-03,  3.82752274e-03, -1.78570114e-02,\n",
       "          3.79498750e-02,  3.22156288e-02,  5.43518253e-02,  9.69025493e-03,\n",
       "         -4.39189449e-02, -4.16089743e-02,  5.38692102e-02, -2.29899446e-03,\n",
       "          1.44204241e-03, -4.12001424e-02, -3.82310785e-02, -3.88510376e-02,\n",
       "         -1.01585891e-02, -4.41642925e-02,  2.17903908e-02,  2.07283646e-02,\n",
       "         -8.50509293e-03,  5.82079403e-02, -1.72270071e-02,  1.02012814e-03,\n",
       "         -4.84231263e-02,  7.58399302e-03,  4.48373221e-02,  3.50690894e-02,\n",
       "          2.21601985e-02, -5.60151879e-03, -4.66693155e-02, -7.08169909e-03,\n",
       "         -2.86909621e-02,  3.74925397e-02, -8.06605369e-02,  7.35235261e-03,\n",
       "          2.23223250e-02, -6.20746538e-02,  2.13910155e-02, -2.06913333e-02,\n",
       "         -7.90688861e-03,  1.24407373e-02, -1.35781271e-02,  5.25440238e-02,\n",
       "         -7.95829743e-02, -5.74359819e-02, -6.14210777e-03, -6.32300153e-02,\n",
       "         -1.78033020e-02,  2.89646573e-02,  6.53962493e-02,  2.49327067e-02,\n",
       "         -2.52832752e-02,  2.45963763e-02,  2.55042389e-02, -4.93392721e-03,\n",
       "          1.86531637e-02, -3.89918387e-02,  4.59316820e-02,  4.54633348e-02,\n",
       "          2.57403422e-02, -9.70779210e-02, -4.80366591e-03, -2.43538227e-02,\n",
       "         -3.23652476e-02, -1.69211626e-02,  1.99574884e-02, -1.61461276e-03,\n",
       "         -2.97081154e-02,  4.22127265e-03,  2.66239624e-02,  4.94334623e-02,\n",
       "          2.05534566e-02, -3.48114632e-02, -2.60123294e-02,  1.95790529e-02,\n",
       "         -7.73605797e-03,  3.11384592e-02, -2.66326554e-02,  2.69168001e-02,\n",
       "         -5.67671731e-02, -1.28162391e-02, -4.46875533e-03, -5.21283448e-02,\n",
       "         -7.25163962e-04,  7.16447830e-03,  2.95907874e-02,  1.40632400e-02,\n",
       "          6.99648308e-03,  8.41424093e-02,  2.69568665e-03, -9.61958058e-03,\n",
       "          1.43346032e-02, -8.03258717e-02, -3.40206623e-02, -6.73994720e-02,\n",
       "         -3.95807251e-02,  8.04383233e-02, -4.94571589e-02,  3.88035513e-02,\n",
       "          4.58880328e-04, -1.58581566e-02,  2.21852562e-03,  1.45498132e-02,\n",
       "          5.70148639e-02,  9.68829449e-03, -5.14133181e-03, -1.45569462e-02,\n",
       "         -4.29061800e-02, -4.44537923e-02, -8.20182078e-03, -2.78793126e-02,\n",
       "          7.43782893e-03, -4.62606875e-03, -5.18495701e-02,  1.97978821e-02,\n",
       "         -2.51510926e-02,  1.51161198e-02, -4.19261493e-02,  1.37363817e-03,\n",
       "         -1.69311594e-02,  5.53614832e-02,  1.64588913e-02, -4.15387651e-04,\n",
       "          6.27629738e-03,  2.48806961e-02, -6.16643094e-02, -1.91996340e-02,\n",
       "         -2.04876475e-02, -2.85844109e-03,  1.85368173e-02,  3.31107453e-02,\n",
       "         -8.54754914e-03, -2.68261936e-02, -3.09195854e-02,  8.45849235e-03,\n",
       "          4.91418056e-02, -4.51408736e-02,  2.94331554e-02, -2.81662066e-02,\n",
       "          2.11959388e-02, -9.17042699e-03, -6.72602877e-02,  2.84489468e-02,\n",
       "         -3.13467234e-02,  3.99460904e-02,  2.64804550e-02,  2.97268592e-02,\n",
       "          1.91584006e-02, -2.22857576e-02,  3.29697761e-03,  1.94239635e-02,\n",
       "         -8.79670978e-02, -1.07330680e-02, -5.44906408e-02,  3.26041989e-02,\n",
       "          5.32495491e-02, -1.35691254e-03, -4.11949046e-02,  4.22250591e-02,\n",
       "         -1.09478952e-02,  1.07590551e-03,  3.59446742e-03, -2.48024259e-02,\n",
       "          4.74376557e-03,  5.60014173e-02, -8.60236771e-03,  7.92235732e-02,\n",
       "         -3.12145930e-02, -4.48035188e-02, -3.18931043e-02,  1.08030383e-02,\n",
       "         -6.30989112e-03,  6.18609078e-02,  3.71696986e-02, -2.43481770e-02,\n",
       "         -6.20149123e-03, -3.73127274e-02, -3.75890061e-02,  5.15170321e-02,\n",
       "          1.59484055e-02,  1.20785879e-02, -1.38072595e-02,  4.98013869e-02,\n",
       "          2.44000647e-02,  1.35696204e-02, -2.86011547e-02, -8.18077102e-03,\n",
       "         -5.40838167e-02, -2.49751769e-02,  9.04046930e-03,  1.21981408e-02,\n",
       "         -1.34510535e-03, -1.80871431e-02, -3.71926315e-02, -5.20022353e-03,\n",
       "          1.60588138e-02, -3.42267416e-02, -4.75902520e-02,  1.28929159e-02,\n",
       "         -1.18393693e-02, -1.00223413e-02,  3.71310860e-02, -4.70971353e-02,\n",
       "         -5.28523475e-02,  1.30164186e-02,  7.36491894e-03, -3.10627883e-03,\n",
       "         -4.44685370e-02,  3.21095735e-02, -8.31700116e-03,  3.07565536e-02,\n",
       "          8.58310144e-04,  5.23868017e-02,  1.16656004e-02,  1.07213827e-02,\n",
       "         -5.76001517e-02,  4.48206291e-02,  4.69813757e-02,  3.00735384e-02,\n",
       "          3.88681469e-03, -5.58991767e-02,  7.84103759e-03,  3.51222157e-02,\n",
       "         -1.68907717e-02, -2.57041324e-02,  6.05397560e-02, -5.08814976e-02,\n",
       "         -7.05276802e-02,  1.78198982e-02,  8.17820430e-02,  5.08219488e-02,\n",
       "          2.78272536e-02,  4.77721961e-03, -3.23750041e-02,  2.55622640e-02,\n",
       "         -4.87665571e-02, -8.34113825e-03, -1.43591082e-02, -5.67522012e-02,\n",
       "          1.29669241e-03, -6.05805106e-02, -8.92972052e-02, -1.73927993e-02,\n",
       "         -3.60737368e-02, -4.47794944e-02, -1.11788632e-02,  2.41545811e-02,\n",
       "          7.46356919e-02,  8.85693263e-03, -1.12233509e-03, -7.59306876e-03,\n",
       "         -4.27744202e-02,  3.57141457e-02,  4.01946157e-02, -3.25134918e-02,\n",
       "          2.41268929e-02,  4.22061048e-03, -3.67014576e-03, -9.42326442e-04,\n",
       "         -1.65774077e-02,  2.03435495e-02, -5.81135005e-02, -4.12721224e-02,\n",
       "          2.05381680e-02, -1.16192766e-01, -3.10184471e-02,  5.54941632e-02,\n",
       "         -3.03450152e-02, -2.06953362e-02, -1.18285175e-02,  4.54343893e-02,\n",
       "          4.98881564e-02, -1.63459852e-02, -1.70850623e-02,  1.80994961e-02,\n",
       "          5.81542775e-03, -2.17243489e-02, -4.13147844e-02, -4.29771058e-02,\n",
       "         -2.21058484e-02,  1.42270233e-02, -9.22200829e-03,  1.34614659e-02,\n",
       "         -5.10171726e-02,  1.69055350e-02,  3.09450808e-03,  1.49519742e-02,\n",
       "         -4.01556939e-02, -2.96893939e-02, -5.51625565e-02,  4.96528335e-02,\n",
       "         -1.74562284e-03, -1.03273848e-02, -4.12092886e-05, -1.99439675e-02,\n",
       "         -3.63182016e-02,  3.42526399e-02, -2.40218081e-02,  6.29798621e-02,\n",
       "         -2.48934459e-02, -2.16582213e-02,  1.64288729e-02,  4.82706763e-02,\n",
       "          1.60333887e-02, -3.76100130e-02,  3.50288060e-33, -8.14437214e-03,\n",
       "         -3.93136591e-02,  4.80468087e-02, -5.37363347e-03, -4.34405021e-02,\n",
       "          5.73090184e-03, -4.86984244e-03,  5.58041930e-02,  3.74804102e-02,\n",
       "         -1.01594429e-03,  2.19864715e-02,  1.31108528e-02, -3.91476378e-02,\n",
       "         -1.45789441e-02, -9.97071713e-03,  6.21589879e-03,  6.70287311e-02,\n",
       "          2.18216442e-02, -2.31356844e-02, -1.76420044e-02, -1.59820728e-02,\n",
       "          2.57282853e-02, -7.12221712e-02,  9.28240269e-02,  3.37946676e-02,\n",
       "          4.94719297e-02,  3.24040800e-02, -9.77983978e-03, -5.53186284e-03,\n",
       "          1.92927960e-02,  4.99558561e-02, -1.07180048e-02,  5.04738390e-02,\n",
       "         -2.95083746e-02, -8.24012011e-02, -5.28510809e-02,  1.14670573e-02,\n",
       "          7.76309287e-03, -7.50559848e-03,  2.24402957e-02,  6.67531602e-03,\n",
       "         -1.04322210e-02,  2.37329118e-02, -1.04455510e-03, -1.71160419e-02,\n",
       "         -7.91347772e-03,  3.06671821e-02, -6.55862037e-03,  5.99694764e-03,\n",
       "          2.27004122e-02,  2.93761510e-02,  3.58161181e-02,  3.88727197e-03,\n",
       "         -4.92577590e-02, -3.31309512e-02,  3.70538943e-02,  1.13403507e-01,\n",
       "         -2.75202692e-02,  2.27861907e-02,  1.32560935e-02, -3.02154738e-02,\n",
       "          3.81047800e-02, -4.51491040e-04,  3.33072878e-02, -3.49871255e-02,\n",
       "         -1.91902518e-02,  1.86067373e-02,  4.04182784e-02, -1.90664511e-02,\n",
       "          9.53467656e-03, -2.76758615e-02, -3.86412703e-02, -3.49015333e-02,\n",
       "          4.40032519e-02, -2.64695496e-04, -3.86154018e-02, -3.15181911e-02,\n",
       "         -1.71059426e-02,  8.30384530e-03, -4.41317745e-02,  1.23307528e-02,\n",
       "         -3.22325118e-02,  8.35064575e-02,  3.44882533e-02,  1.74816307e-02,\n",
       "         -1.20601477e-02,  2.55146925e-03,  1.40947411e-02, -4.59141992e-02,\n",
       "          2.67697591e-02,  1.63081044e-03,  4.96926419e-02,  4.45239566e-05,\n",
       "          2.26916280e-03, -1.93310175e-02, -1.41054997e-02,  3.77459973e-02,\n",
       "          1.76122673e-02, -1.85742248e-02,  1.12018296e-02,  2.24717539e-02,\n",
       "         -6.19265065e-02,  3.88254076e-02, -1.10103087e-02, -2.36847755e-02,\n",
       "         -1.69518702e-02, -4.94416282e-02,  4.66037123e-03, -2.45641284e-02,\n",
       "         -5.87401986e-02,  2.20444184e-02,  7.61114731e-02,  1.39335906e-02,\n",
       "         -9.04331580e-02,  1.83715951e-02,  1.01613505e-02, -6.51092036e-03,\n",
       "         -7.33616389e-03,  2.07701828e-02,  1.71170607e-02,  3.02783865e-02,\n",
       "         -4.01274636e-02,  8.12938903e-03, -2.63523776e-02,  5.75201176e-02,\n",
       "          5.06135896e-02, -4.86635640e-02, -8.64346791e-03,  6.57578977e-03,\n",
       "         -1.16750866e-01, -8.05047527e-03,  9.02624521e-03,  2.58250590e-02,\n",
       "         -2.73525044e-02, -1.49254734e-02,  2.19630543e-02,  4.67513725e-02,\n",
       "          4.25763847e-03,  1.48514053e-02, -3.20418254e-02,  1.89162511e-02,\n",
       "         -1.94541942e-02,  4.36440296e-02,  1.22872954e-02, -9.65033541e-04,\n",
       "         -2.86573563e-02, -2.90443543e-02,  2.08959449e-02, -1.89503096e-02,\n",
       "         -2.43360437e-02,  3.91383134e-02, -5.72445628e-04,  2.18807757e-02,\n",
       "         -1.34506878e-02, -2.85163661e-03, -7.60212960e-03, -2.20226664e-02,\n",
       "         -7.94724301e-02, -6.79212883e-02,  3.56275998e-02, -1.67950820e-02,\n",
       "         -7.55426520e-03,  2.50734631e-02, -2.87288819e-02, -1.03240972e-02,\n",
       "          1.00938687e-02,  1.21582244e-02, -5.21329865e-02, -1.76515896e-02,\n",
       "          9.88720544e-03, -5.13891466e-02,  9.82344849e-04,  1.18674412e-02,\n",
       "          2.90852245e-02,  5.39714806e-02,  5.78324683e-02, -2.05328297e-02,\n",
       "          5.50549570e-03,  1.58285182e-02,  1.36606079e-02,  3.77720445e-02,\n",
       "          3.06229126e-02, -3.23891565e-02,  2.33235043e-02,  3.04178838e-02,\n",
       "         -2.78317332e-02, -5.69837317e-02,  7.83453807e-02, -5.00998460e-02,\n",
       "         -1.54031524e-02,  2.58350978e-03, -4.43784194e-03,  4.14472893e-02,\n",
       "          4.51975353e-02,  5.17807193e-02,  6.31339801e-03, -2.75715720e-04,\n",
       "          1.99109633e-02, -1.15158577e-02, -4.46597002e-02,  3.28720622e-02,\n",
       "          1.38285570e-02,  1.87702943e-03, -1.80628709e-02,  7.46080279e-02,\n",
       "          2.74462458e-02,  1.77724473e-02,  2.66611576e-02, -5.35235852e-02,\n",
       "          1.84911583e-02, -2.57936753e-02,  4.72819582e-02, -5.84852658e-02,\n",
       "         -1.19107906e-02, -3.05750985e-02, -2.48642359e-03,  1.58725921e-02,\n",
       "          3.77759635e-02,  7.67396018e-03, -3.69746312e-02,  3.64984572e-02,\n",
       "         -4.09580879e-02, -4.90873121e-02, -5.54395504e-02,  4.06296775e-02,\n",
       "          1.07046030e-01, -3.27994279e-03, -1.45570189e-02, -3.41563076e-02,\n",
       "          1.87488217e-02, -5.24260812e-02,  1.05829589e-01,  4.36867177e-02,\n",
       "         -1.11616300e-02, -1.14710405e-02, -1.07128220e-03, -8.15860345e-04,\n",
       "         -1.96978617e-02,  3.95518541e-02,  3.69122513e-02,  1.00290310e-02,\n",
       "          2.33742464e-02,  8.87764916e-02, -3.24309170e-02, -3.50292996e-02,\n",
       "         -3.92074883e-02,  2.29114387e-03,  3.49260047e-02, -3.90620455e-02,\n",
       "          7.13085458e-02,  3.59329507e-02,  2.57785544e-02,  4.84316759e-02,\n",
       "          5.25405211e-03,  9.46960784e-03, -3.20178159e-02,  5.33843711e-02,\n",
       "         -7.38447085e-02, -4.61352989e-03,  6.87746778e-02, -2.31308453e-02,\n",
       "         -1.42237707e-03, -1.08614820e-03,  2.34585125e-02, -1.23723364e-03,\n",
       "         -2.58185081e-02, -8.65325481e-02, -1.90304313e-02, -2.87794508e-02,\n",
       "          1.80567428e-03,  1.64364427e-02, -3.00386250e-02, -2.79632602e-02]),\n",
       "  'score': tensor(0.0688)},\n",
       " {'page_number': 217,\n",
       "  'sentence_chunk': 'What a year it has been!Hopefully your heads are all a little fuller than they were…you have the whole summer ahead to get them nice and empty before next year starts.…   “Now, as I understand it, the house cup here needs awarding, and the points stand thus: In fourth place, Gryffindor, with three hundred and twelve points; in third, Hufflepuff, with three hundred and fifty-two; Ravenclaw has four hundred and twenty-six and Slytherin, four hundred and seventy-two.”   A storm of cheering and stamping broke out from the Slytherin table. Harry could see Draco Malfoy banging his goblet on the table. It was a sickening sight.   “Yes, Yes, well done, Slytherin,” said Dumbledore. “However, recent events must be taken into account.”',\n",
       "  'chunk_char_count': 734,\n",
       "  'chunk_word_count': 126,\n",
       "  'chunk_token_count': 183.5,\n",
       "  'embedding': array([ 5.24632866e-03, -4.73456122e-02, -3.37183848e-02,  1.45656783e-02,\n",
       "          9.82149988e-02,  4.49371003e-02,  3.90321016e-02,  5.32365926e-02,\n",
       "          4.42242175e-02,  6.83259442e-02,  9.05877445e-03, -1.47385905e-02,\n",
       "         -1.06708414e-03, -3.59040573e-02, -2.33603455e-02,  4.95912209e-02,\n",
       "         -7.01195076e-02, -8.41827318e-02, -8.16758815e-03, -2.61179898e-02,\n",
       "          4.14396115e-02, -2.08528470e-02,  3.23022678e-02, -7.37921000e-02,\n",
       "         -2.53349263e-02, -2.13699359e-02, -4.26946394e-02,  3.71813625e-02,\n",
       "         -2.01686416e-02, -3.61740887e-02, -1.22838421e-02, -3.26767601e-02,\n",
       "          2.23843791e-02,  4.55918089e-02, -1.41199622e-02,  1.43746780e-02,\n",
       "         -1.78370718e-03, -3.76001783e-02,  2.97446065e-02, -6.53757070e-06,\n",
       "         -2.00980473e-02,  1.20108370e-02, -3.70572903e-03, -1.86504982e-03,\n",
       "         -2.24886257e-02, -5.06890491e-02, -5.34154512e-02, -4.44422662e-03,\n",
       "          2.37361751e-02,  7.63276406e-03,  6.52659610e-02, -1.30230850e-02,\n",
       "          1.55641604e-02,  9.82100517e-03, -4.91005462e-03,  4.19307128e-02,\n",
       "          1.09336143e-02,  6.88466206e-02, -4.70823869e-02, -8.36700276e-02,\n",
       "          7.93930590e-02, -4.07075770e-02, -3.91684026e-02,  4.84259278e-02,\n",
       "         -2.39514336e-02, -2.02121474e-02, -3.94319147e-02, -2.07814258e-02,\n",
       "         -2.92889122e-02,  1.18145691e-02,  3.92354652e-03, -1.90281700e-02,\n",
       "         -7.28644570e-03,  4.83509488e-02, -2.81935707e-02, -8.08428694e-03,\n",
       "          1.47110652e-02, -2.23028678e-02,  1.07976133e-02,  5.57204932e-02,\n",
       "         -2.65730154e-02, -7.21768476e-03, -1.44936973e-02, -5.94920367e-02,\n",
       "         -1.74086671e-02, -8.95407945e-02,  1.28515605e-02, -2.15782244e-02,\n",
       "          1.15804551e-02,  3.74895171e-03, -2.28431262e-02,  2.29935478e-02,\n",
       "          1.06006451e-02,  1.04546182e-01, -4.71313670e-02,  6.32849056e-03,\n",
       "         -2.71701571e-02,  1.04103319e-01, -2.81385947e-02,  2.66614463e-02,\n",
       "          1.24884332e-02, -1.79774035e-02, -8.05721059e-03, -4.20863293e-02,\n",
       "          1.04017127e-02, -1.52102588e-02, -1.58599503e-02, -1.85794849e-02,\n",
       "         -2.06839908e-02,  3.63327540e-03,  9.91250202e-03, -4.06825356e-02,\n",
       "         -3.37093547e-02,  1.88943557e-02, -3.51837426e-02,  2.03234772e-03,\n",
       "          2.11463664e-02, -1.23118171e-02,  1.13289542e-02, -9.72665921e-02,\n",
       "          1.49984658e-02, -1.14345727e-02,  6.61351578e-03,  3.20340469e-02,\n",
       "         -6.59577549e-02,  1.02151092e-02, -4.32939152e-04, -1.52933626e-02,\n",
       "          2.56634839e-02, -1.59936165e-03, -6.18053563e-02,  3.22478898e-02,\n",
       "          3.35305370e-02, -3.09885163e-02, -4.19204943e-02,  4.66920482e-03,\n",
       "          6.68183938e-02,  4.85110879e-02, -5.52374264e-03,  3.21143307e-02,\n",
       "          2.95083672e-02,  4.19700854e-02, -7.52884150e-03,  8.23720545e-03,\n",
       "         -2.47669406e-02,  1.40312640e-02,  1.50376959e-02,  6.98106289e-02,\n",
       "          3.60619603e-03,  6.21427372e-02,  1.16810175e-02, -4.57354300e-02,\n",
       "         -1.08061880e-02, -5.80940209e-02, -9.27289389e-03, -3.75801548e-02,\n",
       "          3.39808129e-02, -2.86153834e-02, -8.56820494e-03, -3.04580852e-02,\n",
       "         -1.26060098e-02,  1.02194026e-01, -2.09191795e-02, -2.64679184e-05,\n",
       "          5.88682247e-03, -2.96499673e-02,  1.37674725e-02,  3.23553793e-02,\n",
       "         -1.26664126e-02, -3.36963199e-02, -2.84636822e-02, -1.51156597e-02,\n",
       "          1.47134811e-02,  6.47978634e-02, -1.26974881e-02,  4.71328236e-02,\n",
       "         -6.76276833e-02,  1.06408624e-02, -1.20626716e-02,  7.50521710e-03,\n",
       "         -5.38171381e-02,  4.57812436e-02, -3.82480882e-02, -6.61530271e-02,\n",
       "          4.96344864e-02, -7.97758624e-02, -1.26993656e-02, -2.37407293e-02,\n",
       "         -7.55593553e-02,  1.86184850e-02, -5.39214583e-03, -1.13734091e-02,\n",
       "          5.54600805e-02,  3.72854583e-02,  3.10506043e-03,  2.79961154e-02,\n",
       "         -4.32559736e-02, -1.35697173e-02,  1.75219811e-02, -2.03169640e-02,\n",
       "         -4.81060781e-02, -5.29416353e-02, -4.48884554e-02, -4.64892611e-02,\n",
       "         -2.36274395e-02, -2.88116559e-02,  1.99323334e-02,  4.04249085e-03,\n",
       "         -5.26034497e-02,  8.25112686e-03, -1.14938458e-02, -1.77487545e-02,\n",
       "         -1.49397412e-02,  2.86470056e-02,  9.82061028e-03,  1.95246818e-03,\n",
       "          1.47648687e-02, -1.88138578e-02, -2.62156688e-02, -4.67901453e-02,\n",
       "          3.39678749e-02,  3.24158706e-02, -6.02213629e-02, -3.12254336e-02,\n",
       "          2.41888501e-02, -8.90526772e-02,  3.27379303e-03,  9.36373230e-03,\n",
       "          1.82786386e-03,  4.91886362e-02, -5.79419099e-02,  5.91791868e-02,\n",
       "         -8.65448341e-02, -6.56170473e-02,  4.23689559e-02, -7.11752400e-02,\n",
       "         -3.54672484e-02,  1.77713148e-02,  1.01855155e-02,  1.77596305e-02,\n",
       "         -3.59086245e-02,  4.10757810e-02,  2.62422934e-02,  2.01267004e-02,\n",
       "         -1.58249959e-03, -5.75580373e-02,  1.99436229e-02, -1.03036105e-03,\n",
       "          4.00013663e-02, -5.23495488e-02,  9.44875181e-03,  2.22206339e-02,\n",
       "         -1.48190353e-02, -4.04741913e-02, -1.72358286e-02, -2.17765626e-02,\n",
       "          1.74225401e-02,  3.20276842e-02, -5.80655038e-02,  2.05078684e-02,\n",
       "          5.22528663e-02,  1.52798342e-02,  2.37561632e-02, -6.56120181e-02,\n",
       "         -4.19105850e-02,  4.13447917e-02, -3.67816836e-02,  3.11633106e-02,\n",
       "          2.44053584e-02, -5.04107140e-02, -3.01217847e-03,  1.32224169e-02,\n",
       "          2.50565689e-02,  1.04603474e-03,  1.75161804e-05,  9.73716099e-03,\n",
       "         -1.81372426e-02,  1.01517342e-01, -2.54244767e-02, -8.03668797e-03,\n",
       "          2.95029636e-02, -9.79734808e-02, -1.01985663e-01, -3.13565582e-02,\n",
       "          2.54720040e-02,  1.64341982e-02,  2.89898999e-02, -1.21378638e-02,\n",
       "          3.60337086e-02, -3.54939885e-02, -1.77382659e-02,  5.30440398e-02,\n",
       "          6.12204969e-02,  3.31156701e-02,  1.23695834e-02,  1.26332054e-02,\n",
       "         -3.02458182e-02, -4.18754704e-02, -6.89325808e-03, -2.47419681e-02,\n",
       "         -2.23332345e-02, -3.77276614e-02, -3.48706096e-02,  6.38011768e-02,\n",
       "          1.30226286e-02, -1.39692770e-02, -5.50542809e-02,  4.45783176e-02,\n",
       "         -3.85132851e-03,  5.10489121e-02, -5.69173368e-03,  3.77168953e-02,\n",
       "         -2.85139214e-02,  3.35302129e-02, -5.74068092e-02,  3.21878754e-02,\n",
       "         -1.38160773e-02, -3.33911590e-02,  7.13643059e-02, -7.94604421e-03,\n",
       "         -6.00927160e-04,  8.50736909e-03,  2.84313709e-02,  1.38654569e-02,\n",
       "          2.70479005e-02, -4.69576158e-02,  1.29858535e-02, -4.41223048e-02,\n",
       "         -4.33252454e-02,  4.88385260e-02, -7.76163340e-02, -1.58082787e-02,\n",
       "         -2.48378087e-02, -7.00394437e-03, -5.12267277e-02,  6.36680936e-03,\n",
       "          2.55248770e-02,  1.04887122e-02,  5.57570066e-03,  4.45938557e-02,\n",
       "         -4.58312854e-02,  6.59716176e-03, -1.11488746e-02,  6.02265224e-02,\n",
       "          9.04525891e-02,  1.26395887e-02, -1.19016021e-02,  1.39193714e-03,\n",
       "         -1.10875666e-02, -6.67845656e-04,  7.57602835e-03, -1.53297307e-02,\n",
       "          1.30207995e-02,  5.80539145e-02, -5.64298034e-02,  5.00975959e-02,\n",
       "          1.94999482e-02, -3.09069995e-02, -2.56668888e-02,  2.29988564e-02,\n",
       "          2.86313910e-02,  5.37774246e-03,  2.68791541e-02, -3.82339931e-05,\n",
       "         -3.17131504e-02, -1.17142955e-02,  7.40254205e-03,  8.84332880e-02,\n",
       "         -2.55825697e-03, -9.83504113e-04, -3.93460616e-02,  4.49499786e-02,\n",
       "          1.70854162e-02,  2.16101874e-02,  1.42823327e-02,  1.34404283e-02,\n",
       "         -2.91716028e-02, -6.29841408e-04,  3.86060067e-02, -4.55922959e-03,\n",
       "          3.83285619e-02,  2.36801021e-02, -1.96308754e-02, -3.22160162e-02,\n",
       "          3.50972675e-02, -1.66065656e-02, -3.38775502e-03, -6.10098504e-02,\n",
       "          1.48165952e-02, -4.50200662e-02,  2.28388458e-02, -7.00559001e-03,\n",
       "         -3.01269516e-02, -1.82508416e-02,  2.94006914e-02,  7.12621026e-03,\n",
       "         -2.22593583e-02, -3.74314701e-03,  1.70612670e-02,  1.76236462e-02,\n",
       "         -2.97458638e-02,  2.93095922e-03,  8.55979603e-03, -5.96362427e-02,\n",
       "         -4.41003665e-02, -1.61893887e-03,  2.70251073e-02, -2.36996897e-02,\n",
       "         -2.74638757e-02, -8.34584236e-02, -1.61578823e-02,  6.98078331e-03,\n",
       "         -3.38396579e-02, -5.85139506e-02,  7.18751326e-02, -2.68501732e-02,\n",
       "         -4.44598868e-02, -1.20116044e-02,  2.38842564e-03,  9.97874234e-03,\n",
       "         -2.24516522e-02,  1.72627589e-03, -1.12563008e-02,  8.82313102e-02,\n",
       "         -7.24567240e-03,  3.68557358e-03,  1.70206986e-02, -4.32828031e-02,\n",
       "          4.88312729e-02, -2.03252211e-02, -2.12972313e-02,  1.34999771e-02,\n",
       "          3.74009907e-02,  3.39457244e-02, -9.45391413e-03,  1.77680254e-02,\n",
       "          5.03710359e-02,  3.48402262e-02,  4.05964591e-02, -2.09518597e-02,\n",
       "         -9.84679386e-02,  3.74720879e-02,  2.66182628e-02, -4.36844416e-02,\n",
       "          2.60474663e-02,  6.94697350e-03,  2.53662951e-02,  1.31550580e-02,\n",
       "         -2.39379369e-02, -3.26137282e-02, -5.36789559e-02, -2.22978890e-02,\n",
       "          1.35039417e-02, -9.66457278e-02, -2.15688888e-02,  5.67533299e-02,\n",
       "         -3.26475166e-02, -7.29360664e-03, -1.53833013e-02, -5.08509064e-03,\n",
       "          2.91798152e-02,  9.89759527e-03,  1.35286506e-02,  1.16601316e-02,\n",
       "          1.71994250e-02,  3.26348208e-02,  9.90229007e-03, -6.51549129e-03,\n",
       "         -9.74558387e-03, -1.76136079e-03,  2.46574655e-02,  5.04456349e-02,\n",
       "         -1.24543579e-02,  3.67585868e-02, -2.84300884e-03,  2.24112235e-02,\n",
       "         -6.86776340e-02, -2.37847883e-02, -4.00279509e-03,  3.29907835e-02,\n",
       "          1.43202916e-02,  1.12216510e-02,  4.57005650e-02, -2.60813087e-02,\n",
       "         -9.56418272e-03,  2.42277179e-02, -1.94092859e-02,  1.01887517e-01,\n",
       "         -6.17622882e-02, -1.38003165e-02,  5.51808113e-03, -3.82947698e-02,\n",
       "          1.60067081e-02, -4.89221793e-03,  4.05441226e-33,  2.68966276e-02,\n",
       "          3.37269083e-02,  6.93208128e-02,  2.51916088e-02, -2.03979015e-02,\n",
       "         -3.86362220e-03, -4.88798879e-03,  6.07982762e-02,  8.52618553e-03,\n",
       "          2.41074413e-02,  8.81254382e-04, -3.81566919e-02,  3.43389995e-03,\n",
       "          1.73930568e-03, -8.79547745e-03,  5.93674276e-03,  2.50067525e-02,\n",
       "          1.81864854e-02, -1.17065653e-03,  2.77204551e-02,  3.29605080e-02,\n",
       "         -4.89213616e-02, -4.72288653e-02, -4.32484746e-02,  3.72258984e-02,\n",
       "         -9.34025273e-03,  1.97518785e-02, -2.36792136e-02,  3.04168351e-02,\n",
       "          9.90901235e-03,  3.78063545e-02, -1.21337045e-02,  8.76602717e-03,\n",
       "         -5.00231199e-02, -9.12485272e-02,  1.76736768e-02, -5.57482126e-04,\n",
       "          6.66736662e-02, -2.76636872e-02,  5.94958439e-02, -3.09649669e-02,\n",
       "         -1.74093358e-02, -2.18229033e-02,  1.16765182e-02, -5.27504273e-02,\n",
       "         -9.00818482e-02,  1.09840464e-02,  2.05397122e-02,  2.94317547e-02,\n",
       "          1.18010826e-02, -3.72733339e-03,  7.76640847e-02,  6.43285289e-02,\n",
       "         -3.51142026e-02, -5.34815378e-02, -5.47774211e-02,  7.37036467e-02,\n",
       "         -4.16058227e-02,  5.34636341e-02,  8.12765881e-02, -4.15298017e-03,\n",
       "          3.62022966e-02, -3.17159742e-02,  1.36069627e-02,  1.39823444e-02,\n",
       "         -7.67308986e-04, -3.96247907e-03,  1.69418007e-02, -9.16981045e-03,\n",
       "         -3.46548250e-03,  3.16349156e-02, -6.34428486e-02, -4.90522571e-02,\n",
       "          1.81758776e-02,  5.68840979e-03, -2.40295343e-02, -2.64030937e-02,\n",
       "         -1.66919571e-03,  6.06816337e-02, -2.02019792e-02, -1.61264986e-02,\n",
       "         -1.27487965e-02,  7.28009343e-02,  1.12522505e-02, -2.90714242e-02,\n",
       "         -1.55174974e-02, -1.81531627e-02,  4.40129898e-02, -2.19860300e-02,\n",
       "          6.51115738e-03, -4.03523035e-02,  9.04006734e-02, -5.29419258e-02,\n",
       "          3.90361086e-03, -3.12476698e-02,  9.13972408e-03,  7.72442250e-03,\n",
       "         -2.28797905e-02, -2.34686341e-02,  8.51966720e-03,  8.04495066e-03,\n",
       "         -2.38140244e-02, -1.45096444e-02, -3.44549641e-02,  1.10261245e-02,\n",
       "          2.76201610e-02,  4.08043386e-03, -2.62815342e-03,  2.04281546e-02,\n",
       "         -2.08867919e-02,  5.61045334e-02,  3.95378321e-02,  2.74326317e-02,\n",
       "         -4.07393575e-02,  4.91860397e-02,  6.13907911e-03, -2.06747539e-02,\n",
       "         -1.25214560e-02,  2.90307775e-02,  2.66384874e-02, -1.81717072e-02,\n",
       "         -4.03213874e-02, -5.63178621e-02, -9.94854141e-03,  5.20795174e-02,\n",
       "          4.12911102e-02,  3.09805758e-02,  2.29642913e-02, -4.78366651e-02,\n",
       "         -5.44557311e-02, -3.07678129e-03,  4.36086617e-02,  1.97043195e-02,\n",
       "         -4.61610667e-02, -2.25241315e-02,  1.67422090e-02,  3.06445220e-03,\n",
       "         -9.76109132e-03, -1.08252810e-02, -2.45138048e-03,  6.15758169e-03,\n",
       "          3.92943202e-03,  3.54582481e-02,  3.53123061e-03, -3.66976187e-02,\n",
       "         -5.89271123e-03,  1.05921831e-02,  1.22445850e-02, -2.71407906e-02,\n",
       "         -1.58498064e-02,  4.09443565e-02, -2.06852406e-02, -6.14328608e-02,\n",
       "         -1.64124183e-02, -1.25955362e-02,  3.21790087e-03,  7.70193664e-03,\n",
       "         -1.18075144e-02, -4.55640331e-02,  5.05511276e-02,  4.06786911e-02,\n",
       "          1.87471155e-02,  1.74404420e-02, -6.82759061e-02,  4.38139727e-03,\n",
       "          3.92283499e-02,  3.02120689e-02, -4.06643264e-02,  3.83921862e-02,\n",
       "         -5.46871088e-02, -1.08791413e-02,  7.65949348e-03, -4.42367084e-02,\n",
       "         -2.50785681e-03, -1.23302694e-02,  4.05815057e-02, -5.29599972e-02,\n",
       "         -1.09869633e-02,  5.22347428e-02, -1.02319568e-01,  2.71132831e-02,\n",
       "          9.83824581e-03, -1.56618934e-02,  3.06962170e-02, -8.16864427e-03,\n",
       "          2.93579046e-03,  5.64250746e-04,  4.25589085e-02, -7.04018474e-02,\n",
       "         -2.60101035e-02, -8.24935734e-03,  2.19996367e-02,  1.74295455e-02,\n",
       "         -4.17449065e-02,  4.94763292e-02,  1.38373708e-03,  7.77743116e-04,\n",
       "          3.40461880e-02, -3.31183001e-02, -5.98454811e-02,  7.74384290e-02,\n",
       "         -4.03147144e-03,  4.38125106e-03,  1.19503569e-02, -1.99807086e-03,\n",
       "         -9.56648402e-03, -5.08131832e-02, -4.55548801e-02, -3.37103270e-02,\n",
       "          2.23725438e-02, -1.11348750e-02,  4.94824946e-02, -7.83180818e-02,\n",
       "          5.24035795e-03, -3.10110934e-02,  2.45656352e-03,  2.92734560e-02,\n",
       "          5.26446924e-02, -1.39380572e-02,  1.92199275e-03,  1.65060833e-02,\n",
       "         -3.84869054e-03, -4.32437025e-02,  5.22275306e-02,  5.56007363e-02,\n",
       "          4.37951721e-02,  7.68107967e-03,  3.62138613e-04, -2.16119550e-02,\n",
       "         -5.28807938e-02, -4.47909497e-02,  5.75062707e-02,  9.42231435e-03,\n",
       "         -6.02565035e-02, -3.43563035e-02,  3.54776904e-02, -2.80236118e-02,\n",
       "         -1.07782185e-02,  3.62917446e-02, -2.65318528e-02,  3.09962910e-02,\n",
       "         -5.36484737e-03,  3.18127126e-02, -2.97265723e-02, -5.74623384e-02,\n",
       "         -4.33974378e-02, -3.41455378e-02,  4.97195274e-02, -4.18324838e-04,\n",
       "          7.04385573e-03, -1.76132917e-02,  1.70795489e-02, -7.16335548e-04,\n",
       "         -9.19662509e-03,  2.58607734e-02, -2.92702001e-02,  2.69186194e-03,\n",
       "         -1.01555185e-02,  9.15005710e-03,  7.71418065e-02, -4.67172042e-02,\n",
       "         -4.52342862e-03,  1.49586089e-02,  4.83917631e-02, -1.57090388e-02,\n",
       "          1.50729194e-02, -3.48773338e-02,  2.67403759e-02,  2.20689061e-03,\n",
       "         -2.23857835e-02,  4.81258817e-02, -5.77281676e-02,  1.11151878e-02]),\n",
       "  'score': tensor(0.0683)}]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = random.choice(query_list)\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Answer query with context and return context \n",
    "answer, context_items = ask(query=query, \n",
    "                            temperature=0.3,\n",
    "                            max_new_tokens=512,\n",
    "                            return_answer_only=False)\n",
    "\n",
    "print(f\"Answer:\\n\")\n",
    "print_wrapped(answer)\n",
    "print(f\"Context items:\")\n",
    "context_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what was in the letter for harry potter?\n",
      "[INFO] Time taken to get scores on 359 embeddings: 0.00031 seconds.\n",
      "Answer:\n",
      "\n",
      "The letter contained a wooden flute, which Harry blew a bit like an owl.\n",
      "Context items:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': 143,\n",
       "  'sentence_chunk': 'Will you look at this?I’ve got some presents!”   “What did you expect, turnips?”said Ron, turning to his own pile, which was a lot bigger than Harry’s.   Harry picked up the top parcel. It was wrapped in thick brown paper and scrawled across it was To Harry, from Hagrid. Inside was a roughly cut wooden flute. Hagrid had obviously whittled it himself. Harry blew it — it sounded a bit like an owl.   A second, very small parcel contained a note.',\n",
       "  'chunk_char_count': 446,\n",
       "  'chunk_word_count': 86,\n",
       "  'chunk_token_count': 111.5,\n",
       "  'embedding': array([ 1.92342733e-03, -3.37155275e-02, -3.21562886e-02, -3.06027811e-02,\n",
       "          6.03404976e-02,  5.47559932e-02, -1.30901085e-02,  5.15042730e-02,\n",
       "          4.32562418e-02,  3.15887369e-02, -1.95240062e-02,  1.11879818e-02,\n",
       "          4.67677321e-03, -1.54830134e-02, -3.73718217e-02, -3.19509432e-02,\n",
       "         -3.57455835e-02, -3.72632630e-02, -7.76291713e-02, -6.33546710e-02,\n",
       "          1.95182171e-02, -4.67826752e-03,  3.31620462e-02, -1.39670854e-04,\n",
       "          4.68184706e-03, -2.58074161e-02,  3.24093290e-02,  4.82865423e-03,\n",
       "         -6.32143170e-02, -6.84347153e-02, -1.11515839e-02, -5.31965904e-02,\n",
       "          3.68564054e-02,  4.35421020e-02, -1.65216904e-02, -9.13110003e-03,\n",
       "         -1.16236601e-03,  3.21801528e-02,  4.80355583e-02, -5.04625998e-02,\n",
       "         -6.16154782e-02, -8.57182872e-03, -3.27176899e-02, -6.83665130e-05,\n",
       "         -3.82564850e-02, -2.57789176e-02, -2.30311248e-02,  1.64924078e-02,\n",
       "          2.16661021e-02, -1.33583238e-02,  2.09161360e-02,  5.61848208e-02,\n",
       "         -5.51583916e-02,  2.06833463e-02,  4.71426873e-03,  3.98719609e-02,\n",
       "          6.88914955e-03,  1.50047541e-02, -3.23326923e-02, -6.42563496e-03,\n",
       "          6.94874376e-02, -4.86650644e-03, -2.86823255e-03, -5.88656440e-02,\n",
       "         -4.24003089e-03, -3.50678153e-02, -2.02071574e-02, -1.13285379e-02,\n",
       "         -7.94300735e-02,  8.21054820e-03, -8.57037771e-03, -1.52248275e-02,\n",
       "         -2.61310837e-03,  3.86705250e-02, -1.01877525e-02,  7.03619495e-02,\n",
       "          2.61394065e-02, -2.59695724e-02,  1.89894345e-02,  4.83105704e-02,\n",
       "          2.49332469e-02,  5.67357056e-02, -3.40469331e-02,  3.98717895e-02,\n",
       "         -7.55377579e-04,  4.73107956e-02,  4.40775901e-02, -5.21701872e-02,\n",
       "          2.96170004e-02, -1.13452068e-02,  4.84985411e-02,  8.13998356e-02,\n",
       "         -7.71077408e-04,  1.12472638e-01, -3.19922641e-02,  1.84810385e-02,\n",
       "         -7.51814097e-02,  3.72510180e-02, -1.60430726e-02, -1.42859146e-02,\n",
       "          6.53896779e-02, -1.91798918e-02, -1.14319278e-02, -5.00672087e-02,\n",
       "          1.80392433e-03, -4.89651710e-02,  4.54899296e-02, -1.46634420e-02,\n",
       "         -2.50819442e-03, -7.17246393e-03,  4.39596064e-02, -1.53397154e-02,\n",
       "          9.27659590e-03,  2.42834762e-02,  3.72333475e-03, -3.46433856e-02,\n",
       "         -2.23862473e-02, -2.72308104e-02,  1.98416319e-02,  6.75867638e-03,\n",
       "          3.51291639e-03, -2.14340836e-02,  2.14469098e-02,  5.77277727e-02,\n",
       "          1.51085677e-02,  4.34239507e-02,  6.82613924e-02,  6.88343029e-03,\n",
       "         -3.80968899e-02, -2.20639762e-02, -2.86351033e-02, -8.70191399e-03,\n",
       "          3.97427902e-02, -7.50883073e-02, -3.37314643e-02, -9.71496280e-04,\n",
       "          1.31259514e-02,  1.88339017e-02,  3.22574843e-03,  9.37547535e-03,\n",
       "         -2.24098423e-03,  3.85604613e-02, -2.19322182e-02,  6.04615966e-03,\n",
       "         -7.90018216e-03,  1.12826498e-02,  1.35949450e-02,  3.86400968e-02,\n",
       "          2.48628482e-03,  8.83143246e-02,  2.50515565e-02,  1.98285077e-02,\n",
       "         -1.96871255e-02, -6.70341030e-03,  6.86791958e-03,  3.37893292e-02,\n",
       "         -2.50836145e-02, -3.06139793e-02,  4.57112752e-02,  8.79738554e-02,\n",
       "          3.50884758e-02,  4.80562623e-04,  9.96593107e-03,  3.98389064e-03,\n",
       "         -2.12019179e-02, -9.21682734e-03, -1.70369800e-02,  1.04401065e-02,\n",
       "          4.32534516e-02, -7.01112254e-03, -1.48109160e-02,  3.04606045e-03,\n",
       "          2.63570603e-02,  1.34080760e-02, -2.87740380e-02, -1.90415792e-02,\n",
       "         -3.58524509e-02, -6.68780282e-02,  1.62398014e-02,  3.08536459e-03,\n",
       "         -6.39602095e-02,  3.33090350e-02,  8.19671713e-03, -1.40711125e-02,\n",
       "          2.67112944e-02, -9.93054640e-03, -1.52384527e-02, -6.01568557e-02,\n",
       "         -5.05804233e-02, -1.53675834e-02, -6.90695569e-02,  2.95201596e-02,\n",
       "          4.92604449e-02,  5.52339293e-02,  3.37055475e-02, -7.85204489e-03,\n",
       "         -7.68545344e-02, -1.86197441e-02,  2.87916232e-02, -3.16058397e-02,\n",
       "          3.88476881e-03, -4.62833568e-02, -3.51482593e-02, -5.58555573e-02,\n",
       "          5.86900208e-03, -1.39827048e-02, -5.67046646e-03,  3.45523693e-02,\n",
       "         -4.16404381e-02,  1.68449599e-02, -2.31506340e-02, -3.92400585e-02,\n",
       "          1.28047755e-02, -3.34722223e-03,  8.08882192e-02,  3.02454643e-02,\n",
       "         -8.24056473e-03, -2.01280024e-02, -2.37562824e-02, -8.05318914e-03,\n",
       "         -9.23254807e-03,  2.24030903e-03, -2.09477148e-03,  2.44658701e-02,\n",
       "          8.74127634e-03, -6.02082200e-02, -2.94695199e-02, -1.15843946e-02,\n",
       "         -1.65723264e-02,  3.93191725e-02, -7.64565833e-04, -4.03095931e-02,\n",
       "          2.62807477e-02, -2.68797688e-02, -5.19366935e-02, -5.61521463e-02,\n",
       "         -4.69568670e-02,  1.03012659e-03,  5.86721934e-02, -1.88155158e-03,\n",
       "          2.46738736e-02, -1.10175065e-03,  2.01127175e-02, -4.83006202e-02,\n",
       "         -1.19135762e-02, -7.58297145e-02,  1.70111619e-02,  1.95997953e-02,\n",
       "          5.64759672e-02, -4.07857187e-02,  2.47343518e-02, -2.62779258e-02,\n",
       "         -2.08430067e-02, -1.14540365e-02, -2.12914068e-02, -9.25463345e-03,\n",
       "          4.12668893e-03,  1.30700357e-02,  2.49604248e-02,  6.85130656e-02,\n",
       "          1.42628374e-02, -7.60198683e-02, -1.12826554e-02, -2.40339581e-02,\n",
       "         -8.78181960e-03, -9.30114277e-03, -2.31241751e-02,  3.05341417e-03,\n",
       "          2.95613171e-03, -3.65162976e-02,  2.74925325e-02, -3.31568457e-02,\n",
       "         -2.30141520e-03,  6.85267001e-02,  2.41915789e-02,  1.60416495e-02,\n",
       "         -1.66763421e-02,  4.46741208e-02, -1.94682255e-02, -2.47564744e-02,\n",
       "          1.59100890e-02, -6.51210546e-02,  5.21957874e-03, -1.38619812e-02,\n",
       "         -2.70691290e-02,  5.79596534e-02, -6.54101139e-03,  7.22922683e-02,\n",
       "         -1.23003535e-02,  8.24478455e-03, -2.02882737e-02, -2.58916203e-04,\n",
       "          4.49966379e-02,  5.47681190e-02,  5.52657247e-03,  5.70065752e-02,\n",
       "         -5.31579787e-03, -7.17524216e-02,  1.19888119e-03, -6.68281736e-03,\n",
       "         -1.86087061e-02, -2.85243057e-02, -2.12354679e-02,  3.46290395e-02,\n",
       "         -1.30173415e-02,  2.00888459e-02, -2.90886033e-02,  4.46781563e-03,\n",
       "         -4.27510925e-02,  6.95282519e-02,  2.41269507e-02, -7.81548850e-04,\n",
       "         -2.80671846e-02,  5.95544949e-02, -5.79261780e-02, -1.49129117e-02,\n",
       "         -3.52793885e-03, -5.45000210e-02,  3.52086835e-02, -4.87672649e-02,\n",
       "          9.78230592e-03, -2.26071458e-02, -5.32746427e-02,  2.29006261e-02,\n",
       "          3.13840341e-03, -9.43806116e-03,  4.61697057e-02,  4.41321172e-02,\n",
       "         -1.55864423e-02,  4.39846963e-02, -9.04744640e-02,  3.42591666e-02,\n",
       "         -5.90898097e-02,  6.23947009e-03,  4.65648286e-02, -4.81246114e-02,\n",
       "          2.73740459e-02, -2.87229773e-02,  1.17082773e-02,  3.73406112e-02,\n",
       "          1.04643023e-02,  1.36492997e-02, -4.55782935e-02, -2.17199046e-02,\n",
       "          6.67060632e-03, -5.61831519e-03, -6.88591227e-02,  2.22817864e-02,\n",
       "          2.11314950e-02,  8.37249309e-03,  2.36812066e-02,  2.33182218e-02,\n",
       "          2.45503522e-02,  3.45012397e-02, -5.28796017e-02, -4.70333034e-03,\n",
       "         -6.68836338e-03, -5.74869476e-02, -1.43702300e-02, -1.26906009e-02,\n",
       "         -4.45915647e-02,  7.88424835e-02,  2.72866692e-02, -2.09105797e-02,\n",
       "         -3.27760018e-02, -6.33802786e-02, -5.05515886e-03,  5.55428788e-02,\n",
       "         -3.05521389e-04,  3.35614234e-02,  1.13765486e-02,  4.52504717e-02,\n",
       "          2.34544594e-02,  1.74138024e-02,  1.22326706e-02,  2.84910370e-02,\n",
       "         -5.00635207e-02,  3.18443612e-03,  5.08826226e-02, -2.10891524e-03,\n",
       "         -4.19365882e-04, -4.21928875e-02,  1.69643890e-02,  6.80696731e-03,\n",
       "          7.40369409e-03,  2.12867167e-02, -3.42381261e-02, -2.29334105e-02,\n",
       "          1.31540010e-02,  2.60977447e-03, -1.45495618e-02,  1.23244124e-02,\n",
       "         -7.26062283e-02, -1.76300928e-02, -1.50203344e-03,  4.16255929e-03,\n",
       "         -1.86292026e-02, -5.11062006e-03, -5.70537942e-03, -1.20133786e-02,\n",
       "          3.90566303e-03,  4.06785533e-02,  1.44868698e-02, -1.49179092e-02,\n",
       "         -2.97065824e-02,  6.18551113e-02, -6.79427832e-02,  3.01912781e-02,\n",
       "         -5.14935665e-02, -8.50546360e-02,  5.15512228e-02,  4.98302653e-02,\n",
       "          5.84895760e-02, -1.03048809e-01,  5.00996597e-02, -4.73059379e-02,\n",
       "         -9.82896890e-03, -5.60472766e-03, -7.86194950e-03,  2.59024724e-02,\n",
       "          9.91339213e-04, -9.11502074e-03, -3.27614620e-02,  1.86137147e-02,\n",
       "          2.02685706e-02, -2.75373310e-02,  1.95998251e-02, -8.92862529e-02,\n",
       "         -1.54606076e-02, -4.31335978e-02, -9.66128930e-02, -6.26619533e-02,\n",
       "          5.66503685e-03, -6.02588318e-02, -2.42010131e-02, -1.86442933e-03,\n",
       "          2.74441279e-02, -1.00006917e-02,  1.64175592e-02, -2.80231722e-02,\n",
       "         -9.29607358e-03,  1.90210231e-02, -5.87724440e-04, -5.36618456e-02,\n",
       "         -6.64610267e-02,  5.29408781e-03, -2.06504520e-02, -3.14344764e-02,\n",
       "         -3.11842747e-02, -3.18986438e-02, -3.70479040e-02, -4.61757779e-02,\n",
       "          3.41229215e-02, -9.82581824e-02,  1.59247350e-02,  4.33259122e-02,\n",
       "          7.63505744e-03, -3.39168422e-02,  3.71456146e-02,  1.88959576e-03,\n",
       "          4.86371592e-02, -2.93163676e-02,  8.83179810e-03,  5.91874495e-03,\n",
       "          5.90863153e-02,  2.72672251e-02, -2.24142987e-02, -6.68971939e-03,\n",
       "         -1.03284176e-02,  1.29928002e-02,  2.53890771e-02,  4.03951481e-02,\n",
       "         -3.53492312e-02,  1.98211148e-02,  6.60068545e-05,  2.60355906e-03,\n",
       "         -2.80257687e-02,  6.51483536e-02, -5.65786846e-02,  1.47143900e-02,\n",
       "         -2.09112037e-02,  3.69327925e-02,  1.30598610e-02, -2.64748354e-02,\n",
       "         -4.03587185e-02,  3.14405598e-02,  4.44106758e-03,  6.57915175e-02,\n",
       "         -2.88436338e-02, -4.21852153e-03, -8.46892782e-03,  1.96680706e-02,\n",
       "         -6.74535483e-02,  2.06843782e-02,  3.53454180e-33, -8.28145910e-03,\n",
       "          5.41013572e-03, -1.08720008e-02,  3.65187079e-02, -2.43821368e-02,\n",
       "         -1.81663735e-03, -1.47048393e-02,  4.80682440e-02,  5.50018139e-02,\n",
       "         -3.08200298e-03, -8.66974983e-03,  1.27365077e-02, -4.15234864e-02,\n",
       "          3.20236720e-02, -1.85563937e-02,  6.89837188e-02,  5.97831197e-02,\n",
       "         -4.96619521e-03,  2.84470664e-03, -2.93773189e-02, -1.95081451e-03,\n",
       "          1.19021237e-02,  3.66541892e-02,  8.20083618e-02,  3.52794267e-02,\n",
       "         -1.05316630e-02, -4.40191887e-02, -5.01274616e-02, -1.76549666e-02,\n",
       "          2.09802482e-02,  2.30371114e-03, -2.67499220e-03,  7.26442188e-02,\n",
       "          1.64316613e-02, -5.53464331e-02, -3.11586820e-02, -9.68323834e-03,\n",
       "         -1.73732955e-02,  7.74522638e-03,  3.15347761e-02, -2.80100089e-02,\n",
       "          2.40323506e-02, -2.76021063e-02, -3.25748809e-02, -2.04843078e-02,\n",
       "         -4.69249189e-02,  2.61861049e-02, -1.95076130e-02, -5.07843820e-03,\n",
       "         -3.84279178e-04,  4.44192663e-02,  6.57925755e-02,  3.37152146e-02,\n",
       "         -3.37502845e-02, -2.77997851e-02,  4.81441356e-02,  2.18243804e-02,\n",
       "         -5.17555140e-02,  1.00643402e-02, -2.20616851e-02,  1.47574712e-02,\n",
       "          4.57097962e-02,  4.64556441e-02,  1.44159012e-02, -1.92928612e-02,\n",
       "         -7.36484677e-03,  2.89194472e-02,  2.06493232e-02,  9.79306549e-03,\n",
       "          1.52516656e-03,  8.17489065e-03,  5.65011706e-03, -7.21264072e-03,\n",
       "          2.97287479e-02,  3.47958249e-03, -1.66390780e-02,  9.50431358e-03,\n",
       "          2.59088259e-02,  6.67050034e-02, -3.05941701e-02,  2.56985910e-02,\n",
       "         -4.33662198e-02,  4.86001372e-02,  3.03195938e-02,  1.73266884e-02,\n",
       "          2.07357779e-02,  2.98991818e-02, -5.32282237e-03, -9.65821743e-03,\n",
       "          1.72777828e-02, -2.91573238e-02,  8.42033420e-03, -6.81802779e-02,\n",
       "          1.61093893e-03, -3.96309569e-02,  4.06640256e-03, -1.25408848e-03,\n",
       "         -4.16623652e-02,  1.33887343e-02, -2.49677896e-03,  2.55846009e-02,\n",
       "         -3.67432535e-02,  1.14496546e-02, -5.88496141e-02,  6.14078082e-02,\n",
       "         -2.33883392e-02, -5.43742701e-02,  2.41891872e-02,  2.20635403e-02,\n",
       "         -1.88746918e-02, -4.87602241e-02,  7.36937150e-02,  1.55849969e-02,\n",
       "         -4.14414816e-02,  3.66683044e-02,  4.45393138e-02,  7.77430180e-03,\n",
       "         -9.76726506e-03,  2.17751358e-02,  5.78268198e-03,  1.57893784e-02,\n",
       "         -2.44081616e-02, -2.63633626e-03, -6.08604997e-02,  1.38524501e-03,\n",
       "          5.68793677e-02, -2.47857943e-02,  1.86143853e-02,  3.86130251e-02,\n",
       "         -9.80350226e-02, -3.17052044e-02, -2.59682238e-02, -2.95785815e-02,\n",
       "         -1.59048494e-02, -1.25916535e-02, -1.72892082e-02,  4.58076335e-02,\n",
       "          2.12453809e-02,  3.76736367e-04, -4.44117654e-03,  1.44081851e-02,\n",
       "         -2.69349925e-02,  5.57913296e-02,  2.74429377e-02, -1.23565132e-02,\n",
       "         -8.28172863e-02, -6.58131540e-02, -2.09601782e-02, -3.12373210e-02,\n",
       "         -3.11181485e-03,  7.83041790e-02, -2.38785427e-02,  2.18924787e-02,\n",
       "         -1.66862253e-02, -1.20261656e-02, -4.67979582e-03,  4.74618822e-02,\n",
       "         -1.32503901e-02, -5.08262962e-02,  1.01210298e-02, -1.05616771e-01,\n",
       "          4.51527573e-02,  2.98242662e-02, -4.32334617e-02, -1.80327334e-02,\n",
       "         -1.03397574e-02, -4.81225476e-02, -4.85006114e-03, -7.51739964e-02,\n",
       "          2.95378501e-04, -4.58383523e-02, -1.31582627e-02,  4.94286045e-02,\n",
       "          1.68346465e-02,  1.22594228e-03,  7.45608062e-02, -2.21589189e-02,\n",
       "          4.75139357e-02,  5.70185706e-02, -3.28144170e-02, -2.71891849e-03,\n",
       "          2.88208816e-02, -1.65678933e-02,  2.85311304e-02,  5.24193048e-02,\n",
       "         -5.86043037e-02, -3.80188711e-02,  4.99273054e-02, -6.30245730e-02,\n",
       "          2.10619606e-02, -1.62091833e-02, -3.49001251e-02,  2.53834552e-03,\n",
       "          1.18764210e-02,  1.07595846e-01,  1.54048931e-02,  4.63070944e-02,\n",
       "          1.88222062e-02,  3.08211502e-02, -3.00046783e-02,  2.12507602e-02,\n",
       "          3.65702473e-02,  5.09374263e-03,  2.12374777e-02,  8.34833644e-03,\n",
       "         -2.46730801e-02,  1.45929465e-02,  3.40017630e-03, -3.03010438e-02,\n",
       "         -5.10144187e-03, -4.46592411e-03,  5.94663918e-02, -1.89710930e-02,\n",
       "         -2.07939427e-02, -3.03833671e-02,  3.57036442e-02, -2.28072703e-02,\n",
       "          2.70987432e-02, -2.31153506e-04,  3.64967878e-03,  1.92436650e-02,\n",
       "         -1.43780261e-02,  3.96994390e-02,  9.68375476e-04,  3.01253479e-02,\n",
       "          8.91606435e-02, -1.98723152e-02,  1.11675141e-02, -4.12921682e-02,\n",
       "          1.49168530e-02, -5.12540266e-02,  1.01867609e-01,  5.90757877e-02,\n",
       "         -3.61961760e-02,  1.41198039e-02,  2.68763043e-02,  6.40890189e-03,\n",
       "         -4.88801748e-02,  6.44527748e-02,  4.82478738e-02,  2.73776520e-03,\n",
       "         -2.92813051e-02,  5.81007637e-02, -3.46880853e-02, -4.27376926e-02,\n",
       "         -8.91836174e-03, -4.28213216e-02,  1.25387460e-01, -1.81708075e-02,\n",
       "          2.95091439e-02,  2.08027065e-02,  2.76087765e-02,  6.67697787e-02,\n",
       "         -1.05124051e-02,  4.17104810e-02, -5.88679165e-02,  1.57337487e-02,\n",
       "         -2.88460348e-02, -9.92858317e-03,  6.43670112e-02, -2.75323875e-02,\n",
       "          6.33829879e-03, -9.73158609e-03,  3.56022678e-02, -2.17468347e-02,\n",
       "         -1.75285060e-02, -4.18223850e-02, -3.65195461e-02, -4.10368759e-03,\n",
       "          4.38437611e-02, -2.59891357e-02, -6.57492429e-02, -1.43549059e-05]),\n",
       "  'score': tensor(0.0772)},\n",
       " {'page_number': 33,\n",
       "  'sentence_chunk': '“Could do with some of those letters now, eh?”he said cheerfully.   He was in a very good mood. Obviously he thought nobody stood a chance of reaching them here in a storm to deliver mail. Harry privately agreed, though the thought didn’t cheer him up at all.   As night fell, the promised storm blew up around them. Spray from the high waves splattered the walls of the hut and a fierce wind rattled the filthy windows. Aunt Petunia found a few moldy blankets in the second room and made up a bed for Dudley on the moth-eaten sofa. She and Uncle Vernon went off to the lumpy bed next door, and Harry was left to find the softest bit of floor he could and to curl up under the thinnest, most ragged blanket.   The storm raged more and more ferociously as the night went on. Harry couldn’t sleep. He shivered and turned over, trying to get comfortable, his stomach rumbling with hunger. Dudley’s snores were drowned by the low rolls',\n",
       "  'chunk_char_count': 931,\n",
       "  'chunk_word_count': 176,\n",
       "  'chunk_token_count': 232.75,\n",
       "  'embedding': array([-9.02658328e-03, -5.62483892e-02,  3.73763703e-02,  2.85436623e-02,\n",
       "          4.83051203e-02,  1.81351472e-02,  6.29197434e-03,  2.85656415e-02,\n",
       "          9.30383615e-03,  1.64981205e-02, -5.52994618e-03,  7.07925949e-03,\n",
       "         -1.68317314e-02, -2.01880485e-02, -3.74716967e-02,  7.81648513e-03,\n",
       "          7.98953325e-03, -6.71298383e-03, -5.75120151e-02, -7.54821347e-03,\n",
       "          4.41874787e-02, -2.38323808e-02,  9.28998366e-03, -3.71135883e-02,\n",
       "         -3.97800189e-03, -4.96033905e-03,  6.08949224e-04,  1.35295568e-02,\n",
       "         -9.41243209e-03,  6.41838461e-03, -2.83466689e-02, -4.47150730e-02,\n",
       "          3.13033015e-02,  7.45184813e-03, -1.68408770e-02,  8.90967250e-03,\n",
       "          2.45402027e-02,  1.19941263e-02,  5.83501197e-02, -1.34530384e-02,\n",
       "         -8.65018740e-03,  3.40845156e-03, -1.67010762e-02,  2.81028170e-02,\n",
       "          2.81556007e-02,  1.66211778e-03, -3.51033136e-02,  1.60152987e-02,\n",
       "          2.05003917e-02,  2.61067580e-02,  4.07381468e-02,  7.71698216e-03,\n",
       "         -3.44527066e-02,  6.02096394e-02,  1.74514437e-03, -2.38852035e-02,\n",
       "          1.64328944e-02,  2.23710909e-02, -4.07617539e-02, -3.14870104e-02,\n",
       "          1.23738959e-01, -1.29661374e-02, -1.17845135e-03,  5.81944771e-02,\n",
       "          3.00172288e-02, -3.81182916e-02, -2.87797172e-02,  4.31945100e-02,\n",
       "         -4.22692485e-02,  3.43176234e-03, -8.02989025e-03, -1.23901153e-02,\n",
       "         -1.08060567e-02, -3.46625820e-02, -1.75370798e-02,  1.56666785e-02,\n",
       "          1.62100866e-02, -3.49290855e-02,  1.57934669e-02,  2.67313933e-03,\n",
       "          5.56236599e-04,  8.26418400e-02, -6.63243094e-03,  6.89387973e-03,\n",
       "          3.60304713e-02,  1.20479800e-02,  6.73888111e-03, -4.12790217e-02,\n",
       "          2.12356616e-02,  5.71048819e-03,  2.11194139e-02,  8.11241046e-02,\n",
       "          5.21938596e-03,  9.75837633e-02, -1.77146774e-02,  1.67036634e-02,\n",
       "         -4.38334458e-02,  6.06572852e-02,  1.40505489e-02, -3.02524026e-02,\n",
       "          3.38877365e-02, -1.97002366e-02, -9.55213830e-02,  8.98247017e-05,\n",
       "          8.54392722e-03,  1.73467398e-02, -4.54459153e-02, -2.66243834e-02,\n",
       "         -1.80287007e-02, -1.16710076e-02, -1.41445054e-02, -1.38189271e-02,\n",
       "          1.15044937e-02,  1.95728242e-02,  1.33587616e-02, -1.97247379e-02,\n",
       "          3.41104865e-02,  5.10730036e-03, -1.20144757e-02,  8.38273671e-03,\n",
       "          4.96984161e-02, -9.18943807e-03,  1.81977563e-02,  7.35484138e-02,\n",
       "         -1.29207934e-03,  8.79752934e-02,  3.56793925e-02, -3.88260297e-02,\n",
       "         -1.41817955e-02,  6.24030977e-02, -8.89693350e-02,  8.68062582e-03,\n",
       "          5.30414581e-02, -6.83954880e-02, -6.35229200e-02,  2.05838773e-02,\n",
       "          1.05714966e-02,  3.28305885e-02, -1.16558746e-02,  1.07111391e-02,\n",
       "          2.94034053e-02,  4.21905853e-02, -4.32410575e-02,  2.33110022e-02,\n",
       "         -3.11926939e-03,  1.08372071e-03, -1.93858473e-03, -1.24497898e-02,\n",
       "         -7.95524865e-02,  6.37143031e-02,  2.76085380e-02,  4.73749498e-03,\n",
       "         -1.90169476e-02, -5.25085665e-02,  1.85266957e-02, -6.27623731e-03,\n",
       "          4.17943404e-04,  2.99447402e-03,  1.07471747e-02,  4.82438095e-02,\n",
       "         -2.24439744e-02,  4.74923551e-02,  3.43812257e-02, -2.48210840e-02,\n",
       "         -1.49471452e-02,  3.22919264e-02,  1.40601899e-02,  3.07862833e-02,\n",
       "          1.26699349e-02, -1.04999607e-02, -1.83262508e-02, -1.25992065e-02,\n",
       "          1.25036919e-02,  6.54925257e-02, -3.05431597e-02,  9.28870030e-03,\n",
       "          8.48229881e-03, -1.91355832e-02,  3.91717777e-02,  4.00209017e-02,\n",
       "         -4.73376364e-02,  3.32318321e-02,  1.14698000e-02, -1.21582504e-02,\n",
       "          2.91254744e-02,  3.76569517e-02, -2.87838606e-03, -4.17646654e-02,\n",
       "         -3.39500420e-02,  2.33941451e-02, -2.42731329e-02,  2.54742857e-02,\n",
       "          4.96034473e-02,  4.55168001e-02,  2.68385373e-03,  8.45139176e-02,\n",
       "          2.98483707e-02,  1.48798926e-02,  1.90153010e-02, -5.11269830e-02,\n",
       "          2.40220129e-02, -3.67071107e-02, -1.44870365e-02, -5.33011854e-02,\n",
       "         -7.70669132e-02, -1.81653835e-02,  4.90333047e-03, -5.21779293e-03,\n",
       "         -2.73633972e-02,  4.55673561e-02, -9.30345058e-03, -4.73375842e-02,\n",
       "         -2.39836648e-02,  1.17077241e-02,  4.01000585e-03, -1.40253203e-02,\n",
       "          2.94815954e-02,  1.13678062e-02, -3.69392857e-02,  3.08323931e-03,\n",
       "          5.80575038e-03,  2.06352174e-02, -1.18567690e-03, -3.08628567e-03,\n",
       "         -5.43492921e-02, -1.25124797e-01, -1.76142622e-03, -1.80147111e-03,\n",
       "         -4.35500331e-02,  4.97627296e-02, -5.23083359e-02,  1.57545544e-02,\n",
       "         -6.50412962e-02, -8.94463137e-02, -4.40666378e-02, -4.12436053e-02,\n",
       "         -5.17674126e-02, -2.28127409e-02,  2.47160383e-02,  2.18733996e-02,\n",
       "         -2.54457835e-02,  8.62246961e-04,  5.80686191e-03,  2.67659537e-02,\n",
       "         -7.85992574e-03,  3.95245943e-03,  2.44946461e-02,  2.30596233e-02,\n",
       "          4.10176739e-02, -2.15974934e-02, -2.29805764e-02, -1.53385885e-02,\n",
       "         -1.40974270e-02, -5.13014616e-03,  3.92217189e-02, -4.17524687e-04,\n",
       "          4.74136248e-02,  4.06368747e-02,  2.89833471e-02,  4.77539189e-02,\n",
       "          1.02057653e-02, -3.88021469e-02, -1.36866411e-02,  4.49764449e-03,\n",
       "         -2.15764400e-02,  2.19333405e-03, -4.37300541e-02,  2.82231476e-02,\n",
       "         -6.70510009e-02, -5.10016875e-03, -1.49992863e-02, -1.48112988e-02,\n",
       "         -3.48396460e-03,  9.00758728e-02, -8.79017171e-03,  3.18095610e-02,\n",
       "         -1.85355209e-02,  5.98316491e-02,  5.13749290e-03, -4.52988669e-02,\n",
       "         -6.61216152e-04, -4.53294031e-02, -2.51375530e-02, -5.92396483e-02,\n",
       "         -2.08098907e-02,  3.58821601e-02,  8.29343684e-03,  6.00498356e-02,\n",
       "         -9.11365170e-03, -4.01208661e-02, -4.47260439e-02, -2.17104387e-02,\n",
       "          5.43616600e-02,  3.54965217e-02, -3.66294757e-03,  5.20812459e-02,\n",
       "         -2.29731183e-02, -9.21793431e-02,  9.24817286e-03, -1.47392340e-02,\n",
       "         -3.72248527e-04, -3.49647403e-02, -8.80433545e-02,  3.52888443e-02,\n",
       "         -2.41530351e-02,  2.45497283e-02,  4.73403409e-02, -3.02362442e-02,\n",
       "         -3.62607166e-02,  3.62537093e-02, -3.06767151e-02,  1.52605716e-02,\n",
       "         -1.23181883e-02,  2.63673905e-02, -2.82685552e-02,  3.68351466e-03,\n",
       "         -7.64822867e-03, -3.37489992e-02,  3.13497223e-02,  8.97892565e-03,\n",
       "         -3.73416282e-02, -3.66249755e-02, -5.35942577e-02, -1.93459541e-02,\n",
       "          3.45604494e-02, -3.02472934e-02,  1.03140071e-01,  2.76969578e-02,\n",
       "         -1.49653982e-02,  3.48577090e-02, -8.98905843e-02,  1.36220651e-02,\n",
       "          4.43202909e-03, -9.16577503e-03,  1.41689675e-02, -1.56065337e-02,\n",
       "          2.10474934e-02, -2.38994583e-02,  2.91620567e-02,  1.58765782e-02,\n",
       "         -1.97166353e-02, -6.41915575e-03, -2.19066069e-02,  2.16588657e-02,\n",
       "         -3.20352730e-03,  4.23010178e-02, -7.04904497e-02,  2.50476468e-02,\n",
       "          2.42844895e-02, -2.31112000e-02,  2.81523238e-03,  8.22866242e-03,\n",
       "         -3.13381180e-02,  2.48032529e-02, -7.76644750e-03, -3.26040424e-02,\n",
       "         -4.78891702e-03, -4.43718284e-02, -2.16911547e-03, -4.22513187e-02,\n",
       "         -3.49467210e-02,  9.18447897e-02,  1.61826052e-02, -7.33328890e-03,\n",
       "         -2.09984947e-02, -1.03770800e-01, -5.26466854e-02,  7.71929249e-02,\n",
       "         -1.03851855e-02,  9.52168368e-03, -9.44911223e-03,  6.85651451e-02,\n",
       "          9.96433571e-03, -3.87629047e-02,  1.20435259e-03, -1.19181732e-02,\n",
       "         -5.48050702e-02, -2.13520918e-02,  8.22207481e-02, -1.21016297e-02,\n",
       "          3.36666987e-03, -1.22516826e-02, -1.44951604e-02, -1.11514479e-02,\n",
       "          5.08959033e-03,  8.41228571e-03,  2.04166695e-02,  2.20260657e-02,\n",
       "          8.04638267e-02,  4.17841114e-02,  4.14179750e-02, -2.65047438e-02,\n",
       "         -5.32548875e-02, -4.72846813e-02,  6.91471016e-03,  2.33760234e-02,\n",
       "         -7.59851336e-02, -6.17033010e-03,  2.70857867e-02,  2.15551443e-02,\n",
       "          4.83740121e-02,  2.27758829e-02, -1.62263513e-02, -2.79081650e-02,\n",
       "         -6.77670017e-02,  4.10434157e-02, -4.20508813e-03, -2.76405970e-03,\n",
       "         -5.38564427e-03, -3.87205631e-02, -9.84528102e-03,  5.43708168e-02,\n",
       "         -1.68639142e-02, -1.02063984e-01,  5.70098497e-02, -1.61867645e-02,\n",
       "         -1.22293010e-02, -1.33254314e-02,  5.01670726e-02, -8.46339390e-03,\n",
       "          7.38836732e-03,  1.13510406e-02, -5.96763417e-02,  2.25611106e-02,\n",
       "         -2.35569309e-02, -2.30236594e-02, -1.65932812e-02, -8.01043212e-02,\n",
       "         -3.44001059e-03, -8.23245421e-02, -2.75387820e-02, -6.70333114e-03,\n",
       "         -3.91899385e-02, -3.06248330e-02, -5.36183864e-02,  3.79673485e-03,\n",
       "          2.50240625e-03, -2.95794811e-02, -3.07113361e-02, -1.25485007e-02,\n",
       "         -2.18184460e-02,  1.19360257e-03,  6.35069907e-02, -7.30979368e-02,\n",
       "         -2.87617538e-02,  1.28385695e-02, -5.27177602e-02,  3.35528851e-02,\n",
       "          9.85496677e-03,  4.47260449e-03, -5.16783409e-02, -4.39614467e-02,\n",
       "         -1.88537687e-02, -8.06185678e-02,  2.50652507e-02, -1.70029122e-02,\n",
       "          3.06646582e-02, -2.01764107e-02, -1.38556603e-02,  2.22772714e-02,\n",
       "         -5.52908191e-03,  2.08154954e-02, -2.12974604e-02,  2.79533137e-02,\n",
       "          2.61758883e-02, -2.01024637e-02, -6.32163361e-02, -6.37393370e-02,\n",
       "         -4.12466377e-02,  4.61703390e-02,  1.94929559e-02,  3.80750075e-02,\n",
       "         -3.29548051e-03,  2.07385439e-02,  1.37554910e-02,  4.24063280e-02,\n",
       "          3.92379006e-03,  2.10041329e-02, -1.45178363e-02, -1.29139628e-02,\n",
       "         -1.08567178e-02,  2.91456450e-02,  1.20300306e-02, -3.25224213e-02,\n",
       "         -4.84570563e-02,  2.60203667e-02, -3.93298902e-02,  1.00047491e-01,\n",
       "         -5.32422736e-02,  2.00338103e-03,  6.41713524e-03, -1.62079167e-02,\n",
       "          2.42060013e-02,  2.22506188e-02,  3.63258832e-33, -1.70890670e-02,\n",
       "          5.73892370e-02,  3.77448648e-02, -2.69446261e-02, -1.71807352e-02,\n",
       "         -6.03664992e-03, -1.28342193e-02,  2.71708407e-02,  3.32016237e-02,\n",
       "          1.25659425e-02, -5.60280541e-03, -5.96935768e-03,  1.29955467e-02,\n",
       "          1.90878585e-02,  1.54371010e-02,  2.00461540e-02,  9.96707305e-02,\n",
       "         -1.67770572e-02,  4.39734943e-03, -9.19410586e-03, -4.24240157e-02,\n",
       "          2.99871080e-02, -4.80900817e-02,  8.10644180e-02,  3.08641288e-02,\n",
       "         -1.46216573e-02,  1.57103967e-02, -1.41232153e-02,  2.15022452e-03,\n",
       "          4.84263338e-02,  1.64783597e-02, -3.98476906e-02,  9.34697967e-03,\n",
       "         -4.51043397e-02, -1.05170183e-01, -6.02056794e-02,  5.35629392e-02,\n",
       "          5.67703554e-03,  3.06947716e-03,  6.01262553e-03,  7.36393617e-04,\n",
       "          6.83018640e-02, -1.45512698e-02, -4.74351272e-03, -5.33706769e-02,\n",
       "          6.72922353e-04,  4.59435992e-02,  1.53875714e-02,  1.74807329e-02,\n",
       "         -1.90489199e-02, -1.20269880e-02,  6.22559860e-02, -2.14831010e-02,\n",
       "         -4.71837558e-02,  9.48884785e-02,  3.32101434e-02,  1.17269740e-01,\n",
       "         -5.48061915e-02,  5.63998222e-02,  6.67798659e-03,  1.88105851e-02,\n",
       "          5.82530089e-02,  1.89164374e-02, -1.51792727e-02, -2.35025212e-02,\n",
       "          2.11312845e-02,  3.29381749e-02,  8.05845205e-03,  2.46505346e-02,\n",
       "          1.30776884e-02,  3.28368060e-02, -9.46178567e-03,  3.35382717e-03,\n",
       "          1.32985990e-02, -3.44994180e-02, -8.13860726e-03, -1.09009417e-02,\n",
       "         -1.78801958e-02,  8.30051526e-02, -1.85525883e-02, -1.21847950e-02,\n",
       "          5.89788426e-03, -1.86375296e-03, -1.57385729e-02, -5.27278706e-02,\n",
       "         -1.41595555e-02,  2.81818677e-03, -3.01924371e-03, -2.92712934e-02,\n",
       "         -1.36458334e-02, -8.48102272e-02,  3.05685098e-03, -3.07252090e-02,\n",
       "          2.05593416e-03, -3.80208995e-03,  2.42704228e-02, -9.66419652e-03,\n",
       "         -3.32713053e-02, -1.86706107e-04, -1.40978247e-02, -5.41166812e-02,\n",
       "         -5.86845875e-02,  2.43267305e-02, -1.11418273e-02,  2.35317741e-02,\n",
       "         -1.76174380e-02, -9.43038892e-03, -1.19318198e-02, -1.01404320e-02,\n",
       "         -4.80868071e-02,  4.70690988e-02,  5.89159466e-02,  2.79334979e-03,\n",
       "         -4.12901640e-02,  2.66343728e-02,  2.94950083e-02,  1.01951538e-02,\n",
       "          3.15971300e-02,  5.01684472e-02,  2.28910856e-02, -2.58624479e-02,\n",
       "         -1.85392126e-02, -4.96584140e-02, -6.46875054e-02,  3.04641109e-02,\n",
       "          9.15373769e-03, -5.45528233e-02,  8.83441791e-03, -1.09754661e-02,\n",
       "         -1.29748628e-01, -2.32526492e-02,  4.99143125e-03, -3.94365340e-02,\n",
       "         -4.02143002e-02, -4.07840125e-02, -5.95261808e-03,  5.13306521e-02,\n",
       "         -9.00992937e-03, -3.10700703e-02,  2.69565801e-03,  3.10293548e-02,\n",
       "          1.19355917e-02,  2.95754559e-02,  1.29768532e-02,  7.28351297e-03,\n",
       "         -3.56904930e-03, -3.43039781e-02, -1.66924242e-02,  2.19901325e-04,\n",
       "         -3.53106447e-02,  3.25424112e-02, -2.35092975e-02, -2.81579676e-03,\n",
       "         -9.38949920e-03, -3.24785449e-02, -3.95908020e-03, -1.65516157e-02,\n",
       "          3.95315699e-03, -4.99761403e-02,  1.89399365e-02,  1.42831188e-02,\n",
       "          1.50171276e-02,  3.52340974e-02, -5.47578298e-02, -2.83925841e-03,\n",
       "          5.62796146e-02,  5.95071875e-02,  6.79451739e-04, -3.61875035e-02,\n",
       "          2.05913037e-02, -2.35734880e-02,  1.29846623e-02,  1.15733659e-02,\n",
       "          1.89478509e-02,  3.99990678e-02,  3.91813926e-02, -4.21407633e-02,\n",
       "          3.98931988e-02,  8.36566091e-03, -7.62259439e-02,  6.27687722e-02,\n",
       "          2.67128125e-02, -6.34380803e-02, -1.63867455e-02,  1.69716645e-02,\n",
       "         -1.09848902e-02, -2.81942729e-02,  1.26177920e-02, -5.39067611e-02,\n",
       "         -5.09239137e-02, -6.42896863e-04,  2.42416747e-04,  3.42579349e-03,\n",
       "         -1.05906175e-02,  3.40510942e-02,  1.21683243e-03,  1.09625366e-02,\n",
       "         -8.56416952e-03, -6.39722217e-03,  2.21983884e-02,  4.54114936e-02,\n",
       "         -2.11660005e-02, -1.62720389e-03, -4.51653562e-02,  4.03330736e-02,\n",
       "         -1.87806156e-03, -1.03828721e-02,  1.86641719e-02, -4.56730425e-02,\n",
       "          1.65197812e-02, -1.53524550e-02,  7.56477267e-02, -5.70495538e-02,\n",
       "         -8.76390282e-03, -7.95300864e-03,  1.58773102e-02, -1.91619142e-03,\n",
       "          3.49936597e-02, -2.84764916e-02, -1.45439226e-02, -2.80576833e-02,\n",
       "         -5.09190336e-02,  6.74860738e-03,  1.39193013e-02,  3.43529023e-02,\n",
       "          4.50416654e-02, -2.46629920e-02, -1.04626175e-02, -4.97879907e-02,\n",
       "          3.84146557e-03, -3.82469930e-02,  1.08228326e-01,  6.29143498e-04,\n",
       "         -3.11288461e-02, -2.40909532e-02,  3.93754132e-02,  6.39643986e-04,\n",
       "          3.16935480e-02,  4.39675339e-02,  6.38939813e-02,  1.28116608e-02,\n",
       "          1.75025538e-02,  8.50890204e-02, -2.13223826e-02, -8.86604339e-02,\n",
       "          2.42484454e-02, -1.09199015e-02,  1.02207281e-01,  2.40034447e-03,\n",
       "          2.74968110e-02,  1.11641176e-02,  2.13744752e-02, -7.44218472e-03,\n",
       "         -9.83915478e-03,  4.58996892e-02, -3.28883156e-02,  8.73101577e-02,\n",
       "         -1.00643979e-02,  3.81871946e-02, -5.07492125e-02, -4.22682352e-02,\n",
       "         -4.22646552e-02, -1.80198234e-02,  1.34937651e-02,  2.94322008e-03,\n",
       "          8.42196122e-03,  3.30579258e-03, -8.06757994e-03, -9.30485874e-03,\n",
       "          1.46861151e-02, -2.96845566e-02, -4.24582744e-03,  2.16779415e-03]),\n",
       "  'score': tensor(0.0697)},\n",
       " {'page_number': 29,\n",
       "  'sentence_chunk': 'moved into Dudley’s second bedroom.   “Why?”said Harry.   “Don’t ask questions!”snapped his uncle. “Take this stuff upstairs, now.”   The Dursleys’ house had four bedrooms: one for Uncle Vernon and Aunt Petunia, one for visitors (usually Uncle Vernon’s sister, Marge), one where Dudley slept, and one where Dudley kept all the toys and things that wouldn’t fit into his first bedroom. It only took Harry one trip upstairs to move everything he owned from the cupboard to this room. He sat down on the bed and stared around him. Nearly everything in here was broken. The month-old video camera was lying on top of a small, working tank Dudley had once driven over the next door neighbor’s dog; in the corner was Dudley’s first-ever television set, which he’d put his foot through when his favorite program had been canceled; there was a large birdcage, which had once held a parrot that Dudley had swapped at school for a real air rifle, which was up on a shelf with the end all bent because Dudley had sat on it. Other shelves were full of books. They were the only things in the room that looked as though they’d never been touched.   From downstairs came the sound of Dudley bawling at his mother, I don’t want him in there…I need that room…make him get out...”    Harry sighed and stretched out on the bed. Yesterday he’d have given anything to be up here. Today he’d rather be back in his cupboard with that letter than up here without it. Next morning at breakfast, everyone was rather quiet. Dudley was in shock. He’d screamed, whacked his father with his Smelting stick, been sick on purpose, kicked his mother, and thrown his tortoise through the greenhouse roof, and he still didn’t have his room back. Harry was thinking about this time yesterday and bitterly wishing he’d opened the letter in the hall. Uncle Vernon and Aunt Petunia kept looking at each other darkly.   When the mail arrived, Uncle Vernon, who seemed to be trying to be nice to Harry, made Dudley go and get it. They heard him banging things with his Smelting stick all the way down the hall. Then he shouted, “There’s another one! ‘Mr. H. Potter, The Smallest Bedroom, 4 Privet Drive —’”    With a strangled cry, Uncle Vernon leapt from his seat and ran down the hall, Harry right behind him.',\n",
       "  'chunk_char_count': 2271,\n",
       "  'chunk_word_count': 414,\n",
       "  'chunk_token_count': 567.75,\n",
       "  'embedding': array([-1.38468668e-03, -5.58052175e-02,  1.73510276e-02,  1.38397058e-02,\n",
       "          4.92111966e-02,  6.92575276e-02,  1.44707020e-02,  1.70884635e-02,\n",
       "          2.32792944e-02,  4.17526513e-02, -2.24509239e-02,  3.66367982e-03,\n",
       "         -9.41190962e-03,  1.75524643e-03, -3.70400399e-02, -2.25614589e-02,\n",
       "         -1.38842417e-02, -5.96304983e-02, -7.97291026e-02, -3.09975538e-02,\n",
       "          4.01378684e-02, -4.11470979e-02,  4.48161662e-02, -1.03362827e-02,\n",
       "         -3.95610742e-02, -2.88009457e-02,  3.97925973e-02,  4.40831892e-02,\n",
       "         -4.23740447e-02,  3.48568223e-02, -8.32307711e-03, -5.12340181e-02,\n",
       "          3.28664370e-02,  5.41057847e-02, -1.93041582e-02, -5.37962979e-03,\n",
       "          2.93803029e-02,  3.81234568e-03,  6.28410559e-03, -3.58100981e-02,\n",
       "         -5.56608923e-02, -1.37722818e-02,  8.24997388e-03, -6.47046696e-03,\n",
       "          2.48637814e-02, -2.07052361e-02, -1.96176418e-03,  1.73102580e-02,\n",
       "          3.23245265e-02,  1.47986617e-02,  5.96814081e-02,  1.92776378e-02,\n",
       "         -2.67576929e-02,  5.17750196e-02,  2.63777822e-02,  2.11423766e-02,\n",
       "          2.28312351e-02,  9.00950581e-02, -3.93731520e-02, -5.67655116e-02,\n",
       "          7.88033679e-02, -5.49567118e-03, -1.70657802e-02,  3.31911370e-02,\n",
       "         -8.13710224e-03, -1.96552724e-02, -3.82309459e-04, -3.05313561e-02,\n",
       "         -2.76881121e-02, -3.91007075e-03,  1.17808040e-02, -2.19809543e-02,\n",
       "         -2.30657589e-02,  1.36906002e-02, -2.62094382e-02, -9.22166370e-03,\n",
       "          7.35951355e-03,  5.38118090e-03, -2.06371099e-02,  2.24156538e-03,\n",
       "         -1.36587368e-02,  6.58542663e-02,  1.06580360e-02,  2.02668086e-03,\n",
       "          1.50365643e-02,  3.06346156e-02,  1.20465821e-02, -9.29064900e-02,\n",
       "         -1.89641900e-02,  1.49109578e-02,  3.75168794e-03,  1.19841129e-01,\n",
       "          4.81423782e-03,  9.50318649e-02, -7.48466142e-03,  1.59326373e-04,\n",
       "         -5.50196394e-02,  7.79619142e-02, -1.08993435e-02,  1.79015920e-02,\n",
       "          4.69020791e-02, -9.31194518e-03, -5.77004664e-02,  5.64754987e-03,\n",
       "          3.21053788e-02, -1.31376437e-03,  1.36514381e-02, -1.05167329e-02,\n",
       "         -3.59913372e-02, -1.81619935e-02, -5.70513599e-04, -1.43486951e-02,\n",
       "         -1.24469902e-02,  2.38281749e-02, -2.33684806e-03, -1.13836564e-02,\n",
       "          2.46632826e-02, -7.58482888e-03,  2.90060155e-02, -1.21358745e-02,\n",
       "          1.99265908e-02, -1.62220150e-02,  7.51033006e-03,  5.71443439e-02,\n",
       "          2.48211175e-02,  5.52312806e-02,  3.71524915e-02, -4.71542813e-02,\n",
       "         -3.10680196e-02,  2.25256309e-02, -6.16794489e-02, -4.59163170e-03,\n",
       "          4.75844592e-02, -5.95506504e-02, -2.72626821e-02,  2.84789782e-02,\n",
       "          3.20205241e-02,  4.14961167e-02,  2.37184763e-02,  5.96186519e-02,\n",
       "         -2.20038798e-02,  3.28891724e-02, -5.53144282e-03,  1.26173273e-02,\n",
       "         -6.13132715e-02,  2.93962564e-02,  2.23000045e-03,  3.84721830e-02,\n",
       "         -3.49384286e-02,  1.16065651e-01,  5.43786287e-02,  5.62065456e-04,\n",
       "         -2.36617941e-02, -3.49738486e-02, -8.91336985e-03,  2.02898420e-02,\n",
       "         -1.00572892e-02,  4.15542051e-02, -1.16266115e-02,  2.13852478e-03,\n",
       "          3.13034072e-03,  2.68603340e-02,  2.78551131e-02,  2.05261726e-03,\n",
       "         -5.95121793e-02, -2.92326673e-03,  4.46742810e-02, -6.54177507e-04,\n",
       "          5.87322563e-02, -4.04490680e-02, -2.02719569e-02, -9.21664978e-05,\n",
       "          3.23243067e-02,  1.16716139e-02, -3.59509178e-02,  1.26308389e-02,\n",
       "         -1.57298497e-03, -2.92120650e-02,  1.00633306e-02,  4.66925427e-02,\n",
       "         -2.10362244e-02,  4.43297885e-02,  2.36453675e-02, -1.27639556e-02,\n",
       "          1.55267641e-02,  2.84283725e-03, -2.29088664e-02, -2.71560578e-03,\n",
       "         -7.36291632e-02,  2.14612763e-02, -2.03577690e-02,  2.32380647e-02,\n",
       "          4.09921259e-02,  3.33409663e-03,  3.47477719e-02,  2.10645832e-02,\n",
       "          1.17926076e-02,  1.70059539e-02,  4.44074124e-02, -6.93718940e-02,\n",
       "          1.44951949e-02, -2.64501683e-02, -1.10483794e-02, -5.99312633e-02,\n",
       "         -7.28147551e-02,  4.40456672e-03,  1.26290116e-02,  8.92599765e-03,\n",
       "         -1.62619166e-02,  1.89545359e-02, -2.58149728e-02, -4.65034880e-02,\n",
       "         -3.49527411e-02,  3.21028545e-03,  2.23093498e-02, -8.84788763e-03,\n",
       "          2.73340885e-02, -1.45115145e-02, -2.55942065e-03, -1.04573611e-02,\n",
       "         -1.78512670e-02, -2.15480700e-02, -6.80118101e-03,  2.45061200e-02,\n",
       "         -3.02031841e-02, -9.83181596e-02, -3.59351262e-02,  3.93403880e-03,\n",
       "         -1.95133314e-02,  7.67479315e-02, -2.90878359e-02, -1.44308778e-02,\n",
       "         -4.44380790e-02, -7.20758289e-02, -1.33430772e-02, -7.01692626e-02,\n",
       "          2.20967620e-03, -3.25829238e-02,  1.88498627e-02,  3.85261178e-02,\n",
       "         -4.40886617e-03,  5.86396642e-03,  2.16741804e-02, -1.60395950e-02,\n",
       "          1.31102535e-03, -3.87245715e-02,  2.57784706e-02,  4.21457104e-02,\n",
       "          4.68441434e-02, -2.32962370e-02,  4.48977947e-02, -4.01819684e-03,\n",
       "         -4.41994034e-02, -8.44628271e-03,  2.65679546e-02, -3.03903073e-02,\n",
       "          1.07096275e-02,  3.27081531e-02,  6.52993470e-02,  6.11262731e-02,\n",
       "          1.19418269e-02,  1.23577882e-02, -3.98190394e-02,  4.10172641e-02,\n",
       "         -3.95879708e-02,  8.92626867e-03, -2.77904514e-02,  4.10888018e-03,\n",
       "         -5.07373661e-02, -1.11413617e-02,  3.21847526e-03, -3.48697556e-03,\n",
       "         -3.83756519e-03,  4.20074761e-02,  6.21315278e-03,  4.44178805e-02,\n",
       "         -1.01260375e-02,  4.82361168e-02,  1.49282357e-02, -4.83684391e-02,\n",
       "         -3.17910425e-02, -1.77157596e-02, -9.78737324e-03, -1.55737030e-03,\n",
       "         -2.64554992e-02,  3.55600640e-02, -2.59553771e-02,  2.41862517e-02,\n",
       "          1.54575454e-02, -2.51591019e-03, -1.45100821e-02,  6.81786565e-04,\n",
       "          6.04950450e-02,  5.62492991e-03,  5.55868435e-04,  5.20234816e-02,\n",
       "         -1.57516282e-02, -6.09827489e-02,  6.74629491e-03, -4.77958135e-02,\n",
       "          7.35501666e-03, -5.80633841e-02, -1.15979761e-01,  4.74483334e-02,\n",
       "         -3.53669777e-04,  4.67925984e-03,  2.77628284e-02, -5.73939038e-03,\n",
       "          1.03877885e-02,  2.34635510e-02, -2.88697388e-02, -2.44247075e-02,\n",
       "         -4.35133651e-02,  1.96365081e-02, -7.76517838e-02, -8.87109526e-03,\n",
       "         -1.50078982e-02, -2.85962671e-02,  5.28625846e-02,  7.02505698e-03,\n",
       "         -1.84384212e-02, -3.48927900e-02, -4.14429903e-02,  2.30700113e-02,\n",
       "          1.96573380e-02, -6.70089945e-02,  4.39639315e-02,  3.30066457e-02,\n",
       "         -3.52436467e-03, -1.12106360e-03, -6.15332983e-02,  5.68113197e-03,\n",
       "         -3.85901630e-02,  3.50901671e-02,  1.82649661e-02, -8.77295807e-03,\n",
       "          2.50893347e-02, -1.98426861e-02,  1.77508537e-02,  2.94469558e-02,\n",
       "         -5.79040051e-02, -8.12264392e-04, -5.90340197e-02,  1.00895325e-02,\n",
       "          7.21862912e-03, -2.55865436e-02, -5.13415895e-02,  5.43501340e-02,\n",
       "         -7.95114879e-03, -1.48800376e-03,  2.48033106e-02, -2.24841340e-03,\n",
       "          6.82308245e-03,  5.93127310e-02, -5.11060655e-02,  4.04957309e-03,\n",
       "         -3.35672274e-02, -2.27670129e-02, -2.79921275e-02, -2.20812149e-02,\n",
       "         -3.20305154e-02,  4.49028946e-02,  3.26145105e-02, -2.56129559e-02,\n",
       "          1.86542552e-02, -9.80627909e-02, -2.99155768e-02,  9.53610092e-02,\n",
       "          2.34606154e-02, -1.07951649e-02, -2.00827513e-02,  6.31190687e-02,\n",
       "          5.35618514e-02, -1.07992031e-02, -1.68067385e-02, -3.87611636e-03,\n",
       "         -4.58173230e-02, -5.34320436e-02,  4.51314040e-02, -2.50936616e-02,\n",
       "          9.88056976e-03, -4.07157540e-02, -4.70364951e-02, -8.90738051e-03,\n",
       "          2.55547278e-02, -3.43565666e-03,  4.22637304e-03, -2.35456318e-04,\n",
       "          7.00787902e-02,  5.77973761e-03,  9.40745138e-03, -2.90578865e-02,\n",
       "         -2.37456989e-02,  4.09912225e-03,  2.41908506e-02,  2.30421172e-03,\n",
       "          8.36137962e-03, -8.56341422e-03,  1.95449982e-02,  2.46410631e-02,\n",
       "          1.61347333e-02,  8.27886462e-02,  1.95902940e-02, -1.74649730e-02,\n",
       "         -4.20424826e-02,  2.56925654e-02, -7.73771200e-03, -8.77834577e-03,\n",
       "         -3.13749500e-02, -5.49175069e-02,  2.20150482e-02,  5.34645654e-02,\n",
       "         -1.34227015e-02, -7.41831884e-02,  4.05350588e-02, -3.85228172e-02,\n",
       "         -2.06545219e-02,  2.03379430e-02,  2.72839218e-02,  2.60573048e-02,\n",
       "          4.42696773e-02,  2.92397235e-02, -6.29312694e-02,  2.94627510e-02,\n",
       "         -1.76172815e-02, -1.41950063e-02, -8.59443378e-03, -6.48125783e-02,\n",
       "          8.94649047e-03, -5.12369461e-02, -7.31688067e-02, -1.99411660e-02,\n",
       "          4.73943353e-03, -3.52613889e-02, -3.13524492e-02, -2.18926207e-03,\n",
       "          7.00791404e-02, -2.67436765e-02,  4.86759614e-04,  8.96796864e-03,\n",
       "         -2.50165295e-02,  2.39432901e-02,  3.27046365e-02, -9.67724323e-02,\n",
       "         -1.80068035e-02, -6.61823247e-03,  1.37864910e-02,  7.38845952e-03,\n",
       "          3.74046457e-03,  5.34829358e-03, -5.73271811e-02, -3.33442353e-02,\n",
       "          2.04179604e-02, -1.43892094e-01, -2.78199036e-02,  1.34168528e-02,\n",
       "          2.44638650e-04, -4.19064425e-02, -2.96112411e-02,  2.97477748e-02,\n",
       "          2.24299263e-02, -6.52841479e-03, -1.85842738e-02,  4.78974618e-02,\n",
       "          2.62415390e-02, -2.89470218e-02, -4.73925844e-02, -3.50388065e-02,\n",
       "         -7.86616281e-03,  2.78545413e-02, -3.86683876e-03,  3.19232866e-02,\n",
       "         -3.22691500e-02, -2.34178007e-02,  2.81906649e-02,  5.02275042e-02,\n",
       "         -1.91474892e-02,  2.33892258e-02, -8.17246139e-02,  2.10983697e-02,\n",
       "         -2.71570291e-02,  3.18220444e-02, -9.31260455e-03, -1.29697639e-02,\n",
       "         -5.51479682e-02, -7.13769393e-03, -4.38483246e-02,  1.13619253e-01,\n",
       "         -1.72828920e-02, -2.63174791e-02,  2.76611801e-02,  3.42338793e-02,\n",
       "          2.57380605e-02,  2.11277287e-02,  3.67296104e-33, -2.12238804e-02,\n",
       "         -8.58223364e-02,  1.21241761e-02,  3.24181952e-02, -1.85535681e-02,\n",
       "          1.15106460e-02, -2.05806922e-02,  5.56854606e-02,  1.97050609e-02,\n",
       "          7.24668568e-03, -2.14235988e-02,  7.70851271e-04, -1.05556315e-02,\n",
       "          9.09593143e-03, -1.24730840e-02,  1.34384800e-02,  7.86454231e-02,\n",
       "         -1.66849643e-02, -1.25774536e-02,  7.07457028e-03, -3.09187081e-02,\n",
       "          4.91232537e-02, -5.13334423e-02,  7.68440366e-02,  5.79684088e-03,\n",
       "          1.37874177e-02, -7.10874610e-03, -1.12368185e-02, -3.79716083e-02,\n",
       "          4.01336476e-02,  3.69135290e-02, -2.74277125e-02,  5.29491939e-02,\n",
       "         -5.23284003e-02, -9.89177451e-02, -4.80014049e-02,  1.28548383e-03,\n",
       "          2.41537094e-02, -2.30879448e-02, -3.32048908e-02, -1.94388181e-02,\n",
       "          2.13686321e-02, -2.26996047e-03,  1.77633762e-02, -7.05064163e-02,\n",
       "          5.14137298e-02,  2.64839679e-02, -1.19523881e-02,  8.17421544e-03,\n",
       "          2.69764615e-03,  7.11251423e-02,  9.38788727e-02,  2.52046360e-04,\n",
       "         -4.41145934e-02,  4.28032018e-02,  4.89882901e-02,  1.05846271e-01,\n",
       "         -5.43485060e-02,  5.51409572e-02, -6.40876475e-04, -1.19738542e-02,\n",
       "          4.82483804e-02,  7.38507556e-03,  4.60119778e-03, -3.61285992e-02,\n",
       "          1.88029520e-02,  1.46580860e-02,  9.78541747e-03,  2.03852803e-02,\n",
       "          2.08141226e-02,  1.85800809e-02, -6.01382852e-02,  3.23999114e-02,\n",
       "          3.12864780e-02, -1.23005006e-02,  1.90953687e-02, -3.80080976e-02,\n",
       "         -2.83104498e-02,  8.19793940e-02, -4.26165015e-02, -1.74255073e-02,\n",
       "         -2.43493337e-02,  4.62243035e-02,  9.70228296e-03, -3.95560339e-02,\n",
       "          3.13229160e-03,  1.09278606e-02,  2.07163524e-02, -4.00398336e-02,\n",
       "         -1.49097191e-02, -2.94242911e-02,  8.39676056e-03,  1.06152222e-02,\n",
       "          2.10366282e-03, -2.26909071e-02,  2.51719635e-02,  2.33380497e-03,\n",
       "          1.50150340e-02, -3.51350382e-02, -1.86938886e-02, -1.93802156e-02,\n",
       "         -4.88681458e-02, -7.71030784e-03, -2.23289803e-02,  4.49121222e-02,\n",
       "         -5.47648378e-05, -1.62427332e-02, -7.62817403e-03, -7.74836773e-03,\n",
       "         -3.94935161e-02,  2.43924242e-02,  1.03200473e-01,  1.66168585e-02,\n",
       "         -9.92269590e-02,  2.01864578e-02,  2.56434418e-02, -3.32338139e-02,\n",
       "          2.31790524e-02,  2.94536464e-02,  2.26717666e-02, -4.95984443e-02,\n",
       "         -3.36405709e-02, -2.13196818e-02, -5.86723946e-02,  6.87610731e-02,\n",
       "          2.71428712e-02, -1.76921338e-02, -2.68525607e-03, -4.01309058e-02,\n",
       "         -7.19535425e-02, -5.17402217e-02,  1.30275059e-02,  1.05737541e-02,\n",
       "         -2.55810395e-02, -8.30948725e-03, -1.52229695e-02,  4.22280096e-02,\n",
       "         -1.39522622e-03, -2.41999216e-02, -2.78636925e-02,  3.48405726e-02,\n",
       "         -2.14514937e-02,  2.29990166e-02,  1.40633034e-02,  1.90102344e-03,\n",
       "         -1.81409921e-02, -2.97650397e-02, -2.54316721e-02, -6.48339689e-02,\n",
       "         -1.04605872e-02,  1.45858945e-02, -3.32886465e-02,  5.03180549e-02,\n",
       "          7.46432878e-03, -3.09381522e-02,  1.22652641e-02, -4.47401591e-02,\n",
       "         -4.83855046e-03, -4.38541397e-02,  2.83661969e-02, -4.95534390e-02,\n",
       "         -1.20964767e-02,  5.32093719e-02, -5.57522699e-02, -5.19600790e-03,\n",
       "          9.00892634e-03,  3.79476100e-02, -3.82358348e-03, -1.38221160e-02,\n",
       "          3.50898430e-02, -3.60985659e-02, -1.74936485e-02, -2.09883284e-02,\n",
       "          2.96545234e-02,  5.81963509e-02,  5.50098158e-02, -1.96424443e-02,\n",
       "          2.16027070e-02,  2.53352895e-02, -4.74260747e-02,  1.88478325e-02,\n",
       "         -1.05920276e-02, -4.62472215e-02, -2.99239270e-02, -1.86506044e-02,\n",
       "         -1.77953616e-02, -5.82074709e-02,  6.19630404e-02, -6.04235791e-02,\n",
       "          1.13317901e-02,  1.63485780e-02,  3.42461397e-03,  1.49175758e-02,\n",
       "          7.16854027e-03,  3.88186201e-02, -2.37758760e-03,  1.39786983e-02,\n",
       "          1.88051444e-02, -2.31927074e-03, -1.81578267e-02,  3.33146006e-02,\n",
       "          4.93751315e-04, -9.79100261e-03, -3.87907848e-02,  2.24604495e-02,\n",
       "          1.70931499e-02,  8.70639365e-03,  1.13937268e-02, -1.92373525e-02,\n",
       "          2.01543514e-02, -3.97869796e-02,  6.97093531e-02, -7.26785436e-02,\n",
       "         -1.05824852e-02, -9.57962405e-03, -2.45598331e-02,  2.15092283e-02,\n",
       "          3.63142490e-02, -2.45837532e-02, -2.78843157e-02, -2.62053087e-02,\n",
       "         -3.21524069e-02,  1.69303343e-02, -5.23390807e-02,  3.28837298e-02,\n",
       "          4.48213518e-02, -1.87514874e-03, -2.08187513e-02, -1.82474945e-02,\n",
       "         -5.87567827e-03, -3.04997843e-02,  9.74382684e-02,  7.55812321e-03,\n",
       "         -4.03265841e-02, -1.58386342e-02, -4.05146508e-03,  2.54237186e-02,\n",
       "          9.39978193e-03,  5.16591184e-02,  5.53766564e-02,  3.80933918e-02,\n",
       "          1.81998685e-02,  5.17535508e-02, -3.00680920e-02, -4.66711558e-02,\n",
       "          2.24953312e-02, -2.90104151e-02,  9.80887339e-02, -4.76070009e-02,\n",
       "          5.81859574e-02,  5.49799763e-02,  2.56446190e-02,  2.88393088e-02,\n",
       "         -1.19696250e-02,  3.41978855e-03, -2.87465509e-02,  5.26519082e-02,\n",
       "         -2.52697337e-02,  3.78001221e-02, -3.39567624e-02, -6.93635121e-02,\n",
       "         -4.61658277e-02,  1.40905967e-02,  8.70780088e-03,  4.38394397e-02,\n",
       "         -2.56731063e-02, -5.09387217e-02,  7.82569125e-03,  1.75707638e-02,\n",
       "          3.41350585e-02, -4.48664278e-02, -3.41492146e-02,  4.32477146e-03]),\n",
       "  'score': tensor(0.0644)},\n",
       " {'page_number': 29,\n",
       "  'sentence_chunk': 'Uncle Vernon had to wrestle Dudley to the ground to get the letter from him, which was made difficult by the fact that Harry had grabbed Uncle Vernon around the neck from behind. After a minute of confused fighting, in which everyone got hit a lot by the Smelting stick, Uncle Vernon straightened up, gasping for breath, with Harry’s letter clutched in his hand.   “Go to your cupboard — I mean, your bedroom,” he wheezed at Harry.',\n",
       "  'chunk_char_count': 431,\n",
       "  'chunk_word_count': 79,\n",
       "  'chunk_token_count': 107.75,\n",
       "  'embedding': array([-6.21606829e-03, -8.34541246e-02,  2.84685101e-03,  1.28072756e-03,\n",
       "         -5.97148342e-03,  6.05515018e-02,  2.56812456e-03,  5.00644818e-02,\n",
       "          3.00104246e-02,  2.92342119e-02, -4.27272636e-03, -2.20338535e-02,\n",
       "         -4.68036272e-02, -3.38687189e-02, -1.30918082e-02, -5.14678583e-02,\n",
       "          3.43195051e-02, -4.14692611e-02, -3.21271867e-02, -1.85193121e-02,\n",
       "          6.64236546e-02, -3.73313725e-02,  1.31529514e-02, -2.51712054e-02,\n",
       "          5.99051267e-02, -1.69841368e-02,  3.56991068e-02, -2.07913816e-02,\n",
       "          3.37118329e-03,  3.34540531e-02, -9.27739777e-03, -3.48296016e-02,\n",
       "         -6.04106160e-03,  1.42301060e-02, -2.97490209e-02,  1.05547253e-02,\n",
       "          2.36352738e-02, -2.40069609e-02,  1.00164833e-02,  1.24460766e-02,\n",
       "          3.18022445e-03,  5.57724312e-02,  3.86543013e-02, -1.79416891e-02,\n",
       "          3.03430576e-02, -2.89430823e-02,  1.48548028e-02,  9.29773878e-03,\n",
       "          1.40772499e-02, -2.00229809e-02,  7.32248276e-02,  1.02721648e-02,\n",
       "         -2.23697647e-02,  2.16208156e-02, -1.03101283e-02,  4.25366089e-02,\n",
       "          5.35202213e-03,  6.29613772e-02, -9.86818969e-03, -6.39592186e-02,\n",
       "          1.19944409e-01,  3.79712023e-02, -4.38568629e-02,  9.53400508e-03,\n",
       "          5.11012673e-02, -5.09402417e-02, -1.30484495e-02, -1.79440744e-04,\n",
       "         -1.13602332e-03, -4.13983455e-03, -2.46008523e-02, -4.68193144e-02,\n",
       "         -2.00568065e-02,  1.29734417e-02, -6.66488986e-03, -1.84711386e-02,\n",
       "          1.60698947e-02, -6.50605187e-02,  1.71373356e-02, -2.43222203e-05,\n",
       "         -5.25436252e-02,  8.16885754e-02,  3.05514876e-02,  9.07050632e-03,\n",
       "         -1.90863889e-02,  5.58803603e-02,  3.10487803e-02, -6.60170168e-02,\n",
       "          2.13396251e-02, -8.62157252e-03,  1.10006239e-02,  7.05688447e-02,\n",
       "          3.32319215e-02,  8.66320431e-02, -2.74423994e-02,  2.04027891e-02,\n",
       "         -4.89495061e-02,  3.50174047e-02, -1.47831338e-02, -2.07993835e-02,\n",
       "          5.16275540e-02,  2.11938303e-02, -9.68746543e-02, -3.71134304e-03,\n",
       "          2.60984376e-02,  2.40407940e-02,  3.00025921e-02, -8.83077038e-04,\n",
       "         -4.68108468e-02, -6.79830415e-03, -1.31808985e-02, -3.56369875e-02,\n",
       "         -1.65183842e-03,  3.75191458e-02, -1.18035320e-02, -3.08768288e-03,\n",
       "          6.47754297e-02, -3.75629701e-02, -7.30772316e-03,  3.83100333e-03,\n",
       "          4.08627652e-02, -1.63713121e-03,  3.10138613e-02,  9.03725922e-02,\n",
       "         -6.67743105e-03,  3.81291285e-02,  4.08601761e-02, -3.55381556e-02,\n",
       "         -3.81275527e-02,  1.68751683e-02, -7.61060491e-02, -1.09376861e-02,\n",
       "          2.99248360e-02, -7.28842989e-02, -2.45824251e-02,  5.01519926e-02,\n",
       "          2.39887778e-02,  3.81477028e-02,  8.65065679e-03,  2.92662755e-02,\n",
       "         -1.83459166e-02,  1.55961718e-02,  2.41954010e-02,  2.45150607e-02,\n",
       "         -4.33859751e-02,  1.70972999e-02,  1.99612100e-02,  1.93772353e-02,\n",
       "         -2.34934762e-02,  7.14909360e-02,  2.51602177e-02, -3.48473974e-02,\n",
       "          3.78917484e-03, -4.58722971e-02,  3.15163247e-02,  2.50481465e-03,\n",
       "         -1.69628896e-02,  8.11481923e-02, -2.46803481e-02,  1.37209063e-02,\n",
       "         -2.99301092e-02,  8.02442357e-02,  2.24146768e-02,  2.24775039e-02,\n",
       "         -2.73857787e-02, -8.07287078e-03,  2.77341418e-02,  5.12415310e-03,\n",
       "          2.32713539e-02, -3.55806947e-02,  6.83761027e-04, -2.37843134e-02,\n",
       "          3.08716372e-02,  2.75836624e-02, -2.18400620e-02,  9.99976695e-03,\n",
       "          3.15582193e-03, -5.62858162e-03,  2.16272436e-02,  3.16662341e-02,\n",
       "         -5.29702120e-02,  3.75667959e-02,  3.35092540e-03, -3.00832540e-02,\n",
       "          1.53973224e-02,  2.75580529e-02, -3.05937547e-02, -4.18889448e-02,\n",
       "         -6.65368587e-02,  9.46409348e-03, -3.63993138e-04,  2.01152340e-02,\n",
       "          5.68427071e-02, -6.86934358e-03,  3.10980286e-02,  2.93044336e-02,\n",
       "         -1.50876306e-02, -3.42107862e-02, -1.88757814e-02, -2.72144284e-02,\n",
       "         -1.18510108e-02, -1.35526899e-02, -4.63982411e-02, -4.07119729e-02,\n",
       "         -7.02278316e-02, -1.10093225e-02,  4.44607437e-02,  1.55067910e-02,\n",
       "         -3.22944000e-02, -4.37846780e-03, -9.64892097e-04, -2.78772470e-02,\n",
       "         -2.70433724e-02, -1.17494985e-02,  1.96629614e-02,  5.45838028e-02,\n",
       "          4.80427928e-02,  2.12955363e-02, -3.80018912e-02,  6.62698224e-03,\n",
       "         -4.47854623e-02,  5.01818815e-03, -3.05185653e-02, -2.78651621e-02,\n",
       "         -1.30762591e-03, -4.93246876e-02, -1.09116714e-02,  2.47639371e-03,\n",
       "         -1.48999151e-02,  5.88610880e-02, -2.39114966e-02, -2.41749175e-02,\n",
       "         -3.39329452e-03, -8.21900666e-02, -1.56988036e-02, -3.75512242e-02,\n",
       "         -9.32861771e-03, -5.20751905e-03,  6.14311872e-03,  5.22285439e-02,\n",
       "         -1.82566401e-02, -1.77830383e-02,  2.70034303e-03, -2.56752744e-02,\n",
       "         -1.08712213e-02, -3.09063178e-02,  7.07385391e-02,  7.01709464e-02,\n",
       "          1.56959444e-02, -1.36139393e-02, -3.47874686e-03, -9.79280192e-03,\n",
       "         -1.99799277e-02, -2.68548615e-02,  1.13255717e-02,  1.06269214e-03,\n",
       "          2.74591800e-02,  2.28957254e-02,  3.78408842e-02,  2.64758756e-03,\n",
       "          4.70581977e-03, -1.69937462e-02, -4.89774346e-02,  3.78592238e-02,\n",
       "          2.09860373e-02,  3.78016494e-02, -2.58605424e-02,  6.94260513e-03,\n",
       "         -6.37868196e-02,  1.25692040e-03,  4.56867553e-03, -1.27145834e-02,\n",
       "         -9.50801652e-03,  2.93063596e-02,  3.13651003e-02,  4.68786247e-02,\n",
       "         -1.00789620e-02,  6.52633160e-02, -6.33803010e-03, -6.97652390e-03,\n",
       "          1.09480312e-02, -3.08073740e-02, -5.52128628e-03, -5.90789281e-02,\n",
       "         -1.59621369e-02,  3.37426178e-02, -6.33180067e-02,  7.16547966e-02,\n",
       "          2.48652678e-02, -3.12700644e-02, -4.43334412e-03,  2.62665562e-02,\n",
       "          2.41045058e-02,  3.94545943e-02,  2.41259276e-03,  6.74341172e-02,\n",
       "         -3.55399102e-02, -6.48681596e-02, -6.60295598e-04, -4.83674146e-02,\n",
       "          6.17068186e-02, -3.41860801e-02, -5.31806722e-02,  1.68727469e-02,\n",
       "          9.20932647e-03,  5.46839554e-03, -1.55728753e-03, -1.34414183e-02,\n",
       "         -2.58973073e-02,  4.19218279e-02, -3.16926613e-02,  1.83541363e-03,\n",
       "         -2.30343081e-02,  3.36215571e-02, -5.38185164e-02,  2.31142482e-03,\n",
       "          5.81756374e-03, -2.15479862e-02,  1.74141321e-02,  8.27603787e-03,\n",
       "         -6.76749181e-03, -1.26526114e-02, -2.47533154e-02,  1.79428831e-02,\n",
       "          4.26351316e-02, -8.07844847e-02,  2.75851227e-02, -8.19338486e-03,\n",
       "         -1.14406943e-02,  5.39476983e-03,  9.58022138e-04,  1.03894388e-02,\n",
       "         -3.28117236e-02,  4.95267361e-02,  9.83767025e-03,  1.36193065e-02,\n",
       "          3.37495767e-02, -2.57962719e-02,  6.27372935e-02,  1.27269272e-02,\n",
       "         -1.82595185e-03,  1.52108381e-02, -2.78621651e-02,  7.46789873e-02,\n",
       "          6.72824122e-03, -8.48148484e-03, -8.13251212e-02,  5.03383577e-02,\n",
       "         -9.30032786e-03, -3.50378379e-02, -3.05068470e-03,  5.02830558e-03,\n",
       "         -4.48636934e-02,  3.90369557e-02,  2.20597461e-02,  8.20872467e-03,\n",
       "         -3.45934331e-02, -7.78106553e-03,  9.77469049e-03,  3.46135371e-03,\n",
       "         -2.84619853e-02,  7.80781135e-02, -1.21540576e-02, -4.47427966e-02,\n",
       "         -1.14228744e-02, -9.34495404e-02, -2.76736673e-02,  6.16737716e-02,\n",
       "          1.00661088e-02,  1.91415413e-04, -3.60159166e-02,  3.24027613e-02,\n",
       "          5.21978624e-02, -2.74995081e-02, -2.28287140e-03, -1.26411789e-03,\n",
       "         -4.80824336e-02, -2.76560970e-02,  8.63490328e-02, -1.07781868e-02,\n",
       "         -1.61249598e-03, -2.73655578e-02, -3.52010839e-02, -2.45839655e-02,\n",
       "          3.82296704e-02,  2.11105514e-02, -2.55592000e-02, -2.30973586e-02,\n",
       "          7.45032430e-02,  1.93360727e-02,  2.35971082e-02, -1.38807064e-02,\n",
       "         -1.95044298e-02, -3.76966462e-04,  3.87240872e-02, -4.30626376e-03,\n",
       "         -5.48176607e-03,  1.99125912e-02, -6.03581220e-03,  3.18982918e-03,\n",
       "         -3.75704793e-03,  1.02826051e-01,  2.27148086e-02, -1.74699351e-02,\n",
       "         -4.23611850e-02,  3.99827212e-02,  2.84075215e-02, -9.52568837e-03,\n",
       "         -8.33460689e-02, -4.18910570e-02,  4.40655416e-03,  7.72584230e-02,\n",
       "         -3.44201885e-02, -6.51801750e-02,  2.11573504e-02, -5.13459072e-02,\n",
       "         -1.02509754e-02, -9.43788700e-03,  5.24687842e-02,  1.57512333e-02,\n",
       "          1.80271193e-02,  4.64010276e-02, -5.59249595e-02,  1.74127650e-02,\n",
       "         -3.84426937e-02, -9.39408131e-03, -2.02259272e-02, -6.10756949e-02,\n",
       "          3.06260251e-02, -7.42047429e-02, -9.61474106e-02, -1.67247951e-02,\n",
       "         -4.24723253e-02, -2.02591866e-02, -6.77602598e-03,  2.21579503e-02,\n",
       "          4.69593219e-02, -1.06902877e-02,  3.02405655e-02,  3.86362076e-02,\n",
       "          2.33660862e-02,  2.88356245e-02, -8.61319568e-05, -1.00226879e-01,\n",
       "         -5.15654637e-03, -9.19535756e-03,  4.03113663e-02, -1.00626443e-02,\n",
       "         -2.30230335e-02,  6.34278264e-03, -3.54236960e-02, -1.56535469e-02,\n",
       "          5.31628095e-02, -1.03611469e-01,  7.60906003e-03,  8.68833996e-03,\n",
       "          5.24918456e-03, -1.39614083e-02, -4.37753499e-02,  2.22145822e-02,\n",
       "          3.23599540e-02,  1.40522411e-02, -2.23572124e-02,  3.58013995e-02,\n",
       "          7.29300664e-04, -3.61612067e-02, -6.47536069e-02, -4.14258614e-02,\n",
       "          9.82583733e-04,  2.65088454e-02,  1.78467799e-02,  5.23238555e-02,\n",
       "          2.28770878e-02, -1.76921987e-03,  3.99668291e-02,  2.77336687e-02,\n",
       "         -2.35777050e-02,  1.09101469e-02, -3.89250144e-02,  4.78791445e-02,\n",
       "         -5.88599220e-03, -8.96757375e-03, -1.94060802e-02,  1.59612170e-03,\n",
       "         -4.77885492e-02,  2.73479056e-02, -1.09634036e-02,  9.99330580e-02,\n",
       "         -3.73519883e-02, -2.91584954e-02,  2.92945020e-02, -4.46223095e-02,\n",
       "          4.19180281e-02,  3.26917544e-02,  3.09923494e-33,  1.35406423e-02,\n",
       "         -6.46378612e-03,  8.00940767e-02,  4.91494080e-03, -1.26748392e-03,\n",
       "          9.41540953e-03, -1.32678226e-02,  5.70569672e-02,  3.09232436e-02,\n",
       "         -1.28387648e-03,  4.43949481e-04,  1.60083268e-02, -4.69601247e-03,\n",
       "          1.55730471e-02, -7.31702475e-03,  2.34485343e-02,  1.91572458e-02,\n",
       "         -1.63661931e-02, -1.08943898e-02,  8.82242341e-03, -3.39862742e-02,\n",
       "          4.07596938e-02, -6.80436715e-02,  7.20138997e-02, -6.49360567e-03,\n",
       "          2.70491317e-02,  2.76703890e-02,  1.01801031e-03, -3.20624281e-03,\n",
       "          2.13048588e-02,  2.89350972e-02, -4.75984439e-02,  7.07720742e-02,\n",
       "         -5.60171902e-02, -6.74320608e-02, -4.00250293e-02,  4.10797633e-02,\n",
       "          7.89656397e-03, -2.75333077e-02, -4.15740460e-02, -4.21587974e-02,\n",
       "          1.55404285e-02, -6.01283042e-04, -4.73976806e-02, -7.56164864e-02,\n",
       "          3.11517864e-02,  6.41481951e-02, -1.43674780e-02,  1.17789477e-03,\n",
       "         -1.04975840e-02,  3.69334556e-02,  1.60890054e-02, -1.29666533e-02,\n",
       "         -3.22229192e-02,  6.46527112e-02,  4.44219336e-02,  6.56912774e-02,\n",
       "         -5.15353493e-02,  3.53155620e-02, -9.41079017e-03, -3.99167975e-03,\n",
       "          6.31214455e-02,  2.01883074e-03,  9.06958990e-03, -1.01577546e-02,\n",
       "          1.61848646e-02, -4.96147238e-02,  2.75281258e-02,  2.05466952e-02,\n",
       "          2.92857271e-02, -9.73171741e-03, -5.92599995e-02,  4.33913246e-02,\n",
       "          5.36635593e-02, -4.67075780e-02, -2.38197632e-02, -3.51437293e-02,\n",
       "         -3.42567824e-02,  7.41950572e-02, -6.75584972e-02, -4.42645187e-03,\n",
       "         -1.97291840e-02,  3.13582048e-02,  5.94913773e-03, -5.21435589e-02,\n",
       "         -4.12772745e-02, -3.55087779e-02,  1.13938563e-02, -2.39379611e-02,\n",
       "          1.56408874e-03, -1.50261503e-02, -6.64502988e-03, -1.80532709e-02,\n",
       "          7.16565002e-04, -4.97447550e-02,  5.99072641e-03,  6.16828129e-02,\n",
       "          3.72681543e-02, -7.97112472e-03, -4.49941792e-02,  4.82679228e-04,\n",
       "         -7.62579143e-02, -2.39685066e-02, -4.77403589e-03,  3.27506028e-02,\n",
       "          7.77784211e-04,  9.40233655e-03,  1.48005104e-02, -2.46752128e-02,\n",
       "         -6.76285774e-02,  2.17864271e-02,  6.97903484e-02, -9.44709592e-03,\n",
       "         -4.66607474e-02,  3.66887189e-02,  3.47298384e-02, -4.18218002e-02,\n",
       "         -7.12507311e-03,  5.57642244e-02,  2.14997735e-02, -3.62453572e-02,\n",
       "         -3.08646765e-02,  8.55871662e-03, -5.51908500e-02,  4.48925234e-02,\n",
       "          1.26042692e-02, -1.93503257e-02,  5.35859400e-03, -5.52599728e-02,\n",
       "         -8.09645280e-02, -3.86619866e-02,  1.51483221e-02,  1.15399836e-02,\n",
       "         -4.32169437e-02,  1.12929819e-02, -1.62427817e-02,  1.12044262e-02,\n",
       "         -1.96966045e-02, -2.34390311e-02, -1.81178339e-02,  4.26211581e-02,\n",
       "         -4.54321504e-02,  6.98256269e-02,  1.44951511e-02, -1.13588842e-02,\n",
       "         -3.57126854e-02,  6.94319094e-03,  1.37418928e-02, -4.12221774e-02,\n",
       "         -1.79586262e-02,  3.33564356e-02, -3.88308987e-02,  3.12982723e-02,\n",
       "         -3.27896215e-02, -3.59330233e-03,  2.45306566e-02, -4.82232384e-02,\n",
       "         -1.62500106e-02, -4.26786914e-02,  3.09232939e-02, -2.95728520e-02,\n",
       "         -6.57398701e-02,  5.52653596e-02, -3.94064263e-02,  3.50694172e-03,\n",
       "          2.24651247e-02,  6.56594113e-02, -1.56933814e-02, -4.14019488e-02,\n",
       "         -2.16796622e-02, -3.24114896e-02, -1.14378206e-04,  1.29604107e-02,\n",
       "          9.69166122e-03,  4.26665097e-02,  7.95746446e-02, -5.05833365e-02,\n",
       "          5.06130010e-02,  2.72401120e-03, -2.69059930e-02,  5.03453091e-02,\n",
       "          4.52930741e-02, -4.85944487e-02, -1.86743401e-02,  3.56186889e-02,\n",
       "         -2.63759792e-02, -5.30626476e-02,  5.95516749e-02, -5.49286492e-02,\n",
       "         -1.82352010e-02,  6.73303427e-03,  1.25535978e-02,  1.97866615e-02,\n",
       "         -1.34851951e-02,  1.02686873e-02,  2.07688883e-02,  4.25718166e-02,\n",
       "          8.43099505e-03, -8.80067144e-03, -2.82641947e-02,  5.71592934e-02,\n",
       "         -2.35818122e-02,  1.60125121e-02, -4.93834056e-02,  4.41780724e-02,\n",
       "          2.88255736e-02, -1.96388420e-02,  2.24126205e-02, -1.71245355e-02,\n",
       "         -3.56065528e-03, -2.05277409e-02,  7.56327510e-02, -5.53401671e-02,\n",
       "         -5.60698565e-03, -1.39477625e-02, -1.03040263e-02,  1.74755789e-02,\n",
       "          5.40576428e-02, -2.82362173e-03, -3.98677913e-03, -1.40531342e-02,\n",
       "         -3.87696996e-02, -5.30439876e-02, -4.53705192e-02,  1.66066960e-02,\n",
       "          4.27011847e-02, -2.66702436e-02, -2.14734506e-02, -4.20805812e-02,\n",
       "         -5.00576161e-02, -3.90935093e-02,  4.46314029e-02,  1.42659210e-02,\n",
       "         -7.67839048e-03, -3.87265123e-02,  1.63363498e-02, -2.52543613e-02,\n",
       "          2.37796642e-02,  5.41979745e-02,  4.73445728e-02,  4.93465960e-02,\n",
       "         -6.23228122e-03,  8.34136978e-02, -5.05925380e-02, -5.82449473e-02,\n",
       "         -9.18184780e-03, -1.59667190e-02,  7.83184022e-02, -3.44820507e-02,\n",
       "          3.63217741e-02,  1.62527394e-02,  4.52755317e-02,  5.42612262e-02,\n",
       "         -3.52443047e-02,  6.96089491e-02, -4.41108644e-02,  5.68280146e-02,\n",
       "          6.24870183e-03,  4.36260663e-02,  1.46347694e-02, -2.69974340e-02,\n",
       "         -2.52817236e-02,  2.78671924e-02,  3.12308259e-02,  1.21025890e-02,\n",
       "         -8.88523646e-03, -4.24790196e-02,  1.41246282e-02, -5.40934410e-03,\n",
       "         -2.52110721e-03, -2.55060792e-02, -3.09093278e-02, -5.00194728e-04]),\n",
       "  'score': tensor(0.0616)},\n",
       " {'page_number': 13,\n",
       "  'sentence_chunk': 'He clicked it once, and twelve balls of light sped back to their street lamps so that Privet Drive glowed suddenly orange and he could make out a tabby cat slinking around the corner at the other end of the street. He could just see the bundle of blankets on the step of number four.   “Good luck, Harry,” he murmured. He turned on his heel and with a swish of his cloak, he was gone.   A breeze ruffled the neat hedges of Privet Drive, which lay silent and tidy under the inky sky, the very last place you would expect astonishing things to',\n",
       "  'chunk_char_count': 541,\n",
       "  'chunk_word_count': 107,\n",
       "  'chunk_token_count': 135.25,\n",
       "  'embedding': array([-2.66983593e-03, -1.88588109e-02,  1.65371355e-02,  3.46193425e-02,\n",
       "          8.60169232e-02, -1.92778336e-03,  6.96286792e-03,  2.21349020e-02,\n",
       "         -5.56791872e-02,  9.11657885e-03,  1.99615397e-02, -3.90170850e-02,\n",
       "         -1.49236070e-02, -4.81670499e-02, -6.20430335e-02, -5.75582031e-03,\n",
       "          1.70887765e-02,  4.72051725e-02, -5.29784262e-02, -3.95917185e-02,\n",
       "          1.80951338e-02, -1.37834838e-02,  1.21545689e-02, -4.54506613e-02,\n",
       "          6.24006875e-02, -3.71408910e-02, -8.89884681e-03,  4.55341190e-02,\n",
       "         -4.26392667e-02,  2.69683171e-02, -1.26893446e-02, -3.09563763e-02,\n",
       "          4.30075340e-02,  1.48924869e-02,  8.12520913e-04, -1.40448357e-03,\n",
       "          4.28677574e-02, -5.96814603e-03,  3.78514864e-02, -4.37749960e-02,\n",
       "         -3.77320051e-02, -2.02959403e-02, -3.09880041e-02,  5.14379004e-03,\n",
       "          3.37051973e-02,  1.71850715e-02, -7.33095780e-02,  3.74737978e-02,\n",
       "          1.92504153e-02,  7.05605187e-03,  3.09546590e-02, -1.29900398e-02,\n",
       "         -3.83436233e-02,  2.22142655e-02,  4.99672368e-02,  1.01370336e-02,\n",
       "          2.18570270e-02, -9.28534381e-03,  4.24733758e-03, -1.53222783e-02,\n",
       "          6.11778274e-02, -6.23943806e-02, -2.70339251e-02,  2.31849346e-02,\n",
       "          2.50324253e-02, -6.12187479e-03, -5.78731634e-02, -6.19326998e-03,\n",
       "         -1.11519899e-02,  9.71515197e-03,  2.99175382e-02, -1.58673283e-02,\n",
       "          2.51401612e-03,  4.24000956e-02, -2.49863192e-02,  3.96218114e-02,\n",
       "          4.98914383e-02, -6.23778999e-02, -1.98661741e-02,  5.08384965e-03,\n",
       "          5.12972567e-03,  4.20791693e-02,  2.32120906e-03, -2.42482815e-02,\n",
       "          5.21470457e-02, -8.32834318e-02, -1.66114364e-02, -1.44524714e-02,\n",
       "         -4.19233547e-04, -1.63678844e-02,  2.32161209e-02,  5.68347573e-02,\n",
       "         -1.69383306e-02,  6.59598187e-02,  7.64151569e-04,  1.94345266e-02,\n",
       "          2.12484272e-03,  7.36983269e-02, -9.26735159e-03, -5.16026393e-02,\n",
       "         -9.68131796e-03, -1.09342281e-02, -1.21098891e-01, -5.03400676e-02,\n",
       "          2.03417223e-02,  4.81358031e-03, -3.05308192e-03,  1.38973864e-02,\n",
       "          6.41886191e-03, -6.88808560e-02, -3.22937705e-02, -3.18531157e-03,\n",
       "         -3.42079401e-02,  7.02072447e-03, -1.11036038e-03, -2.98126172e-02,\n",
       "         -3.57391089e-02, -5.79052512e-03,  7.20387558e-03, -1.27919158e-03,\n",
       "          8.24059080e-03, -4.51034680e-02,  2.31086835e-02, -1.66357439e-02,\n",
       "         -3.28926891e-02,  4.08873521e-02,  1.81076620e-02, -5.05681969e-02,\n",
       "         -3.08063608e-02,  1.77805405e-02, -5.05349934e-02, -1.55851273e-02,\n",
       "          3.91950384e-02, -2.19554156e-02, -2.04582661e-02,  4.98185586e-03,\n",
       "         -9.86758154e-03,  5.77638410e-02, -2.26476393e-03, -1.17641059e-03,\n",
       "          3.19755115e-02,  3.46201956e-02, -5.89136686e-03,  1.50444973e-02,\n",
       "         -2.06795987e-03,  1.67119969e-02,  1.29464250e-02,  1.99455023e-02,\n",
       "         -6.19999133e-02,  4.56072725e-02, -7.08476116e-04, -1.55516490e-02,\n",
       "         -1.00808842e-02, -5.27525805e-02, -9.98596288e-03,  1.50793847e-02,\n",
       "         -7.17064925e-03,  1.41892731e-02, -5.02196653e-03,  1.00696525e-02,\n",
       "          1.90383743e-03,  4.99619059e-02,  2.04981100e-02, -2.44756080e-02,\n",
       "          2.78294049e-02,  2.86968146e-02, -3.89591511e-03,  2.36347020e-02,\n",
       "          3.83481830e-02, -3.56937610e-02, -1.08844396e-02, -4.81537869e-03,\n",
       "          1.36923662e-03,  5.18113114e-02, -4.71474305e-02, -1.04361903e-02,\n",
       "         -4.03554924e-02, -2.51314342e-02,  5.01828045e-02,  2.08363403e-02,\n",
       "         -3.98129635e-02,  3.41739990e-02, -9.26952343e-03, -1.35447672e-02,\n",
       "          2.50123721e-02,  3.65526602e-02, -2.84898770e-03, -5.37949316e-02,\n",
       "         -8.25881734e-02,  7.24892784e-03, -2.56689433e-02,  2.45333114e-03,\n",
       "         -2.95583997e-02,  1.43318810e-02,  3.44576687e-02,  3.03884093e-02,\n",
       "          3.39034125e-02,  1.88778490e-02,  4.57303859e-02, -5.81621565e-02,\n",
       "         -1.27829127e-02, -2.51234099e-02, -9.58108436e-03, -4.64542918e-02,\n",
       "         -3.85222882e-02,  2.88372152e-02, -4.83270250e-02, -3.02244853e-02,\n",
       "         -1.97685454e-02,  1.11776600e-02, -6.84505235e-03,  9.44412220e-03,\n",
       "         -9.42261145e-03,  6.76910579e-02,  7.20457956e-02, -5.45045510e-02,\n",
       "          1.81569066e-02, -4.02081087e-02,  2.20538601e-02,  1.07118096e-02,\n",
       "          7.62084778e-03,  8.36453028e-03,  5.83503954e-03,  1.16419769e-03,\n",
       "         -2.37589795e-02, -1.32001385e-01,  6.78833202e-02, -5.21238986e-03,\n",
       "         -7.47559741e-02,  5.19805923e-02, -2.82048956e-02,  3.17689739e-02,\n",
       "         -3.37780006e-02, -2.56333388e-02,  2.01983918e-02,  1.81836411e-02,\n",
       "         -4.32754643e-02, -1.00743184e-02,  3.61591242e-02, -1.06879696e-02,\n",
       "          2.41296086e-02,  1.69795584e-02,  4.20082361e-02,  3.89681868e-02,\n",
       "          2.22998969e-02, -4.24437970e-02,  1.65287852e-02, -2.68656593e-02,\n",
       "          3.56532410e-02, -6.26108497e-02,  4.64432174e-03, -1.77814085e-02,\n",
       "          6.68687746e-02,  8.85742251e-03,  1.48211019e-02,  9.29571502e-03,\n",
       "          4.05859277e-02,  2.01151557e-02,  4.82005905e-03,  1.06147774e-01,\n",
       "         -6.94437930e-03, -3.42092551e-02, -5.62736839e-02, -3.76522839e-02,\n",
       "          1.42113613e-02, -4.28133830e-02, -4.23090942e-02,  5.37700690e-02,\n",
       "         -3.81311141e-02, -1.49753189e-03, -1.31224189e-02, -3.32289375e-02,\n",
       "          3.90613936e-02,  1.66038685e-02, -8.57374538e-03,  2.66436059e-02,\n",
       "         -2.68974099e-02, -1.71496929e-03, -1.38319340e-02, -7.08568469e-02,\n",
       "         -1.78931770e-03,  1.81714334e-02, -1.76155120e-02, -4.22184914e-02,\n",
       "          2.26929486e-02,  7.22533017e-02,  7.23191723e-03,  3.80387157e-02,\n",
       "          3.17904167e-02, -2.37516556e-02, -5.54446578e-02,  1.06970752e-02,\n",
       "          5.86672463e-02,  1.32500082e-02,  1.08935619e-02,  4.82514873e-02,\n",
       "          3.09892539e-02, -6.79485872e-02, -2.46510021e-02,  4.91229258e-03,\n",
       "         -5.50659979e-03, -6.04156740e-02,  4.53090155e-03,  4.99654263e-02,\n",
       "          2.20538341e-02, -1.38760414e-02, -1.03166085e-02, -9.12445015e-04,\n",
       "          1.63717419e-02,  3.11907306e-02, -3.99737246e-02,  7.15162884e-03,\n",
       "          1.41420253e-02, -1.48363523e-02, -3.26488353e-02,  2.82165334e-02,\n",
       "          3.79717089e-02, -3.28029916e-02,  7.03621283e-02,  1.58260134e-03,\n",
       "         -3.43708433e-02, -2.95237992e-02, -3.48402411e-02,  5.93213970e-03,\n",
       "          1.17773470e-02,  7.88029190e-03,  7.50237927e-02,  3.32639366e-02,\n",
       "          3.42200696e-02,  3.22885513e-02, -8.96277055e-02,  4.02351171e-02,\n",
       "          7.97379576e-03,  2.66937576e-02,  2.23620087e-02,  1.03832027e-02,\n",
       "          2.59601027e-02, -1.09536359e-02,  2.59146690e-02,  3.04139964e-02,\n",
       "          6.37716008e-03, -3.63528319e-02, -8.43974352e-02,  1.21850166e-02,\n",
       "         -2.23004241e-02,  1.31431352e-02, -6.36179820e-02,  5.71255013e-02,\n",
       "          4.24576551e-02, -5.26560731e-02,  3.76562763e-04, -2.81198230e-03,\n",
       "         -3.93278860e-02,  3.14439721e-02,  1.06499549e-02, -2.57296674e-02,\n",
       "         -4.88352962e-03, -4.25620237e-03, -3.09878085e-02, -5.86125515e-02,\n",
       "          1.92448609e-02,  3.84864695e-02,  8.63014720e-03, -5.02260868e-03,\n",
       "         -2.87598488e-03, -5.85492887e-02, -6.47192970e-02,  5.87473661e-02,\n",
       "          1.71871725e-02, -3.69058102e-02, -2.21502054e-02,  6.38848543e-02,\n",
       "          1.98135488e-02, -3.92092625e-03, -1.36796124e-02,  7.04356609e-03,\n",
       "         -3.90280485e-02, -3.35419625e-02,  9.26949605e-02, -2.51885466e-02,\n",
       "          1.37354843e-02, -2.44668219e-02,  3.75204757e-02,  5.01390453e-03,\n",
       "          1.18119502e-02,  1.83420870e-02,  2.43188199e-02,  2.34075636e-02,\n",
       "          1.08244784e-01,  4.26176786e-02,  1.03765642e-02, -2.39738170e-02,\n",
       "         -6.57228902e-02, -5.45993336e-02,  8.35057721e-03,  2.08318979e-03,\n",
       "          1.16324797e-02, -4.68763337e-02,  1.95135809e-02,  4.75285314e-02,\n",
       "          4.43950221e-02,  4.53998111e-02,  5.46116121e-02, -2.96742711e-02,\n",
       "         -6.43746629e-02,  7.67209148e-03, -3.92983556e-02, -1.93535443e-02,\n",
       "         -5.60787879e-02, -4.38709483e-02, -3.52332294e-02,  1.04910761e-01,\n",
       "          2.19683554e-02, -1.86616331e-02,  7.40289688e-02, -2.55679991e-02,\n",
       "         -2.62954360e-04,  2.39199549e-02,  5.89610040e-02,  4.17867191e-02,\n",
       "         -1.15882242e-02,  2.46042553e-02, -2.89252438e-02, -8.95889290e-03,\n",
       "          3.18534486e-02, -2.96757314e-02, -3.56225506e-03, -3.12276687e-02,\n",
       "         -4.22140285e-02, -4.77324799e-02, -2.36395448e-02,  1.06143933e-02,\n",
       "          8.19121487e-03, -1.32378424e-02, -2.11321153e-02,  2.47884978e-04,\n",
       "          5.57498261e-02, -2.33445261e-02, -2.43029073e-02, -4.21927646e-02,\n",
       "         -2.79759895e-03,  1.00179845e-02,  6.82592839e-02, -3.05502564e-02,\n",
       "         -3.72869857e-02, -1.63006177e-03,  1.29727647e-02,  6.88415840e-02,\n",
       "         -4.46912553e-03, -5.61177824e-03, -4.98780794e-02, -4.30145934e-02,\n",
       "         -8.13404173e-02, -9.64023173e-02,  4.38774042e-02, -1.00731384e-02,\n",
       "          5.56700677e-03, -5.75199015e-02,  5.22531150e-03,  8.71864089e-04,\n",
       "          1.56615656e-02,  3.27548422e-02, -1.99652221e-02, -2.12391578e-02,\n",
       "          3.90282348e-02, -3.18627208e-02, -3.35747115e-02, -7.35520571e-02,\n",
       "         -2.03237426e-03,  4.84283417e-02, -5.49854571e-03,  4.02038246e-02,\n",
       "         -3.60100679e-02,  4.34501609e-03,  3.25263180e-02,  2.66832691e-02,\n",
       "          3.11132101e-03,  8.08666870e-02, -4.74933255e-03, -2.09145136e-02,\n",
       "         -2.82025281e-02, -2.29778490e-03,  2.95930840e-02, -1.89372022e-02,\n",
       "         -2.62841135e-02,  8.40311230e-04, -6.09504208e-02,  3.72922830e-02,\n",
       "         -7.26533756e-02, -3.27632278e-02,  3.21870251e-03, -1.42622637e-02,\n",
       "          1.45380273e-02,  9.71873943e-03,  3.66806914e-33, -2.65701227e-02,\n",
       "          4.65430021e-02, -7.31566502e-03, -1.63475554e-02,  8.49914178e-03,\n",
       "         -4.63669561e-02, -4.07821266e-03,  1.84974875e-02,  2.29144935e-02,\n",
       "          1.97276529e-02, -8.90024602e-02, -1.65031124e-02, -1.66878700e-02,\n",
       "          5.48538230e-02,  4.01962025e-04,  8.13023094e-03,  9.53751802e-02,\n",
       "          2.44863387e-02,  1.88936815e-02, -2.74980329e-02, -4.43331674e-02,\n",
       "          2.33341679e-02, -8.43000114e-02,  5.00558242e-02,  7.30672851e-02,\n",
       "         -6.15416951e-02, -5.65933995e-03,  3.02956775e-02, -1.09690772e-02,\n",
       "          4.46521398e-03,  1.82438884e-02, -1.07608680e-02,  6.07240945e-02,\n",
       "         -2.32592002e-02, -7.91936144e-02, -1.55821256e-02,  6.61082193e-02,\n",
       "         -1.84381902e-02, -3.88903869e-03,  2.60714721e-02,  1.83701906e-02,\n",
       "          1.19720749e-03, -1.26966564e-02,  2.24371161e-02, -3.01811434e-02,\n",
       "          2.25942694e-02, -1.81468409e-02, -2.06417665e-02, -2.73363292e-02,\n",
       "         -1.63661316e-02,  8.87832046e-03,  5.91961332e-02, -6.30922802e-03,\n",
       "         -5.63792139e-02,  6.21897094e-02,  7.35924691e-02,  4.21270393e-02,\n",
       "         -3.60757336e-02,  6.15361556e-02, -2.12090239e-02,  6.56708563e-03,\n",
       "          6.04417995e-02,  1.72933172e-02,  6.56203134e-03, -5.60497902e-02,\n",
       "          2.51516085e-02, -3.12117226e-02,  2.70838686e-03, -5.49449073e-03,\n",
       "          2.76538003e-02,  1.57423355e-02, -1.43924979e-02,  5.73496558e-02,\n",
       "         -1.76223845e-03,  4.77300084e-04,  2.22837720e-02, -3.16236615e-02,\n",
       "         -8.74411417e-05,  2.03295872e-02,  4.16554557e-03,  1.88416312e-03,\n",
       "          4.02224921e-02,  3.95895392e-02,  8.86111148e-03,  3.71296108e-02,\n",
       "          1.85245089e-02, -8.85700400e-04,  7.49356439e-03, -4.44541425e-02,\n",
       "         -4.63436916e-02, -4.60903756e-02, -1.89963188e-02, -2.26761084e-02,\n",
       "          1.69036677e-03, -2.73865424e-02,  4.04350497e-02,  2.63547264e-02,\n",
       "          1.10779181e-02, -1.05097005e-02, -8.04023817e-03,  4.10750657e-02,\n",
       "          3.06728110e-02,  4.64718118e-02,  7.12435506e-03,  2.18839310e-02,\n",
       "          5.90311969e-03, -7.84119591e-02, -2.32319012e-02, -1.29787633e-02,\n",
       "         -3.28390673e-02, -2.44602188e-02,  6.55299127e-02, -9.14353132e-03,\n",
       "         -2.86045484e-02, -1.27384346e-02,  2.52466369e-02, -3.78207751e-02,\n",
       "          9.09631699e-03,  2.07651518e-02, -7.40803592e-03,  3.32760159e-03,\n",
       "         -2.11733263e-02, -1.96256172e-02, -7.96720311e-02,  4.14280593e-02,\n",
       "          1.00513862e-03, -1.79678984e-02, -2.55267732e-02,  3.68089564e-02,\n",
       "         -1.48109406e-01, -1.92915332e-02,  2.82404106e-02, -6.96666166e-02,\n",
       "         -2.20352020e-02, -1.86488070e-02, -1.06578404e-02,  7.37206489e-02,\n",
       "          9.97157767e-03, -9.06809047e-03,  2.18532737e-02,  2.48925220e-02,\n",
       "          4.33967747e-02, -7.18545076e-03, -1.18869562e-02,  1.03228549e-02,\n",
       "          1.44516397e-02,  2.16311403e-02, -1.40856626e-02, -1.72537882e-02,\n",
       "         -6.47936612e-02,  3.21926810e-02,  2.30623130e-03, -2.97937216e-03,\n",
       "          2.00846735e-02, -7.93791190e-03,  1.05868010e-02,  1.10170385e-02,\n",
       "         -4.15385775e-02, -2.90531702e-02,  3.20092179e-02,  2.75653955e-02,\n",
       "          1.07856421e-02,  4.54787239e-02, -3.31000350e-02, -1.34309707e-02,\n",
       "          2.25149877e-02,  2.55119093e-02,  3.84946503e-02, -3.64777930e-02,\n",
       "          8.69839713e-02, -2.09339671e-02, -2.91490834e-02,  2.52352022e-02,\n",
       "         -6.75785413e-04, -5.35273599e-03, -6.90998789e-03, -2.67180451e-03,\n",
       "         -7.03313737e-04, -2.75359098e-02, -6.88310266e-02,  4.46530879e-02,\n",
       "         -1.95683669e-02, -4.66029942e-02, -3.52626443e-02,  1.40566155e-02,\n",
       "         -2.37463769e-02, -9.77377221e-03,  3.80803719e-02, -9.58396569e-02,\n",
       "          3.27652358e-02,  1.13805092e-03, -3.06954198e-02,  3.60223688e-02,\n",
       "          1.74430832e-02,  2.25385707e-02, -5.91837394e-04, -3.03188805e-03,\n",
       "         -3.46014537e-02,  2.32384056e-02,  4.57217731e-03,  4.01896648e-02,\n",
       "         -1.74641740e-02, -1.94290595e-04, -2.55629756e-02,  6.47101104e-02,\n",
       "         -1.50156030e-02, -2.79088542e-02, -1.65890064e-02, -2.46838052e-02,\n",
       "          1.03705293e-02, -5.81201017e-02,  5.85303977e-02, -3.67673784e-02,\n",
       "         -1.73159235e-03, -1.01275640e-02, -1.39559777e-02, -2.30893283e-03,\n",
       "          1.47759560e-02, -1.16932057e-02,  5.81894722e-03, -3.61041911e-02,\n",
       "          4.29028198e-02,  1.99920330e-02, -9.75528806e-02,  3.46417949e-02,\n",
       "          6.81738481e-02, -2.20890646e-03, -2.07624380e-02, -4.07989398e-02,\n",
       "          3.08424104e-02, -5.52798733e-02,  6.17355034e-02,  2.25169882e-02,\n",
       "         -1.94245353e-02, -4.30867560e-02,  1.17310360e-02,  1.19744940e-02,\n",
       "          1.99987274e-02,  8.39991197e-02,  7.44510293e-02,  1.53493788e-02,\n",
       "         -1.28847118e-02,  3.92993689e-02,  4.72177844e-03, -1.93198025e-02,\n",
       "          4.00768481e-02,  4.15589251e-02,  2.55940761e-02, -6.54383451e-02,\n",
       "         -3.79677839e-03,  4.19616103e-02, -2.92619952e-04, -1.32926684e-02,\n",
       "          1.89602952e-02,  2.53131744e-02, -5.30384593e-02,  6.49006888e-02,\n",
       "          4.41225991e-02,  2.68864390e-02, -4.57080118e-02, -3.67233939e-02,\n",
       "         -4.58622235e-04, -3.04793082e-02, -2.20389385e-02,  2.59194039e-02,\n",
       "         -4.96046320e-02, -3.09413765e-02, -4.74577472e-02,  2.95764655e-02,\n",
       "         -4.21062857e-02, -2.33874470e-02, -1.60010774e-02, -9.95003898e-03]),\n",
       "  'score': tensor(0.0605)}]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques = \"what was in the letter for harry potter?\"\n",
    "print(f\"Query: {ques}\")\n",
    "\n",
    "# Answer query with context and return context \n",
    "answer, context_items = ask(query=ques, \n",
    "                            temperature=0.7,\n",
    "                            max_new_tokens=512,\n",
    "                            return_answer_only=False)\n",
    "\n",
    "print(f\"Answer:\\n\")\n",
    "print_wrapped(answer)\n",
    "print(f\"Context items:\")\n",
    "context_items"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
